{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "of Forest Fires Deep Learning Pract.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "on1Cjy3Zq73D",
        "W1V-d9d6sqKU"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras import layers, optimizers\n",
        "from keras.layers import Dense\n",
        "from sklearn import svm, datasets"
      ],
      "metadata": {
        "id": "gdG_oVnr9GEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/forest-fires/forestfires.csv')\n",
        "data  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "ezXCbRKD9KxT",
        "outputId": "548b4b73-45d1-4db3-898a-2647426e6f77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-009226de-fc8e-433c-9670-2bd2477c85d2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>X</th>\n",
              "      <th>Y</th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>FFMC</th>\n",
              "      <th>DMC</th>\n",
              "      <th>DC</th>\n",
              "      <th>ISI</th>\n",
              "      <th>temp</th>\n",
              "      <th>RH</th>\n",
              "      <th>wind</th>\n",
              "      <th>rain</th>\n",
              "      <th>area</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>mar</td>\n",
              "      <td>fri</td>\n",
              "      <td>86.2</td>\n",
              "      <td>26.2</td>\n",
              "      <td>94.3</td>\n",
              "      <td>5.1</td>\n",
              "      <td>8.2</td>\n",
              "      <td>51</td>\n",
              "      <td>6.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>oct</td>\n",
              "      <td>tue</td>\n",
              "      <td>90.6</td>\n",
              "      <td>35.4</td>\n",
              "      <td>669.1</td>\n",
              "      <td>6.7</td>\n",
              "      <td>18.0</td>\n",
              "      <td>33</td>\n",
              "      <td>0.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>oct</td>\n",
              "      <td>sat</td>\n",
              "      <td>90.6</td>\n",
              "      <td>43.7</td>\n",
              "      <td>686.9</td>\n",
              "      <td>6.7</td>\n",
              "      <td>14.6</td>\n",
              "      <td>33</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>mar</td>\n",
              "      <td>fri</td>\n",
              "      <td>91.7</td>\n",
              "      <td>33.3</td>\n",
              "      <td>77.5</td>\n",
              "      <td>9.0</td>\n",
              "      <td>8.3</td>\n",
              "      <td>97</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>mar</td>\n",
              "      <td>sun</td>\n",
              "      <td>89.3</td>\n",
              "      <td>51.3</td>\n",
              "      <td>102.2</td>\n",
              "      <td>9.6</td>\n",
              "      <td>11.4</td>\n",
              "      <td>99</td>\n",
              "      <td>1.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>512</th>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>aug</td>\n",
              "      <td>sun</td>\n",
              "      <td>81.6</td>\n",
              "      <td>56.7</td>\n",
              "      <td>665.6</td>\n",
              "      <td>1.9</td>\n",
              "      <td>27.8</td>\n",
              "      <td>32</td>\n",
              "      <td>2.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>513</th>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>aug</td>\n",
              "      <td>sun</td>\n",
              "      <td>81.6</td>\n",
              "      <td>56.7</td>\n",
              "      <td>665.6</td>\n",
              "      <td>1.9</td>\n",
              "      <td>21.9</td>\n",
              "      <td>71</td>\n",
              "      <td>5.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>54.29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>514</th>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>aug</td>\n",
              "      <td>sun</td>\n",
              "      <td>81.6</td>\n",
              "      <td>56.7</td>\n",
              "      <td>665.6</td>\n",
              "      <td>1.9</td>\n",
              "      <td>21.2</td>\n",
              "      <td>70</td>\n",
              "      <td>6.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>aug</td>\n",
              "      <td>sat</td>\n",
              "      <td>94.4</td>\n",
              "      <td>146.0</td>\n",
              "      <td>614.7</td>\n",
              "      <td>11.3</td>\n",
              "      <td>25.6</td>\n",
              "      <td>42</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>516</th>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>nov</td>\n",
              "      <td>tue</td>\n",
              "      <td>79.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>106.7</td>\n",
              "      <td>1.1</td>\n",
              "      <td>11.8</td>\n",
              "      <td>31</td>\n",
              "      <td>4.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>517 rows × 13 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-009226de-fc8e-433c-9670-2bd2477c85d2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-009226de-fc8e-433c-9670-2bd2477c85d2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-009226de-fc8e-433c-9670-2bd2477c85d2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     X  Y month  day  FFMC    DMC     DC   ISI  temp  RH  wind  rain   area\n",
              "0    7  5   mar  fri  86.2   26.2   94.3   5.1   8.2  51   6.7   0.0   0.00\n",
              "1    7  4   oct  tue  90.6   35.4  669.1   6.7  18.0  33   0.9   0.0   0.00\n",
              "2    7  4   oct  sat  90.6   43.7  686.9   6.7  14.6  33   1.3   0.0   0.00\n",
              "3    8  6   mar  fri  91.7   33.3   77.5   9.0   8.3  97   4.0   0.2   0.00\n",
              "4    8  6   mar  sun  89.3   51.3  102.2   9.6  11.4  99   1.8   0.0   0.00\n",
              "..  .. ..   ...  ...   ...    ...    ...   ...   ...  ..   ...   ...    ...\n",
              "512  4  3   aug  sun  81.6   56.7  665.6   1.9  27.8  32   2.7   0.0   6.44\n",
              "513  2  4   aug  sun  81.6   56.7  665.6   1.9  21.9  71   5.8   0.0  54.29\n",
              "514  7  4   aug  sun  81.6   56.7  665.6   1.9  21.2  70   6.7   0.0  11.16\n",
              "515  1  4   aug  sat  94.4  146.0  614.7  11.3  25.6  42   4.0   0.0   0.00\n",
              "516  6  3   nov  tue  79.5    3.0  106.7   1.1  11.8  31   4.5   0.0   0.00\n",
              "\n",
              "[517 rows x 13 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "data['month'] = encoder.fit_transform(data['month'])\n",
        "data['day'] = encoder.fit_transform(data['day'])"
      ],
      "metadata": {
        "id": "zWTSJ4UQvnpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.month.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fndd3JBFv8FE",
        "outputId": "f465828b-0d18-499a-bca6-0bd91ce84585"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1     184\n",
              "11    172\n",
              "7      54\n",
              "5      32\n",
              "3      20\n",
              "6      17\n",
              "10     15\n",
              "0       9\n",
              "2       9\n",
              "4       2\n",
              "8       2\n",
              "9       1\n",
              "Name: month, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.day.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HzAyTRtxzVi",
        "outputId": "b750237d-72aa-44d3-f6e0-a7bebc910d75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    95\n",
              "0    85\n",
              "2    84\n",
              "1    74\n",
              "5    64\n",
              "4    61\n",
              "6    54\n",
              "Name: day, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.rain.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8a-xjTBy8qX",
        "outputId": "8a9b60aa-33e2-486e-b798-f2405f023aa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0    509\n",
              "0.2      2\n",
              "0.8      2\n",
              "1.0      1\n",
              "6.4      1\n",
              "0.4      1\n",
              "1.4      1\n",
              "Name: rain, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#OHE = pd.get_dummies(data[['day','month','X']], prefix='C')\n",
        "OHE_day = pd.get_dummies(data.day, prefix='day')\n",
        "OHE_month = pd.get_dummies(data.month, prefix='month')"
      ],
      "metadata": {
        "id": "Rjel06CTgFrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OHE_day"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "pweJVQj1gHw3",
        "outputId": "6c526395-79bf-4eba-f926-8e1c1701e991"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-ceb58371-79ff-4002-a76d-6198dabd681c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>day_0</th>\n",
              "      <th>day_1</th>\n",
              "      <th>day_2</th>\n",
              "      <th>day_3</th>\n",
              "      <th>day_4</th>\n",
              "      <th>day_5</th>\n",
              "      <th>day_6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>512</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>513</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>514</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>516</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>517 rows × 7 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ceb58371-79ff-4002-a76d-6198dabd681c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ceb58371-79ff-4002-a76d-6198dabd681c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ceb58371-79ff-4002-a76d-6198dabd681c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     day_0  day_1  day_2  day_3  day_4  day_5  day_6\n",
              "0        1      0      0      0      0      0      0\n",
              "1        0      0      0      0      0      1      0\n",
              "2        0      0      1      0      0      0      0\n",
              "3        1      0      0      0      0      0      0\n",
              "4        0      0      0      1      0      0      0\n",
              "..     ...    ...    ...    ...    ...    ...    ...\n",
              "512      0      0      0      1      0      0      0\n",
              "513      0      0      0      1      0      0      0\n",
              "514      0      0      0      1      0      0      0\n",
              "515      0      0      1      0      0      0      0\n",
              "516      0      0      0      0      0      1      0\n",
              "\n",
              "[517 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "OHE_month"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "Tlj75HIWhLMc",
        "outputId": "7a1460f8-d94a-45cb-bd10-ebb86365e0d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-cf06fcae-760e-4846-adaa-ae319319f5b5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>month_0</th>\n",
              "      <th>month_1</th>\n",
              "      <th>month_2</th>\n",
              "      <th>month_3</th>\n",
              "      <th>month_4</th>\n",
              "      <th>month_5</th>\n",
              "      <th>month_6</th>\n",
              "      <th>month_7</th>\n",
              "      <th>month_8</th>\n",
              "      <th>month_9</th>\n",
              "      <th>month_10</th>\n",
              "      <th>month_11</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>512</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>513</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>514</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>516</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>517 rows × 12 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cf06fcae-760e-4846-adaa-ae319319f5b5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cf06fcae-760e-4846-adaa-ae319319f5b5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cf06fcae-760e-4846-adaa-ae319319f5b5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     month_0  month_1  month_2  month_3  ...  month_8  month_9  month_10  month_11\n",
              "0          0        0        0        0  ...        0        0         0         0\n",
              "1          0        0        0        0  ...        0        0         1         0\n",
              "2          0        0        0        0  ...        0        0         1         0\n",
              "3          0        0        0        0  ...        0        0         0         0\n",
              "4          0        0        0        0  ...        0        0         0         0\n",
              "..       ...      ...      ...      ...  ...      ...      ...       ...       ...\n",
              "512        0        1        0        0  ...        0        0         0         0\n",
              "513        0        1        0        0  ...        0        0         0         0\n",
              "514        0        1        0        0  ...        0        0         0         0\n",
              "515        0        1        0        0  ...        0        0         0         0\n",
              "516        0        0        0        0  ...        0        1         0         0\n",
              "\n",
              "[517 rows x 12 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "def minmaxscal(s,c,cc):\n",
        "  x = (cc+'mx')\n",
        "  sc = MinMaxScaler()\n",
        "  c[x] = sc.fit_transform(s)\n",
        "  c.pop(cc)\n",
        "  return c[x]"
      ],
      "metadata": {
        "id": "Z7YH9Tfh6xBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "minmaxscal((data[['area']]), data, ('area'))\n",
        "\n",
        "minmaxscal((data[['X']]), data, ('X'))\n",
        "minmaxscal((data[['Y']]), data, ('Y'))\n",
        "minmaxscal((data[['FFMC']]), data, ('FFMC'))\n",
        "minmaxscal((data[['DMC']]), data, ('DMC'))\n",
        "\n",
        "minmaxscal((data[['DC']]), data, ('DC'))\n",
        "minmaxscal((data[['ISI']]), data, ('ISI'))\n",
        "minmaxscal((data[['temp']]), data, ('temp'))\n",
        "minmaxscal((data[['RH']]), data, ('RH'))\n",
        "\n",
        "minmaxscal((data[['wind']]), data, ('wind'))\n",
        "minmaxscal((data[['rain']]), data, ('rain'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7sKGGTJ6z8G",
        "outputId": "1bd2c615-2f6a-40aa-8482-1d293bd118f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      0.00000\n",
              "1      0.00000\n",
              "2      0.00000\n",
              "3      0.03125\n",
              "4      0.00000\n",
              "        ...   \n",
              "512    0.00000\n",
              "513    0.00000\n",
              "514    0.00000\n",
              "515    0.00000\n",
              "516    0.00000\n",
              "Name: rainmx, Length: 517, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "K9DRL4te-met",
        "outputId": "125a6a42-60f1-4375-b44b-6a993dc6a235"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-97784e0c-718c-4d80-87e7-fedd6e492b71\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>areamx</th>\n",
              "      <th>Xmx</th>\n",
              "      <th>Ymx</th>\n",
              "      <th>FFMCmx</th>\n",
              "      <th>DMCmx</th>\n",
              "      <th>DCmx</th>\n",
              "      <th>ISImx</th>\n",
              "      <th>tempmx</th>\n",
              "      <th>RHmx</th>\n",
              "      <th>windmx</th>\n",
              "      <th>rainmx</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.750</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.870968</td>\n",
              "      <td>0.086492</td>\n",
              "      <td>0.101325</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.192926</td>\n",
              "      <td>0.423529</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>5</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.750</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.927742</td>\n",
              "      <td>0.118194</td>\n",
              "      <td>0.775419</td>\n",
              "      <td>0.119430</td>\n",
              "      <td>0.508039</td>\n",
              "      <td>0.211765</td>\n",
              "      <td>0.055556</td>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.750</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.927742</td>\n",
              "      <td>0.146795</td>\n",
              "      <td>0.796294</td>\n",
              "      <td>0.119430</td>\n",
              "      <td>0.398714</td>\n",
              "      <td>0.211765</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.875</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.941935</td>\n",
              "      <td>0.110958</td>\n",
              "      <td>0.081623</td>\n",
              "      <td>0.160428</td>\n",
              "      <td>0.196141</td>\n",
              "      <td>0.964706</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.03125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.875</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.910968</td>\n",
              "      <td>0.172984</td>\n",
              "      <td>0.110590</td>\n",
              "      <td>0.171123</td>\n",
              "      <td>0.295820</td>\n",
              "      <td>0.988235</td>\n",
              "      <td>0.155556</td>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>512</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0.005904</td>\n",
              "      <td>0.375</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.811613</td>\n",
              "      <td>0.191592</td>\n",
              "      <td>0.771315</td>\n",
              "      <td>0.033868</td>\n",
              "      <td>0.823151</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.255556</td>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>513</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0.049769</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.811613</td>\n",
              "      <td>0.191592</td>\n",
              "      <td>0.771315</td>\n",
              "      <td>0.033868</td>\n",
              "      <td>0.633441</td>\n",
              "      <td>0.658824</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>514</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0.010231</td>\n",
              "      <td>0.750</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.811613</td>\n",
              "      <td>0.191592</td>\n",
              "      <td>0.771315</td>\n",
              "      <td>0.033868</td>\n",
              "      <td>0.610932</td>\n",
              "      <td>0.647059</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.976774</td>\n",
              "      <td>0.499311</td>\n",
              "      <td>0.711622</td>\n",
              "      <td>0.201426</td>\n",
              "      <td>0.752412</td>\n",
              "      <td>0.317647</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>516</th>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.625</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.784516</td>\n",
              "      <td>0.006547</td>\n",
              "      <td>0.115867</td>\n",
              "      <td>0.019608</td>\n",
              "      <td>0.308682</td>\n",
              "      <td>0.188235</td>\n",
              "      <td>0.455556</td>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>517 rows × 13 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-97784e0c-718c-4d80-87e7-fedd6e492b71')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-97784e0c-718c-4d80-87e7-fedd6e492b71 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-97784e0c-718c-4d80-87e7-fedd6e492b71');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     month  day    areamx    Xmx  ...    tempmx      RHmx    windmx   rainmx\n",
              "0        7    0  0.000000  0.750  ...  0.192926  0.423529  0.700000  0.00000\n",
              "1       10    5  0.000000  0.750  ...  0.508039  0.211765  0.055556  0.00000\n",
              "2       10    2  0.000000  0.750  ...  0.398714  0.211765  0.100000  0.00000\n",
              "3        7    0  0.000000  0.875  ...  0.196141  0.964706  0.400000  0.03125\n",
              "4        7    3  0.000000  0.875  ...  0.295820  0.988235  0.155556  0.00000\n",
              "..     ...  ...       ...    ...  ...       ...       ...       ...      ...\n",
              "512      1    3  0.005904  0.375  ...  0.823151  0.200000  0.255556  0.00000\n",
              "513      1    3  0.049769  0.125  ...  0.633441  0.658824  0.600000  0.00000\n",
              "514      1    3  0.010231  0.750  ...  0.610932  0.647059  0.700000  0.00000\n",
              "515      1    2  0.000000  0.000  ...  0.752412  0.317647  0.400000  0.00000\n",
              "516      9    5  0.000000  0.625  ...  0.308682  0.188235  0.455556  0.00000\n",
              "\n",
              "[517 rows x 13 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.drop(['month','day'],axis=1)"
      ],
      "metadata": {
        "id": "fW6ieYh-on3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_data = pd.concat([data, OHE_day, OHE_month], axis=1)"
      ],
      "metadata": {
        "id": "ibqgaL0f_BZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "5QWIok2mqCGF",
        "outputId": "2c0992aa-33d1-47e9-a8df-81f2ca485a64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-abb1e7b1-b7e9-447e-82ca-684d59deb79f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>areamx</th>\n",
              "      <th>Xmx</th>\n",
              "      <th>Ymx</th>\n",
              "      <th>FFMCmx</th>\n",
              "      <th>DMCmx</th>\n",
              "      <th>DCmx</th>\n",
              "      <th>ISImx</th>\n",
              "      <th>tempmx</th>\n",
              "      <th>RHmx</th>\n",
              "      <th>windmx</th>\n",
              "      <th>rainmx</th>\n",
              "      <th>day_0</th>\n",
              "      <th>day_1</th>\n",
              "      <th>day_2</th>\n",
              "      <th>day_3</th>\n",
              "      <th>day_4</th>\n",
              "      <th>day_5</th>\n",
              "      <th>day_6</th>\n",
              "      <th>month_0</th>\n",
              "      <th>month_1</th>\n",
              "      <th>month_2</th>\n",
              "      <th>month_3</th>\n",
              "      <th>month_4</th>\n",
              "      <th>month_5</th>\n",
              "      <th>month_6</th>\n",
              "      <th>month_7</th>\n",
              "      <th>month_8</th>\n",
              "      <th>month_9</th>\n",
              "      <th>month_10</th>\n",
              "      <th>month_11</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.750</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.870968</td>\n",
              "      <td>0.086492</td>\n",
              "      <td>0.101325</td>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.192926</td>\n",
              "      <td>0.423529</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.750</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.927742</td>\n",
              "      <td>0.118194</td>\n",
              "      <td>0.775419</td>\n",
              "      <td>0.119430</td>\n",
              "      <td>0.508039</td>\n",
              "      <td>0.211765</td>\n",
              "      <td>0.055556</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.750</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.927742</td>\n",
              "      <td>0.146795</td>\n",
              "      <td>0.796294</td>\n",
              "      <td>0.119430</td>\n",
              "      <td>0.398714</td>\n",
              "      <td>0.211765</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.875</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.941935</td>\n",
              "      <td>0.110958</td>\n",
              "      <td>0.081623</td>\n",
              "      <td>0.160428</td>\n",
              "      <td>0.196141</td>\n",
              "      <td>0.964706</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.03125</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.875</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.910968</td>\n",
              "      <td>0.172984</td>\n",
              "      <td>0.110590</td>\n",
              "      <td>0.171123</td>\n",
              "      <td>0.295820</td>\n",
              "      <td>0.988235</td>\n",
              "      <td>0.155556</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>512</th>\n",
              "      <td>0.005904</td>\n",
              "      <td>0.375</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.811613</td>\n",
              "      <td>0.191592</td>\n",
              "      <td>0.771315</td>\n",
              "      <td>0.033868</td>\n",
              "      <td>0.823151</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.255556</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>513</th>\n",
              "      <td>0.049769</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.811613</td>\n",
              "      <td>0.191592</td>\n",
              "      <td>0.771315</td>\n",
              "      <td>0.033868</td>\n",
              "      <td>0.633441</td>\n",
              "      <td>0.658824</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>514</th>\n",
              "      <td>0.010231</td>\n",
              "      <td>0.750</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.811613</td>\n",
              "      <td>0.191592</td>\n",
              "      <td>0.771315</td>\n",
              "      <td>0.033868</td>\n",
              "      <td>0.610932</td>\n",
              "      <td>0.647059</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.976774</td>\n",
              "      <td>0.499311</td>\n",
              "      <td>0.711622</td>\n",
              "      <td>0.201426</td>\n",
              "      <td>0.752412</td>\n",
              "      <td>0.317647</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>516</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.625</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.784516</td>\n",
              "      <td>0.006547</td>\n",
              "      <td>0.115867</td>\n",
              "      <td>0.019608</td>\n",
              "      <td>0.308682</td>\n",
              "      <td>0.188235</td>\n",
              "      <td>0.455556</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>517 rows × 30 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-abb1e7b1-b7e9-447e-82ca-684d59deb79f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-abb1e7b1-b7e9-447e-82ca-684d59deb79f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-abb1e7b1-b7e9-447e-82ca-684d59deb79f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       areamx    Xmx       Ymx    FFMCmx  ...  month_8  month_9  month_10  month_11\n",
              "0    0.000000  0.750  0.428571  0.870968  ...        0        0         0         0\n",
              "1    0.000000  0.750  0.285714  0.927742  ...        0        0         1         0\n",
              "2    0.000000  0.750  0.285714  0.927742  ...        0        0         1         0\n",
              "3    0.000000  0.875  0.571429  0.941935  ...        0        0         0         0\n",
              "4    0.000000  0.875  0.571429  0.910968  ...        0        0         0         0\n",
              "..        ...    ...       ...       ...  ...      ...      ...       ...       ...\n",
              "512  0.005904  0.375  0.142857  0.811613  ...        0        0         0         0\n",
              "513  0.049769  0.125  0.285714  0.811613  ...        0        0         0         0\n",
              "514  0.010231  0.750  0.285714  0.811613  ...        0        0         0         0\n",
              "515  0.000000  0.000  0.285714  0.976774  ...        0        0         0         0\n",
              "516  0.000000  0.625  0.142857  0.784516  ...        0        1         0         0\n",
              "\n",
              "[517 rows x 30 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_data.isnull().any()"
      ],
      "metadata": {
        "id": "Q0rDOnPbvgTT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58284cf8-b3ac-4277-a25a-3ad3fa7b33c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "areamx      False\n",
              "Xmx         False\n",
              "Ymx         False\n",
              "FFMCmx      False\n",
              "DMCmx       False\n",
              "DCmx        False\n",
              "ISImx       False\n",
              "tempmx      False\n",
              "RHmx        False\n",
              "windmx      False\n",
              "rainmx      False\n",
              "day_0       False\n",
              "day_1       False\n",
              "day_2       False\n",
              "day_3       False\n",
              "day_4       False\n",
              "day_5       False\n",
              "day_6       False\n",
              "month_0     False\n",
              "month_1     False\n",
              "month_2     False\n",
              "month_3     False\n",
              "month_4     False\n",
              "month_5     False\n",
              "month_6     False\n",
              "month_7     False\n",
              "month_8     False\n",
              "month_9     False\n",
              "month_10    False\n",
              "month_11    False\n",
              "dtype: bool"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_data = new_data.dropna()"
      ],
      "metadata": {
        "id": "X1L9JYHxvqsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_data.corr()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CGM0uoTlwVho",
        "outputId": "0c98c158-578e-45dd-efe2-57e8d80231ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-e0688c9f-335a-4365-9780-6a6f36acc4f0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>areamx</th>\n",
              "      <th>Xmx</th>\n",
              "      <th>Ymx</th>\n",
              "      <th>FFMCmx</th>\n",
              "      <th>DMCmx</th>\n",
              "      <th>DCmx</th>\n",
              "      <th>ISImx</th>\n",
              "      <th>tempmx</th>\n",
              "      <th>RHmx</th>\n",
              "      <th>windmx</th>\n",
              "      <th>rainmx</th>\n",
              "      <th>day_0</th>\n",
              "      <th>day_1</th>\n",
              "      <th>day_2</th>\n",
              "      <th>day_3</th>\n",
              "      <th>day_4</th>\n",
              "      <th>day_5</th>\n",
              "      <th>day_6</th>\n",
              "      <th>month_0</th>\n",
              "      <th>month_1</th>\n",
              "      <th>month_2</th>\n",
              "      <th>month_3</th>\n",
              "      <th>month_4</th>\n",
              "      <th>month_5</th>\n",
              "      <th>month_6</th>\n",
              "      <th>month_7</th>\n",
              "      <th>month_8</th>\n",
              "      <th>month_9</th>\n",
              "      <th>month_10</th>\n",
              "      <th>month_11</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>areamx</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.063385</td>\n",
              "      <td>0.044873</td>\n",
              "      <td>0.040122</td>\n",
              "      <td>0.072994</td>\n",
              "      <td>0.049383</td>\n",
              "      <td>0.008258</td>\n",
              "      <td>0.097844</td>\n",
              "      <td>-0.075519</td>\n",
              "      <td>0.012317</td>\n",
              "      <td>-0.007366</td>\n",
              "      <td>-0.052911</td>\n",
              "      <td>-0.021206</td>\n",
              "      <td>0.087868</td>\n",
              "      <td>-0.020463</td>\n",
              "      <td>0.020121</td>\n",
              "      <td>-0.001333</td>\n",
              "      <td>-0.011452</td>\n",
              "      <td>-0.008280</td>\n",
              "      <td>-0.004187</td>\n",
              "      <td>0.001010</td>\n",
              "      <td>-0.020732</td>\n",
              "      <td>-0.012589</td>\n",
              "      <td>0.006149</td>\n",
              "      <td>-0.020314</td>\n",
              "      <td>-0.045596</td>\n",
              "      <td>0.006264</td>\n",
              "      <td>-0.008893</td>\n",
              "      <td>-0.016878</td>\n",
              "      <td>0.056573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Xmx</th>\n",
              "      <td>0.063385</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.539548</td>\n",
              "      <td>-0.021039</td>\n",
              "      <td>-0.048384</td>\n",
              "      <td>-0.085916</td>\n",
              "      <td>0.006210</td>\n",
              "      <td>-0.051258</td>\n",
              "      <td>0.085223</td>\n",
              "      <td>0.018798</td>\n",
              "      <td>0.065387</td>\n",
              "      <td>-0.020061</td>\n",
              "      <td>0.039367</td>\n",
              "      <td>0.024461</td>\n",
              "      <td>-0.029338</td>\n",
              "      <td>0.000457</td>\n",
              "      <td>-0.027521</td>\n",
              "      <td>0.016035</td>\n",
              "      <td>0.063832</td>\n",
              "      <td>-0.059669</td>\n",
              "      <td>-0.006547</td>\n",
              "      <td>0.041721</td>\n",
              "      <td>-0.045002</td>\n",
              "      <td>0.061062</td>\n",
              "      <td>0.129616</td>\n",
              "      <td>0.007827</td>\n",
              "      <td>0.008917</td>\n",
              "      <td>0.025344</td>\n",
              "      <td>0.089545</td>\n",
              "      <td>-0.087216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ymx</th>\n",
              "      <td>0.044873</td>\n",
              "      <td>0.539548</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.046308</td>\n",
              "      <td>0.007782</td>\n",
              "      <td>-0.101178</td>\n",
              "      <td>-0.024488</td>\n",
              "      <td>-0.024103</td>\n",
              "      <td>0.062221</td>\n",
              "      <td>-0.020341</td>\n",
              "      <td>0.033234</td>\n",
              "      <td>-0.048773</td>\n",
              "      <td>0.021640</td>\n",
              "      <td>0.003483</td>\n",
              "      <td>0.006172</td>\n",
              "      <td>0.027873</td>\n",
              "      <td>-0.053475</td>\n",
              "      <td>0.050495</td>\n",
              "      <td>-0.008405</td>\n",
              "      <td>-0.010404</td>\n",
              "      <td>0.075850</td>\n",
              "      <td>0.016358</td>\n",
              "      <td>0.010153</td>\n",
              "      <td>0.061449</td>\n",
              "      <td>0.078594</td>\n",
              "      <td>0.050495</td>\n",
              "      <td>-0.015206</td>\n",
              "      <td>-0.046570</td>\n",
              "      <td>0.004717</td>\n",
              "      <td>-0.105465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>FFMCmx</th>\n",
              "      <td>0.040122</td>\n",
              "      <td>-0.021039</td>\n",
              "      <td>-0.046308</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.382619</td>\n",
              "      <td>0.330512</td>\n",
              "      <td>0.531805</td>\n",
              "      <td>0.431532</td>\n",
              "      <td>-0.300995</td>\n",
              "      <td>-0.028485</td>\n",
              "      <td>0.056702</td>\n",
              "      <td>0.019306</td>\n",
              "      <td>-0.059396</td>\n",
              "      <td>-0.019637</td>\n",
              "      <td>-0.089517</td>\n",
              "      <td>0.071730</td>\n",
              "      <td>0.011225</td>\n",
              "      <td>0.093908</td>\n",
              "      <td>-0.117199</td>\n",
              "      <td>0.228103</td>\n",
              "      <td>-0.137044</td>\n",
              "      <td>-0.281535</td>\n",
              "      <td>-0.454771</td>\n",
              "      <td>0.031833</td>\n",
              "      <td>-0.040634</td>\n",
              "      <td>-0.074327</td>\n",
              "      <td>-0.037230</td>\n",
              "      <td>-0.088964</td>\n",
              "      <td>-0.005998</td>\n",
              "      <td>0.076609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DMCmx</th>\n",
              "      <td>0.072994</td>\n",
              "      <td>-0.048384</td>\n",
              "      <td>0.007782</td>\n",
              "      <td>0.382619</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.682192</td>\n",
              "      <td>0.305128</td>\n",
              "      <td>0.469594</td>\n",
              "      <td>0.073795</td>\n",
              "      <td>-0.105342</td>\n",
              "      <td>0.074790</td>\n",
              "      <td>-0.012010</td>\n",
              "      <td>-0.107921</td>\n",
              "      <td>-0.003653</td>\n",
              "      <td>0.025355</td>\n",
              "      <td>0.087672</td>\n",
              "      <td>0.000016</td>\n",
              "      <td>0.017939</td>\n",
              "      <td>-0.197543</td>\n",
              "      <td>0.497928</td>\n",
              "      <td>-0.176301</td>\n",
              "      <td>-0.317899</td>\n",
              "      <td>-0.105647</td>\n",
              "      <td>-0.001946</td>\n",
              "      <td>-0.050403</td>\n",
              "      <td>-0.407404</td>\n",
              "      <td>-0.081980</td>\n",
              "      <td>-0.074218</td>\n",
              "      <td>-0.187632</td>\n",
              "      <td>0.110907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DCmx</th>\n",
              "      <td>0.049383</td>\n",
              "      <td>-0.085916</td>\n",
              "      <td>-0.101178</td>\n",
              "      <td>0.330512</td>\n",
              "      <td>0.682192</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.229154</td>\n",
              "      <td>0.496208</td>\n",
              "      <td>-0.039192</td>\n",
              "      <td>-0.203466</td>\n",
              "      <td>0.035861</td>\n",
              "      <td>-0.004220</td>\n",
              "      <td>-0.052993</td>\n",
              "      <td>-0.035189</td>\n",
              "      <td>-0.001431</td>\n",
              "      <td>0.051859</td>\n",
              "      <td>0.028368</td>\n",
              "      <td>0.024803</td>\n",
              "      <td>-0.268211</td>\n",
              "      <td>0.279361</td>\n",
              "      <td>-0.105642</td>\n",
              "      <td>-0.399277</td>\n",
              "      <td>-0.115064</td>\n",
              "      <td>-0.100887</td>\n",
              "      <td>-0.186183</td>\n",
              "      <td>-0.650427</td>\n",
              "      <td>-0.114209</td>\n",
              "      <td>-0.078380</td>\n",
              "      <td>0.093279</td>\n",
              "      <td>0.531857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ISImx</th>\n",
              "      <td>0.008258</td>\n",
              "      <td>0.006210</td>\n",
              "      <td>-0.024488</td>\n",
              "      <td>0.531805</td>\n",
              "      <td>0.305128</td>\n",
              "      <td>0.229154</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.394287</td>\n",
              "      <td>-0.132517</td>\n",
              "      <td>0.106826</td>\n",
              "      <td>0.067668</td>\n",
              "      <td>0.046695</td>\n",
              "      <td>-0.158601</td>\n",
              "      <td>-0.038585</td>\n",
              "      <td>-0.003243</td>\n",
              "      <td>-0.022406</td>\n",
              "      <td>0.068610</td>\n",
              "      <td>0.125415</td>\n",
              "      <td>-0.106478</td>\n",
              "      <td>0.334639</td>\n",
              "      <td>-0.162322</td>\n",
              "      <td>-0.249777</td>\n",
              "      <td>-0.103588</td>\n",
              "      <td>0.020982</td>\n",
              "      <td>0.111516</td>\n",
              "      <td>-0.143520</td>\n",
              "      <td>-0.060493</td>\n",
              "      <td>-0.076559</td>\n",
              "      <td>-0.071154</td>\n",
              "      <td>-0.068877</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tempmx</th>\n",
              "      <td>0.097844</td>\n",
              "      <td>-0.051258</td>\n",
              "      <td>-0.024103</td>\n",
              "      <td>0.431532</td>\n",
              "      <td>0.469594</td>\n",
              "      <td>0.496208</td>\n",
              "      <td>0.394287</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.527390</td>\n",
              "      <td>-0.227116</td>\n",
              "      <td>0.069491</td>\n",
              "      <td>-0.071949</td>\n",
              "      <td>-0.136529</td>\n",
              "      <td>0.034899</td>\n",
              "      <td>0.014403</td>\n",
              "      <td>0.051432</td>\n",
              "      <td>0.035630</td>\n",
              "      <td>0.090580</td>\n",
              "      <td>-0.157051</td>\n",
              "      <td>0.351404</td>\n",
              "      <td>-0.329648</td>\n",
              "      <td>-0.320015</td>\n",
              "      <td>-0.146520</td>\n",
              "      <td>0.142588</td>\n",
              "      <td>0.051015</td>\n",
              "      <td>-0.341797</td>\n",
              "      <td>-0.045540</td>\n",
              "      <td>-0.053798</td>\n",
              "      <td>-0.053513</td>\n",
              "      <td>0.088006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RHmx</th>\n",
              "      <td>-0.075519</td>\n",
              "      <td>0.085223</td>\n",
              "      <td>0.062221</td>\n",
              "      <td>-0.300995</td>\n",
              "      <td>0.073795</td>\n",
              "      <td>-0.039192</td>\n",
              "      <td>-0.132517</td>\n",
              "      <td>-0.527390</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.069410</td>\n",
              "      <td>0.099751</td>\n",
              "      <td>0.064506</td>\n",
              "      <td>0.009376</td>\n",
              "      <td>-0.023869</td>\n",
              "      <td>0.136220</td>\n",
              "      <td>-0.123061</td>\n",
              "      <td>-0.014211</td>\n",
              "      <td>-0.087508</td>\n",
              "      <td>0.021235</td>\n",
              "      <td>0.054761</td>\n",
              "      <td>-0.047714</td>\n",
              "      <td>0.140430</td>\n",
              "      <td>0.170923</td>\n",
              "      <td>0.013185</td>\n",
              "      <td>0.009382</td>\n",
              "      <td>-0.089836</td>\n",
              "      <td>0.086822</td>\n",
              "      <td>-0.035885</td>\n",
              "      <td>-0.072334</td>\n",
              "      <td>-0.062596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>windmx</th>\n",
              "      <td>0.012317</td>\n",
              "      <td>0.018798</td>\n",
              "      <td>-0.020341</td>\n",
              "      <td>-0.028485</td>\n",
              "      <td>-0.105342</td>\n",
              "      <td>-0.203466</td>\n",
              "      <td>0.106826</td>\n",
              "      <td>-0.227116</td>\n",
              "      <td>0.069410</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.061119</td>\n",
              "      <td>0.118090</td>\n",
              "      <td>-0.063881</td>\n",
              "      <td>-0.063799</td>\n",
              "      <td>0.027981</td>\n",
              "      <td>-0.062553</td>\n",
              "      <td>0.053396</td>\n",
              "      <td>-0.019965</td>\n",
              "      <td>0.048266</td>\n",
              "      <td>0.028577</td>\n",
              "      <td>0.269702</td>\n",
              "      <td>-0.029431</td>\n",
              "      <td>-0.070245</td>\n",
              "      <td>-0.040645</td>\n",
              "      <td>0.012124</td>\n",
              "      <td>0.181433</td>\n",
              "      <td>0.015054</td>\n",
              "      <td>0.011864</td>\n",
              "      <td>-0.053850</td>\n",
              "      <td>-0.181476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>rainmx</th>\n",
              "      <td>-0.007366</td>\n",
              "      <td>0.065387</td>\n",
              "      <td>0.033234</td>\n",
              "      <td>0.056702</td>\n",
              "      <td>0.074790</td>\n",
              "      <td>0.035861</td>\n",
              "      <td>0.067668</td>\n",
              "      <td>0.069491</td>\n",
              "      <td>0.099751</td>\n",
              "      <td>0.061119</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.004261</td>\n",
              "      <td>-0.029945</td>\n",
              "      <td>-0.032271</td>\n",
              "      <td>-0.017872</td>\n",
              "      <td>-0.026798</td>\n",
              "      <td>0.139311</td>\n",
              "      <td>-0.020744</td>\n",
              "      <td>-0.009752</td>\n",
              "      <td>0.093101</td>\n",
              "      <td>-0.009752</td>\n",
              "      <td>-0.014698</td>\n",
              "      <td>-0.004566</td>\n",
              "      <td>-0.013390</td>\n",
              "      <td>-0.013510</td>\n",
              "      <td>-0.020744</td>\n",
              "      <td>-0.004566</td>\n",
              "      <td>-0.003225</td>\n",
              "      <td>-0.012665</td>\n",
              "      <td>-0.051733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>day_0</th>\n",
              "      <td>-0.052911</td>\n",
              "      <td>-0.020061</td>\n",
              "      <td>-0.048773</td>\n",
              "      <td>0.019306</td>\n",
              "      <td>-0.012010</td>\n",
              "      <td>-0.004220</td>\n",
              "      <td>0.046695</td>\n",
              "      <td>-0.071949</td>\n",
              "      <td>0.064506</td>\n",
              "      <td>0.118090</td>\n",
              "      <td>-0.004261</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.181293</td>\n",
              "      <td>-0.195372</td>\n",
              "      <td>-0.210462</td>\n",
              "      <td>-0.162237</td>\n",
              "      <td>-0.166728</td>\n",
              "      <td>-0.151487</td>\n",
              "      <td>-0.019140</td>\n",
              "      <td>-0.100837</td>\n",
              "      <td>-0.019140</td>\n",
              "      <td>0.046323</td>\n",
              "      <td>-0.027643</td>\n",
              "      <td>-0.048969</td>\n",
              "      <td>0.006000</td>\n",
              "      <td>0.036205</td>\n",
              "      <td>0.056423</td>\n",
              "      <td>-0.019527</td>\n",
              "      <td>-0.045585</td>\n",
              "      <td>0.107671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>day_1</th>\n",
              "      <td>-0.021206</td>\n",
              "      <td>0.039367</td>\n",
              "      <td>0.021640</td>\n",
              "      <td>-0.059396</td>\n",
              "      <td>-0.107921</td>\n",
              "      <td>-0.052993</td>\n",
              "      <td>-0.158601</td>\n",
              "      <td>-0.136529</td>\n",
              "      <td>0.009376</td>\n",
              "      <td>-0.063881</td>\n",
              "      <td>-0.029945</td>\n",
              "      <td>-0.181293</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.180015</td>\n",
              "      <td>-0.193919</td>\n",
              "      <td>-0.149485</td>\n",
              "      <td>-0.153623</td>\n",
              "      <td>-0.139579</td>\n",
              "      <td>-0.012171</td>\n",
              "      <td>-0.130774</td>\n",
              "      <td>0.114519</td>\n",
              "      <td>0.003933</td>\n",
              "      <td>-0.025470</td>\n",
              "      <td>-0.013300</td>\n",
              "      <td>0.017553</td>\n",
              "      <td>0.077125</td>\n",
              "      <td>-0.025470</td>\n",
              "      <td>-0.017992</td>\n",
              "      <td>0.060975</td>\n",
              "      <td>0.039632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>day_2</th>\n",
              "      <td>0.087868</td>\n",
              "      <td>0.024461</td>\n",
              "      <td>0.003483</td>\n",
              "      <td>-0.019637</td>\n",
              "      <td>-0.003653</td>\n",
              "      <td>-0.035189</td>\n",
              "      <td>-0.038585</td>\n",
              "      <td>0.034899</td>\n",
              "      <td>-0.023869</td>\n",
              "      <td>-0.063799</td>\n",
              "      <td>-0.032271</td>\n",
              "      <td>-0.195372</td>\n",
              "      <td>-0.180015</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.208978</td>\n",
              "      <td>-0.161094</td>\n",
              "      <td>-0.165553</td>\n",
              "      <td>-0.150419</td>\n",
              "      <td>-0.018534</td>\n",
              "      <td>-0.009808</td>\n",
              "      <td>-0.058625</td>\n",
              "      <td>0.020406</td>\n",
              "      <td>0.057019</td>\n",
              "      <td>0.060945</td>\n",
              "      <td>-0.022408</td>\n",
              "      <td>0.021024</td>\n",
              "      <td>0.057019</td>\n",
              "      <td>-0.019390</td>\n",
              "      <td>0.017584</td>\n",
              "      <td>-0.032783</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>day_3</th>\n",
              "      <td>-0.020463</td>\n",
              "      <td>-0.029338</td>\n",
              "      <td>0.006172</td>\n",
              "      <td>-0.089517</td>\n",
              "      <td>0.025355</td>\n",
              "      <td>-0.001431</td>\n",
              "      <td>-0.003243</td>\n",
              "      <td>0.014403</td>\n",
              "      <td>0.136220</td>\n",
              "      <td>0.027981</td>\n",
              "      <td>-0.017872</td>\n",
              "      <td>-0.210462</td>\n",
              "      <td>-0.193919</td>\n",
              "      <td>-0.208978</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.173535</td>\n",
              "      <td>-0.178339</td>\n",
              "      <td>-0.162036</td>\n",
              "      <td>0.051409</td>\n",
              "      <td>0.064566</td>\n",
              "      <td>-0.024966</td>\n",
              "      <td>0.008416</td>\n",
              "      <td>0.050887</td>\n",
              "      <td>-0.018241</td>\n",
              "      <td>0.024540</td>\n",
              "      <td>-0.047726</td>\n",
              "      <td>-0.029568</td>\n",
              "      <td>-0.020887</td>\n",
              "      <td>0.007252</td>\n",
              "      <td>-0.048817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>day_4</th>\n",
              "      <td>0.020121</td>\n",
              "      <td>0.000457</td>\n",
              "      <td>0.027873</td>\n",
              "      <td>0.071730</td>\n",
              "      <td>0.087672</td>\n",
              "      <td>0.051859</td>\n",
              "      <td>-0.022406</td>\n",
              "      <td>0.051432</td>\n",
              "      <td>-0.123061</td>\n",
              "      <td>-0.062553</td>\n",
              "      <td>-0.026798</td>\n",
              "      <td>-0.162237</td>\n",
              "      <td>-0.149485</td>\n",
              "      <td>-0.161094</td>\n",
              "      <td>-0.173535</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.137475</td>\n",
              "      <td>-0.124908</td>\n",
              "      <td>0.043007</td>\n",
              "      <td>0.053726</td>\n",
              "      <td>-0.002838</td>\n",
              "      <td>-0.042278</td>\n",
              "      <td>-0.022793</td>\n",
              "      <td>-0.019300</td>\n",
              "      <td>-0.000195</td>\n",
              "      <td>-0.026885</td>\n",
              "      <td>-0.022793</td>\n",
              "      <td>-0.016101</td>\n",
              "      <td>-0.063223</td>\n",
              "      <td>0.008984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>day_5</th>\n",
              "      <td>-0.001333</td>\n",
              "      <td>-0.027521</td>\n",
              "      <td>-0.053475</td>\n",
              "      <td>0.011225</td>\n",
              "      <td>0.000016</td>\n",
              "      <td>0.028368</td>\n",
              "      <td>0.068610</td>\n",
              "      <td>0.035630</td>\n",
              "      <td>-0.014211</td>\n",
              "      <td>0.053396</td>\n",
              "      <td>0.139311</td>\n",
              "      <td>-0.166728</td>\n",
              "      <td>-0.153623</td>\n",
              "      <td>-0.165553</td>\n",
              "      <td>-0.178339</td>\n",
              "      <td>-0.137475</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.128365</td>\n",
              "      <td>-0.050030</td>\n",
              "      <td>0.064061</td>\n",
              "      <td>-0.005125</td>\n",
              "      <td>-0.014491</td>\n",
              "      <td>-0.023424</td>\n",
              "      <td>0.049688</td>\n",
              "      <td>-0.069308</td>\n",
              "      <td>-0.032351</td>\n",
              "      <td>-0.023424</td>\n",
              "      <td>0.117121</td>\n",
              "      <td>0.005008</td>\n",
              "      <td>-0.028570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>day_6</th>\n",
              "      <td>-0.011452</td>\n",
              "      <td>0.016035</td>\n",
              "      <td>0.050495</td>\n",
              "      <td>0.093908</td>\n",
              "      <td>0.017939</td>\n",
              "      <td>0.024803</td>\n",
              "      <td>0.125415</td>\n",
              "      <td>0.090580</td>\n",
              "      <td>-0.087508</td>\n",
              "      <td>-0.019965</td>\n",
              "      <td>-0.020744</td>\n",
              "      <td>-0.151487</td>\n",
              "      <td>-0.139579</td>\n",
              "      <td>-0.150419</td>\n",
              "      <td>-0.162036</td>\n",
              "      <td>-0.124908</td>\n",
              "      <td>-0.128365</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.002899</td>\n",
              "      <td>0.076367</td>\n",
              "      <td>0.002899</td>\n",
              "      <td>-0.035713</td>\n",
              "      <td>-0.021282</td>\n",
              "      <td>-0.008985</td>\n",
              "      <td>0.043422</td>\n",
              "      <td>-0.033917</td>\n",
              "      <td>-0.021282</td>\n",
              "      <td>-0.015034</td>\n",
              "      <td>0.016325</td>\n",
              "      <td>-0.053222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>month_0</th>\n",
              "      <td>-0.008280</td>\n",
              "      <td>0.063832</td>\n",
              "      <td>-0.008405</td>\n",
              "      <td>-0.117199</td>\n",
              "      <td>-0.197543</td>\n",
              "      <td>-0.268211</td>\n",
              "      <td>-0.106478</td>\n",
              "      <td>-0.157051</td>\n",
              "      <td>0.021235</td>\n",
              "      <td>0.048266</td>\n",
              "      <td>-0.009752</td>\n",
              "      <td>-0.019140</td>\n",
              "      <td>-0.012171</td>\n",
              "      <td>-0.018534</td>\n",
              "      <td>0.051409</td>\n",
              "      <td>0.043007</td>\n",
              "      <td>-0.050030</td>\n",
              "      <td>0.002899</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.098941</td>\n",
              "      <td>-0.017717</td>\n",
              "      <td>-0.026701</td>\n",
              "      <td>-0.008295</td>\n",
              "      <td>-0.034190</td>\n",
              "      <td>-0.024543</td>\n",
              "      <td>-0.045456</td>\n",
              "      <td>-0.008295</td>\n",
              "      <td>-0.005860</td>\n",
              "      <td>-0.023008</td>\n",
              "      <td>-0.093982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>month_1</th>\n",
              "      <td>-0.004187</td>\n",
              "      <td>-0.059669</td>\n",
              "      <td>-0.010404</td>\n",
              "      <td>0.228103</td>\n",
              "      <td>0.497928</td>\n",
              "      <td>0.279361</td>\n",
              "      <td>0.334639</td>\n",
              "      <td>0.351404</td>\n",
              "      <td>0.054761</td>\n",
              "      <td>0.028577</td>\n",
              "      <td>0.093101</td>\n",
              "      <td>-0.100837</td>\n",
              "      <td>-0.130774</td>\n",
              "      <td>-0.009808</td>\n",
              "      <td>0.064566</td>\n",
              "      <td>0.053726</td>\n",
              "      <td>0.064061</td>\n",
              "      <td>0.076367</td>\n",
              "      <td>-0.098941</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.098941</td>\n",
              "      <td>-0.149116</td>\n",
              "      <td>-0.046323</td>\n",
              "      <td>-0.190937</td>\n",
              "      <td>-0.137065</td>\n",
              "      <td>-0.253859</td>\n",
              "      <td>-0.046323</td>\n",
              "      <td>-0.032724</td>\n",
              "      <td>-0.128493</td>\n",
              "      <td>-0.524858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>month_2</th>\n",
              "      <td>0.001010</td>\n",
              "      <td>-0.006547</td>\n",
              "      <td>0.075850</td>\n",
              "      <td>-0.137044</td>\n",
              "      <td>-0.176301</td>\n",
              "      <td>-0.105642</td>\n",
              "      <td>-0.162322</td>\n",
              "      <td>-0.329648</td>\n",
              "      <td>-0.047714</td>\n",
              "      <td>0.269702</td>\n",
              "      <td>-0.009752</td>\n",
              "      <td>-0.019140</td>\n",
              "      <td>0.114519</td>\n",
              "      <td>-0.058625</td>\n",
              "      <td>-0.024966</td>\n",
              "      <td>-0.002838</td>\n",
              "      <td>-0.005125</td>\n",
              "      <td>0.002899</td>\n",
              "      <td>-0.017717</td>\n",
              "      <td>-0.098941</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.026701</td>\n",
              "      <td>-0.008295</td>\n",
              "      <td>-0.034190</td>\n",
              "      <td>-0.024543</td>\n",
              "      <td>-0.045456</td>\n",
              "      <td>-0.008295</td>\n",
              "      <td>-0.005860</td>\n",
              "      <td>-0.023008</td>\n",
              "      <td>-0.093982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>month_3</th>\n",
              "      <td>-0.020732</td>\n",
              "      <td>0.041721</td>\n",
              "      <td>0.016358</td>\n",
              "      <td>-0.281535</td>\n",
              "      <td>-0.317899</td>\n",
              "      <td>-0.399277</td>\n",
              "      <td>-0.249777</td>\n",
              "      <td>-0.320015</td>\n",
              "      <td>0.140430</td>\n",
              "      <td>-0.029431</td>\n",
              "      <td>-0.014698</td>\n",
              "      <td>0.046323</td>\n",
              "      <td>0.003933</td>\n",
              "      <td>0.020406</td>\n",
              "      <td>0.008416</td>\n",
              "      <td>-0.042278</td>\n",
              "      <td>-0.014491</td>\n",
              "      <td>-0.035713</td>\n",
              "      <td>-0.026701</td>\n",
              "      <td>-0.149116</td>\n",
              "      <td>-0.026701</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.012501</td>\n",
              "      <td>-0.051528</td>\n",
              "      <td>-0.036989</td>\n",
              "      <td>-0.068508</td>\n",
              "      <td>-0.012501</td>\n",
              "      <td>-0.008831</td>\n",
              "      <td>-0.034676</td>\n",
              "      <td>-0.141642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>month_4</th>\n",
              "      <td>-0.012589</td>\n",
              "      <td>-0.045002</td>\n",
              "      <td>0.010153</td>\n",
              "      <td>-0.454771</td>\n",
              "      <td>-0.105647</td>\n",
              "      <td>-0.115064</td>\n",
              "      <td>-0.103588</td>\n",
              "      <td>-0.146520</td>\n",
              "      <td>0.170923</td>\n",
              "      <td>-0.070245</td>\n",
              "      <td>-0.004566</td>\n",
              "      <td>-0.027643</td>\n",
              "      <td>-0.025470</td>\n",
              "      <td>0.057019</td>\n",
              "      <td>0.050887</td>\n",
              "      <td>-0.022793</td>\n",
              "      <td>-0.023424</td>\n",
              "      <td>-0.021282</td>\n",
              "      <td>-0.008295</td>\n",
              "      <td>-0.046323</td>\n",
              "      <td>-0.008295</td>\n",
              "      <td>-0.012501</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.016007</td>\n",
              "      <td>-0.011491</td>\n",
              "      <td>-0.021282</td>\n",
              "      <td>-0.003883</td>\n",
              "      <td>-0.002743</td>\n",
              "      <td>-0.010772</td>\n",
              "      <td>-0.044001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>month_5</th>\n",
              "      <td>0.006149</td>\n",
              "      <td>0.061062</td>\n",
              "      <td>0.061449</td>\n",
              "      <td>0.031833</td>\n",
              "      <td>-0.001946</td>\n",
              "      <td>-0.100887</td>\n",
              "      <td>0.020982</td>\n",
              "      <td>0.142588</td>\n",
              "      <td>0.013185</td>\n",
              "      <td>-0.040645</td>\n",
              "      <td>-0.013390</td>\n",
              "      <td>-0.048969</td>\n",
              "      <td>-0.013300</td>\n",
              "      <td>0.060945</td>\n",
              "      <td>-0.018241</td>\n",
              "      <td>-0.019300</td>\n",
              "      <td>0.049688</td>\n",
              "      <td>-0.008985</td>\n",
              "      <td>-0.034190</td>\n",
              "      <td>-0.190937</td>\n",
              "      <td>-0.034190</td>\n",
              "      <td>-0.051528</td>\n",
              "      <td>-0.016007</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.047363</td>\n",
              "      <td>-0.087722</td>\n",
              "      <td>-0.016007</td>\n",
              "      <td>-0.011308</td>\n",
              "      <td>-0.044402</td>\n",
              "      <td>-0.181367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>month_6</th>\n",
              "      <td>-0.020314</td>\n",
              "      <td>0.129616</td>\n",
              "      <td>0.078594</td>\n",
              "      <td>-0.040634</td>\n",
              "      <td>-0.050403</td>\n",
              "      <td>-0.186183</td>\n",
              "      <td>0.111516</td>\n",
              "      <td>0.051015</td>\n",
              "      <td>0.009382</td>\n",
              "      <td>0.012124</td>\n",
              "      <td>-0.013510</td>\n",
              "      <td>0.006000</td>\n",
              "      <td>0.017553</td>\n",
              "      <td>-0.022408</td>\n",
              "      <td>0.024540</td>\n",
              "      <td>-0.000195</td>\n",
              "      <td>-0.069308</td>\n",
              "      <td>0.043422</td>\n",
              "      <td>-0.024543</td>\n",
              "      <td>-0.137065</td>\n",
              "      <td>-0.024543</td>\n",
              "      <td>-0.036989</td>\n",
              "      <td>-0.011491</td>\n",
              "      <td>-0.047363</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.062972</td>\n",
              "      <td>-0.011491</td>\n",
              "      <td>-0.008117</td>\n",
              "      <td>-0.031874</td>\n",
              "      <td>-0.130195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>month_7</th>\n",
              "      <td>-0.045596</td>\n",
              "      <td>0.007827</td>\n",
              "      <td>0.050495</td>\n",
              "      <td>-0.074327</td>\n",
              "      <td>-0.407404</td>\n",
              "      <td>-0.650427</td>\n",
              "      <td>-0.143520</td>\n",
              "      <td>-0.341797</td>\n",
              "      <td>-0.089836</td>\n",
              "      <td>0.181433</td>\n",
              "      <td>-0.020744</td>\n",
              "      <td>0.036205</td>\n",
              "      <td>0.077125</td>\n",
              "      <td>0.021024</td>\n",
              "      <td>-0.047726</td>\n",
              "      <td>-0.026885</td>\n",
              "      <td>-0.032351</td>\n",
              "      <td>-0.033917</td>\n",
              "      <td>-0.045456</td>\n",
              "      <td>-0.253859</td>\n",
              "      <td>-0.045456</td>\n",
              "      <td>-0.068508</td>\n",
              "      <td>-0.021282</td>\n",
              "      <td>-0.087722</td>\n",
              "      <td>-0.062972</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.021282</td>\n",
              "      <td>-0.015034</td>\n",
              "      <td>-0.059034</td>\n",
              "      <td>-0.241135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>month_8</th>\n",
              "      <td>0.006264</td>\n",
              "      <td>0.008917</td>\n",
              "      <td>-0.015206</td>\n",
              "      <td>-0.037230</td>\n",
              "      <td>-0.081980</td>\n",
              "      <td>-0.114209</td>\n",
              "      <td>-0.060493</td>\n",
              "      <td>-0.045540</td>\n",
              "      <td>0.086822</td>\n",
              "      <td>0.015054</td>\n",
              "      <td>-0.004566</td>\n",
              "      <td>0.056423</td>\n",
              "      <td>-0.025470</td>\n",
              "      <td>0.057019</td>\n",
              "      <td>-0.029568</td>\n",
              "      <td>-0.022793</td>\n",
              "      <td>-0.023424</td>\n",
              "      <td>-0.021282</td>\n",
              "      <td>-0.008295</td>\n",
              "      <td>-0.046323</td>\n",
              "      <td>-0.008295</td>\n",
              "      <td>-0.012501</td>\n",
              "      <td>-0.003883</td>\n",
              "      <td>-0.016007</td>\n",
              "      <td>-0.011491</td>\n",
              "      <td>-0.021282</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.002743</td>\n",
              "      <td>-0.010772</td>\n",
              "      <td>-0.044001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>month_9</th>\n",
              "      <td>-0.008893</td>\n",
              "      <td>0.025344</td>\n",
              "      <td>-0.046570</td>\n",
              "      <td>-0.088964</td>\n",
              "      <td>-0.074218</td>\n",
              "      <td>-0.078380</td>\n",
              "      <td>-0.076559</td>\n",
              "      <td>-0.053798</td>\n",
              "      <td>-0.035885</td>\n",
              "      <td>0.011864</td>\n",
              "      <td>-0.003225</td>\n",
              "      <td>-0.019527</td>\n",
              "      <td>-0.017992</td>\n",
              "      <td>-0.019390</td>\n",
              "      <td>-0.020887</td>\n",
              "      <td>-0.016101</td>\n",
              "      <td>0.117121</td>\n",
              "      <td>-0.015034</td>\n",
              "      <td>-0.005860</td>\n",
              "      <td>-0.032724</td>\n",
              "      <td>-0.005860</td>\n",
              "      <td>-0.008831</td>\n",
              "      <td>-0.002743</td>\n",
              "      <td>-0.011308</td>\n",
              "      <td>-0.008117</td>\n",
              "      <td>-0.015034</td>\n",
              "      <td>-0.002743</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.007610</td>\n",
              "      <td>-0.031083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>month_10</th>\n",
              "      <td>-0.016878</td>\n",
              "      <td>0.089545</td>\n",
              "      <td>0.004717</td>\n",
              "      <td>-0.005998</td>\n",
              "      <td>-0.187632</td>\n",
              "      <td>0.093279</td>\n",
              "      <td>-0.071154</td>\n",
              "      <td>-0.053513</td>\n",
              "      <td>-0.072334</td>\n",
              "      <td>-0.053850</td>\n",
              "      <td>-0.012665</td>\n",
              "      <td>-0.045585</td>\n",
              "      <td>0.060975</td>\n",
              "      <td>0.017584</td>\n",
              "      <td>0.007252</td>\n",
              "      <td>-0.063223</td>\n",
              "      <td>0.005008</td>\n",
              "      <td>0.016325</td>\n",
              "      <td>-0.023008</td>\n",
              "      <td>-0.128493</td>\n",
              "      <td>-0.023008</td>\n",
              "      <td>-0.034676</td>\n",
              "      <td>-0.010772</td>\n",
              "      <td>-0.044402</td>\n",
              "      <td>-0.031874</td>\n",
              "      <td>-0.059034</td>\n",
              "      <td>-0.010772</td>\n",
              "      <td>-0.007610</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.122053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>month_11</th>\n",
              "      <td>0.056573</td>\n",
              "      <td>-0.087216</td>\n",
              "      <td>-0.105465</td>\n",
              "      <td>0.076609</td>\n",
              "      <td>0.110907</td>\n",
              "      <td>0.531857</td>\n",
              "      <td>-0.068877</td>\n",
              "      <td>0.088006</td>\n",
              "      <td>-0.062596</td>\n",
              "      <td>-0.181476</td>\n",
              "      <td>-0.051733</td>\n",
              "      <td>0.107671</td>\n",
              "      <td>0.039632</td>\n",
              "      <td>-0.032783</td>\n",
              "      <td>-0.048817</td>\n",
              "      <td>0.008984</td>\n",
              "      <td>-0.028570</td>\n",
              "      <td>-0.053222</td>\n",
              "      <td>-0.093982</td>\n",
              "      <td>-0.524858</td>\n",
              "      <td>-0.093982</td>\n",
              "      <td>-0.141642</td>\n",
              "      <td>-0.044001</td>\n",
              "      <td>-0.181367</td>\n",
              "      <td>-0.130195</td>\n",
              "      <td>-0.241135</td>\n",
              "      <td>-0.044001</td>\n",
              "      <td>-0.031083</td>\n",
              "      <td>-0.122053</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e0688c9f-335a-4365-9780-6a6f36acc4f0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e0688c9f-335a-4365-9780-6a6f36acc4f0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e0688c9f-335a-4365-9780-6a6f36acc4f0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "            areamx       Xmx       Ymx  ...   month_9  month_10  month_11\n",
              "areamx    1.000000  0.063385  0.044873  ... -0.008893 -0.016878  0.056573\n",
              "Xmx       0.063385  1.000000  0.539548  ...  0.025344  0.089545 -0.087216\n",
              "Ymx       0.044873  0.539548  1.000000  ... -0.046570  0.004717 -0.105465\n",
              "FFMCmx    0.040122 -0.021039 -0.046308  ... -0.088964 -0.005998  0.076609\n",
              "DMCmx     0.072994 -0.048384  0.007782  ... -0.074218 -0.187632  0.110907\n",
              "DCmx      0.049383 -0.085916 -0.101178  ... -0.078380  0.093279  0.531857\n",
              "ISImx     0.008258  0.006210 -0.024488  ... -0.076559 -0.071154 -0.068877\n",
              "tempmx    0.097844 -0.051258 -0.024103  ... -0.053798 -0.053513  0.088006\n",
              "RHmx     -0.075519  0.085223  0.062221  ... -0.035885 -0.072334 -0.062596\n",
              "windmx    0.012317  0.018798 -0.020341  ...  0.011864 -0.053850 -0.181476\n",
              "rainmx   -0.007366  0.065387  0.033234  ... -0.003225 -0.012665 -0.051733\n",
              "day_0    -0.052911 -0.020061 -0.048773  ... -0.019527 -0.045585  0.107671\n",
              "day_1    -0.021206  0.039367  0.021640  ... -0.017992  0.060975  0.039632\n",
              "day_2     0.087868  0.024461  0.003483  ... -0.019390  0.017584 -0.032783\n",
              "day_3    -0.020463 -0.029338  0.006172  ... -0.020887  0.007252 -0.048817\n",
              "day_4     0.020121  0.000457  0.027873  ... -0.016101 -0.063223  0.008984\n",
              "day_5    -0.001333 -0.027521 -0.053475  ...  0.117121  0.005008 -0.028570\n",
              "day_6    -0.011452  0.016035  0.050495  ... -0.015034  0.016325 -0.053222\n",
              "month_0  -0.008280  0.063832 -0.008405  ... -0.005860 -0.023008 -0.093982\n",
              "month_1  -0.004187 -0.059669 -0.010404  ... -0.032724 -0.128493 -0.524858\n",
              "month_2   0.001010 -0.006547  0.075850  ... -0.005860 -0.023008 -0.093982\n",
              "month_3  -0.020732  0.041721  0.016358  ... -0.008831 -0.034676 -0.141642\n",
              "month_4  -0.012589 -0.045002  0.010153  ... -0.002743 -0.010772 -0.044001\n",
              "month_5   0.006149  0.061062  0.061449  ... -0.011308 -0.044402 -0.181367\n",
              "month_6  -0.020314  0.129616  0.078594  ... -0.008117 -0.031874 -0.130195\n",
              "month_7  -0.045596  0.007827  0.050495  ... -0.015034 -0.059034 -0.241135\n",
              "month_8   0.006264  0.008917 -0.015206  ... -0.002743 -0.010772 -0.044001\n",
              "month_9  -0.008893  0.025344 -0.046570  ...  1.000000 -0.007610 -0.031083\n",
              "month_10 -0.016878  0.089545  0.004717  ... -0.007610  1.000000 -0.122053\n",
              "month_11  0.056573 -0.087216 -0.105465  ... -0.031083 -0.122053  1.000000\n",
              "\n",
              "[30 rows x 30 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#new_data.month_jan.value_counts()"
      ],
      "metadata": {
        "id": "1eSFxYdtwh8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0KK9-Xfjg6v",
        "outputId": "85e84992-b4e2-40f0-add0-450914e70596"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(517, 30)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fitur = new_data.drop(columns='areamx').values\n",
        "label = new_data['areamx']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(fitur, label, test_size=0.3, random_state=20)"
      ],
      "metadata": {
        "id": "Jvmj60fJjhCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nYxO4-jNlBf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# model 1"
      ],
      "metadata": {
        "id": "on1Cjy3Zq73D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(15, input_dim=29)) \n",
        "model.add(Dense(10)) \n",
        "model.add(Dense(1, activation = 'linear'))\n",
        "\n",
        "model.compile(loss=\"mean_absolute_error\", optimizer=optimizers.SGD(learning_rate=0.001), metrics=['accuracy','mae', 'mse'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7M6rGBqllCPH",
        "outputId": "9bd2ec84-eb3c-4166-fcfd-a81c1a7a51b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_50 (Dense)            (None, 15)                450       \n",
            "                                                                 \n",
            " dense_51 (Dense)            (None, 10)                160       \n",
            "                                                                 \n",
            " dense_52 (Dense)            (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 621\n",
            "Trainable params: 621\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=200, batch_size=32, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uw1OBMzW7hAb",
        "outputId": "4d043e85-ca8f-4122-aa96-74e723daef53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "12/12 [==============================] - 1s 18ms/step - loss: 0.5420 - accuracy: 0.4183 - mae: 0.5420 - mse: 0.4292 - val_loss: 0.5200 - val_accuracy: 0.4295 - val_mae: 0.5200 - val_mse: 0.4040\n",
            "Epoch 2/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.5129 - accuracy: 0.4017 - mae: 0.5129 - mse: 0.3873 - val_loss: 0.4928 - val_accuracy: 0.4295 - val_mae: 0.4928 - val_mse: 0.3683\n",
            "Epoch 3/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.4928 - accuracy: 0.3961 - mae: 0.4928 - mse: 0.3592 - val_loss: 0.4709 - val_accuracy: 0.4167 - val_mae: 0.4709 - val_mse: 0.3413\n",
            "Epoch 4/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.4752 - accuracy: 0.3961 - mae: 0.4752 - mse: 0.3375 - val_loss: 0.4521 - val_accuracy: 0.4167 - val_mae: 0.4521 - val_mse: 0.3181\n",
            "Epoch 5/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.4612 - accuracy: 0.3906 - mae: 0.4612 - mse: 0.3200 - val_loss: 0.4411 - val_accuracy: 0.4167 - val_mae: 0.4411 - val_mse: 0.3032\n",
            "Epoch 6/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.4514 - accuracy: 0.3878 - mae: 0.4514 - mse: 0.3080 - val_loss: 0.4324 - val_accuracy: 0.4167 - val_mae: 0.4324 - val_mse: 0.2915\n",
            "Epoch 7/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.4429 - accuracy: 0.3878 - mae: 0.4429 - mse: 0.2970 - val_loss: 0.4239 - val_accuracy: 0.4103 - val_mae: 0.4239 - val_mse: 0.2802\n",
            "Epoch 8/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.4346 - accuracy: 0.3878 - mae: 0.4346 - mse: 0.2864 - val_loss: 0.4171 - val_accuracy: 0.4167 - val_mae: 0.4171 - val_mse: 0.2717\n",
            "Epoch 9/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.4271 - accuracy: 0.3934 - mae: 0.4271 - mse: 0.2768 - val_loss: 0.4096 - val_accuracy: 0.4167 - val_mae: 0.4096 - val_mse: 0.2617\n",
            "Epoch 10/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.4197 - accuracy: 0.3934 - mae: 0.4197 - mse: 0.2677 - val_loss: 0.4025 - val_accuracy: 0.4167 - val_mae: 0.4025 - val_mse: 0.2529\n",
            "Epoch 11/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.4122 - accuracy: 0.3934 - mae: 0.4122 - mse: 0.2585 - val_loss: 0.3952 - val_accuracy: 0.4167 - val_mae: 0.3952 - val_mse: 0.2440\n",
            "Epoch 12/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.4049 - accuracy: 0.3934 - mae: 0.4049 - mse: 0.2499 - val_loss: 0.3884 - val_accuracy: 0.4167 - val_mae: 0.3884 - val_mse: 0.2358\n",
            "Epoch 13/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.3976 - accuracy: 0.3934 - mae: 0.3976 - mse: 0.2415 - val_loss: 0.3817 - val_accuracy: 0.4167 - val_mae: 0.3817 - val_mse: 0.2281\n",
            "Epoch 14/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.3906 - accuracy: 0.3934 - mae: 0.3906 - mse: 0.2334 - val_loss: 0.3754 - val_accuracy: 0.4231 - val_mae: 0.3754 - val_mse: 0.2210\n",
            "Epoch 15/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.3839 - accuracy: 0.3934 - mae: 0.3839 - mse: 0.2257 - val_loss: 0.3692 - val_accuracy: 0.4231 - val_mae: 0.3692 - val_mse: 0.2142\n",
            "Epoch 16/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.3775 - accuracy: 0.3961 - mae: 0.3775 - mse: 0.2184 - val_loss: 0.3631 - val_accuracy: 0.4231 - val_mae: 0.3631 - val_mse: 0.2074\n",
            "Epoch 17/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.3711 - accuracy: 0.3934 - mae: 0.3711 - mse: 0.2116 - val_loss: 0.3572 - val_accuracy: 0.4231 - val_mae: 0.3572 - val_mse: 0.2011\n",
            "Epoch 18/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.3649 - accuracy: 0.3961 - mae: 0.3649 - mse: 0.2046 - val_loss: 0.3514 - val_accuracy: 0.4295 - val_mae: 0.3514 - val_mse: 0.1950\n",
            "Epoch 19/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.3589 - accuracy: 0.3989 - mae: 0.3589 - mse: 0.1986 - val_loss: 0.3459 - val_accuracy: 0.4295 - val_mae: 0.3459 - val_mse: 0.1892\n",
            "Epoch 20/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.3530 - accuracy: 0.4017 - mae: 0.3530 - mse: 0.1920 - val_loss: 0.3402 - val_accuracy: 0.4295 - val_mae: 0.3402 - val_mse: 0.1834\n",
            "Epoch 21/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.3474 - accuracy: 0.4044 - mae: 0.3474 - mse: 0.1860 - val_loss: 0.3350 - val_accuracy: 0.4359 - val_mae: 0.3350 - val_mse: 0.1782\n",
            "Epoch 22/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.3420 - accuracy: 0.4072 - mae: 0.3420 - mse: 0.1803 - val_loss: 0.3297 - val_accuracy: 0.4359 - val_mae: 0.3297 - val_mse: 0.1729\n",
            "Epoch 23/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.3364 - accuracy: 0.4072 - mae: 0.3364 - mse: 0.1747 - val_loss: 0.3243 - val_accuracy: 0.4359 - val_mae: 0.3243 - val_mse: 0.1677\n",
            "Epoch 24/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.3309 - accuracy: 0.4100 - mae: 0.3309 - mse: 0.1693 - val_loss: 0.3191 - val_accuracy: 0.4359 - val_mae: 0.3191 - val_mse: 0.1626\n",
            "Epoch 25/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.3257 - accuracy: 0.4100 - mae: 0.3257 - mse: 0.1642 - val_loss: 0.3142 - val_accuracy: 0.4359 - val_mae: 0.3142 - val_mse: 0.1581\n",
            "Epoch 26/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.3205 - accuracy: 0.4155 - mae: 0.3205 - mse: 0.1590 - val_loss: 0.3095 - val_accuracy: 0.4359 - val_mae: 0.3095 - val_mse: 0.1539\n",
            "Epoch 27/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.3155 - accuracy: 0.4183 - mae: 0.3155 - mse: 0.1543 - val_loss: 0.3046 - val_accuracy: 0.4359 - val_mae: 0.3046 - val_mse: 0.1495\n",
            "Epoch 28/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.3105 - accuracy: 0.4183 - mae: 0.3105 - mse: 0.1498 - val_loss: 0.2999 - val_accuracy: 0.4359 - val_mae: 0.2999 - val_mse: 0.1453\n",
            "Epoch 29/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.3057 - accuracy: 0.4211 - mae: 0.3057 - mse: 0.1455 - val_loss: 0.2951 - val_accuracy: 0.4423 - val_mae: 0.2951 - val_mse: 0.1411\n",
            "Epoch 30/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.3007 - accuracy: 0.4238 - mae: 0.3007 - mse: 0.1410 - val_loss: 0.2906 - val_accuracy: 0.4423 - val_mae: 0.2906 - val_mse: 0.1373\n",
            "Epoch 31/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2963 - accuracy: 0.4404 - mae: 0.2963 - mse: 0.1371 - val_loss: 0.2860 - val_accuracy: 0.4423 - val_mae: 0.2860 - val_mse: 0.1335\n",
            "Epoch 32/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2915 - accuracy: 0.4404 - mae: 0.2915 - mse: 0.1329 - val_loss: 0.2816 - val_accuracy: 0.4423 - val_mae: 0.2816 - val_mse: 0.1298\n",
            "Epoch 33/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2869 - accuracy: 0.4404 - mae: 0.2869 - mse: 0.1290 - val_loss: 0.2780 - val_accuracy: 0.4423 - val_mae: 0.2780 - val_mse: 0.1267\n",
            "Epoch 34/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2823 - accuracy: 0.4488 - mae: 0.2823 - mse: 0.1250 - val_loss: 0.2733 - val_accuracy: 0.4423 - val_mae: 0.2733 - val_mse: 0.1230\n",
            "Epoch 35/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2777 - accuracy: 0.4460 - mae: 0.2777 - mse: 0.1214 - val_loss: 0.2691 - val_accuracy: 0.4423 - val_mae: 0.2691 - val_mse: 0.1196\n",
            "Epoch 36/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.2733 - accuracy: 0.4488 - mae: 0.2733 - mse: 0.1178 - val_loss: 0.2648 - val_accuracy: 0.4551 - val_mae: 0.2648 - val_mse: 0.1163\n",
            "Epoch 37/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2689 - accuracy: 0.4488 - mae: 0.2689 - mse: 0.1143 - val_loss: 0.2610 - val_accuracy: 0.4551 - val_mae: 0.2610 - val_mse: 0.1133\n",
            "Epoch 38/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.2649 - accuracy: 0.4515 - mae: 0.2649 - mse: 0.1111 - val_loss: 0.2572 - val_accuracy: 0.4615 - val_mae: 0.2572 - val_mse: 0.1104\n",
            "Epoch 39/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.2610 - accuracy: 0.4543 - mae: 0.2610 - mse: 0.1081 - val_loss: 0.2532 - val_accuracy: 0.4615 - val_mae: 0.2532 - val_mse: 0.1074\n",
            "Epoch 40/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.2568 - accuracy: 0.4543 - mae: 0.2568 - mse: 0.1049 - val_loss: 0.2498 - val_accuracy: 0.4679 - val_mae: 0.2498 - val_mse: 0.1049\n",
            "Epoch 41/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2530 - accuracy: 0.4571 - mae: 0.2530 - mse: 0.1020 - val_loss: 0.2458 - val_accuracy: 0.4679 - val_mae: 0.2458 - val_mse: 0.1019\n",
            "Epoch 42/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2490 - accuracy: 0.4571 - mae: 0.2490 - mse: 0.0992 - val_loss: 0.2420 - val_accuracy: 0.4679 - val_mae: 0.2420 - val_mse: 0.0992\n",
            "Epoch 43/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.2453 - accuracy: 0.4571 - mae: 0.2453 - mse: 0.0966 - val_loss: 0.2389 - val_accuracy: 0.4679 - val_mae: 0.2389 - val_mse: 0.0971\n",
            "Epoch 44/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2415 - accuracy: 0.4571 - mae: 0.2415 - mse: 0.0938 - val_loss: 0.2357 - val_accuracy: 0.4679 - val_mae: 0.2357 - val_mse: 0.0948\n",
            "Epoch 45/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2381 - accuracy: 0.4598 - mae: 0.2381 - mse: 0.0915 - val_loss: 0.2326 - val_accuracy: 0.4744 - val_mae: 0.2326 - val_mse: 0.0926\n",
            "Epoch 46/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2347 - accuracy: 0.4598 - mae: 0.2347 - mse: 0.0891 - val_loss: 0.2292 - val_accuracy: 0.4744 - val_mae: 0.2292 - val_mse: 0.0903\n",
            "Epoch 47/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2313 - accuracy: 0.4626 - mae: 0.2313 - mse: 0.0867 - val_loss: 0.2258 - val_accuracy: 0.4744 - val_mae: 0.2258 - val_mse: 0.0881\n",
            "Epoch 48/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2279 - accuracy: 0.4654 - mae: 0.2279 - mse: 0.0844 - val_loss: 0.2226 - val_accuracy: 0.4744 - val_mae: 0.2226 - val_mse: 0.0860\n",
            "Epoch 49/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2246 - accuracy: 0.4654 - mae: 0.2246 - mse: 0.0823 - val_loss: 0.2198 - val_accuracy: 0.4744 - val_mae: 0.2198 - val_mse: 0.0841\n",
            "Epoch 50/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.2212 - accuracy: 0.4654 - mae: 0.2212 - mse: 0.0801 - val_loss: 0.2169 - val_accuracy: 0.4744 - val_mae: 0.2169 - val_mse: 0.0822\n",
            "Epoch 51/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2180 - accuracy: 0.4654 - mae: 0.2180 - mse: 0.0781 - val_loss: 0.2136 - val_accuracy: 0.4744 - val_mae: 0.2136 - val_mse: 0.0802\n",
            "Epoch 52/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.2149 - accuracy: 0.4654 - mae: 0.2149 - mse: 0.0761 - val_loss: 0.2108 - val_accuracy: 0.4744 - val_mae: 0.2108 - val_mse: 0.0784\n",
            "Epoch 53/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2117 - accuracy: 0.4709 - mae: 0.2117 - mse: 0.0741 - val_loss: 0.2080 - val_accuracy: 0.4808 - val_mae: 0.2080 - val_mse: 0.0768\n",
            "Epoch 54/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.2086 - accuracy: 0.4709 - mae: 0.2086 - mse: 0.0722 - val_loss: 0.2048 - val_accuracy: 0.4808 - val_mae: 0.2048 - val_mse: 0.0748\n",
            "Epoch 55/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2057 - accuracy: 0.4709 - mae: 0.2057 - mse: 0.0705 - val_loss: 0.2017 - val_accuracy: 0.4808 - val_mae: 0.2017 - val_mse: 0.0729\n",
            "Epoch 56/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2025 - accuracy: 0.4709 - mae: 0.2025 - mse: 0.0687 - val_loss: 0.1991 - val_accuracy: 0.4808 - val_mae: 0.1991 - val_mse: 0.0714\n",
            "Epoch 57/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1996 - accuracy: 0.4709 - mae: 0.1996 - mse: 0.0669 - val_loss: 0.1968 - val_accuracy: 0.4808 - val_mae: 0.1968 - val_mse: 0.0701\n",
            "Epoch 58/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1967 - accuracy: 0.4737 - mae: 0.1967 - mse: 0.0653 - val_loss: 0.1943 - val_accuracy: 0.4808 - val_mae: 0.1943 - val_mse: 0.0687\n",
            "Epoch 59/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1937 - accuracy: 0.4765 - mae: 0.1937 - mse: 0.0636 - val_loss: 0.1916 - val_accuracy: 0.4808 - val_mae: 0.1916 - val_mse: 0.0673\n",
            "Epoch 60/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1909 - accuracy: 0.4765 - mae: 0.1909 - mse: 0.0620 - val_loss: 0.1884 - val_accuracy: 0.4808 - val_mae: 0.1884 - val_mse: 0.0654\n",
            "Epoch 61/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1883 - accuracy: 0.4765 - mae: 0.1883 - mse: 0.0606 - val_loss: 0.1860 - val_accuracy: 0.4808 - val_mae: 0.1860 - val_mse: 0.0642\n",
            "Epoch 62/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1856 - accuracy: 0.4765 - mae: 0.1856 - mse: 0.0592 - val_loss: 0.1839 - val_accuracy: 0.4808 - val_mae: 0.1839 - val_mse: 0.0631\n",
            "Epoch 63/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1830 - accuracy: 0.4765 - mae: 0.1830 - mse: 0.0578 - val_loss: 0.1816 - val_accuracy: 0.4808 - val_mae: 0.1816 - val_mse: 0.0619\n",
            "Epoch 64/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1806 - accuracy: 0.4765 - mae: 0.1806 - mse: 0.0565 - val_loss: 0.1791 - val_accuracy: 0.4808 - val_mae: 0.1791 - val_mse: 0.0606\n",
            "Epoch 65/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1781 - accuracy: 0.4765 - mae: 0.1781 - mse: 0.0552 - val_loss: 0.1766 - val_accuracy: 0.4808 - val_mae: 0.1766 - val_mse: 0.0592\n",
            "Epoch 66/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1759 - accuracy: 0.4765 - mae: 0.1759 - mse: 0.0540 - val_loss: 0.1744 - val_accuracy: 0.4808 - val_mae: 0.1744 - val_mse: 0.0581\n",
            "Epoch 67/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1735 - accuracy: 0.4765 - mae: 0.1735 - mse: 0.0528 - val_loss: 0.1723 - val_accuracy: 0.4808 - val_mae: 0.1723 - val_mse: 0.0570\n",
            "Epoch 68/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1712 - accuracy: 0.4765 - mae: 0.1712 - mse: 0.0517 - val_loss: 0.1703 - val_accuracy: 0.4808 - val_mae: 0.1703 - val_mse: 0.0560\n",
            "Epoch 69/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1690 - accuracy: 0.4765 - mae: 0.1690 - mse: 0.0505 - val_loss: 0.1685 - val_accuracy: 0.4808 - val_mae: 0.1685 - val_mse: 0.0552\n",
            "Epoch 70/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1668 - accuracy: 0.4765 - mae: 0.1668 - mse: 0.0495 - val_loss: 0.1665 - val_accuracy: 0.4808 - val_mae: 0.1665 - val_mse: 0.0542\n",
            "Epoch 71/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1647 - accuracy: 0.4765 - mae: 0.1647 - mse: 0.0484 - val_loss: 0.1646 - val_accuracy: 0.4808 - val_mae: 0.1646 - val_mse: 0.0532\n",
            "Epoch 72/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1626 - accuracy: 0.4765 - mae: 0.1626 - mse: 0.0474 - val_loss: 0.1635 - val_accuracy: 0.4808 - val_mae: 0.1635 - val_mse: 0.0527\n",
            "Epoch 73/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1607 - accuracy: 0.4765 - mae: 0.1607 - mse: 0.0466 - val_loss: 0.1610 - val_accuracy: 0.4808 - val_mae: 0.1610 - val_mse: 0.0514\n",
            "Epoch 74/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1589 - accuracy: 0.4765 - mae: 0.1589 - mse: 0.0455 - val_loss: 0.1598 - val_accuracy: 0.4808 - val_mae: 0.1598 - val_mse: 0.0509\n",
            "Epoch 75/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1570 - accuracy: 0.4765 - mae: 0.1570 - mse: 0.0447 - val_loss: 0.1585 - val_accuracy: 0.4808 - val_mae: 0.1585 - val_mse: 0.0502\n",
            "Epoch 76/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1553 - accuracy: 0.4765 - mae: 0.1553 - mse: 0.0439 - val_loss: 0.1561 - val_accuracy: 0.4808 - val_mae: 0.1561 - val_mse: 0.0490\n",
            "Epoch 77/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1535 - accuracy: 0.4765 - mae: 0.1535 - mse: 0.0429 - val_loss: 0.1548 - val_accuracy: 0.4808 - val_mae: 0.1548 - val_mse: 0.0484\n",
            "Epoch 78/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1518 - accuracy: 0.4765 - mae: 0.1518 - mse: 0.0422 - val_loss: 0.1535 - val_accuracy: 0.4808 - val_mae: 0.1535 - val_mse: 0.0479\n",
            "Epoch 79/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1501 - accuracy: 0.4765 - mae: 0.1501 - mse: 0.0414 - val_loss: 0.1518 - val_accuracy: 0.4808 - val_mae: 0.1518 - val_mse: 0.0470\n",
            "Epoch 80/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1484 - accuracy: 0.4765 - mae: 0.1484 - mse: 0.0405 - val_loss: 0.1505 - val_accuracy: 0.4808 - val_mae: 0.1505 - val_mse: 0.0465\n",
            "Epoch 81/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1468 - accuracy: 0.4765 - mae: 0.1468 - mse: 0.0399 - val_loss: 0.1487 - val_accuracy: 0.4808 - val_mae: 0.1487 - val_mse: 0.0456\n",
            "Epoch 82/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1451 - accuracy: 0.4765 - mae: 0.1451 - mse: 0.0391 - val_loss: 0.1473 - val_accuracy: 0.4808 - val_mae: 0.1473 - val_mse: 0.0450\n",
            "Epoch 83/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1436 - accuracy: 0.4765 - mae: 0.1436 - mse: 0.0384 - val_loss: 0.1455 - val_accuracy: 0.4808 - val_mae: 0.1455 - val_mse: 0.0441\n",
            "Epoch 84/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1420 - accuracy: 0.4765 - mae: 0.1420 - mse: 0.0376 - val_loss: 0.1440 - val_accuracy: 0.4808 - val_mae: 0.1440 - val_mse: 0.0434\n",
            "Epoch 85/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1404 - accuracy: 0.4765 - mae: 0.1404 - mse: 0.0369 - val_loss: 0.1431 - val_accuracy: 0.4808 - val_mae: 0.1431 - val_mse: 0.0431\n",
            "Epoch 86/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1389 - accuracy: 0.4765 - mae: 0.1389 - mse: 0.0363 - val_loss: 0.1421 - val_accuracy: 0.4808 - val_mae: 0.1421 - val_mse: 0.0427\n",
            "Epoch 87/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1374 - accuracy: 0.4765 - mae: 0.1374 - mse: 0.0357 - val_loss: 0.1408 - val_accuracy: 0.4808 - val_mae: 0.1408 - val_mse: 0.0421\n",
            "Epoch 88/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1358 - accuracy: 0.4765 - mae: 0.1358 - mse: 0.0351 - val_loss: 0.1394 - val_accuracy: 0.4808 - val_mae: 0.1394 - val_mse: 0.0414\n",
            "Epoch 89/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1344 - accuracy: 0.4765 - mae: 0.1344 - mse: 0.0344 - val_loss: 0.1378 - val_accuracy: 0.4808 - val_mae: 0.1378 - val_mse: 0.0407\n",
            "Epoch 90/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1331 - accuracy: 0.4765 - mae: 0.1331 - mse: 0.0338 - val_loss: 0.1367 - val_accuracy: 0.4808 - val_mae: 0.1367 - val_mse: 0.0402\n",
            "Epoch 91/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1317 - accuracy: 0.4765 - mae: 0.1317 - mse: 0.0332 - val_loss: 0.1356 - val_accuracy: 0.4808 - val_mae: 0.1356 - val_mse: 0.0397\n",
            "Epoch 92/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1303 - accuracy: 0.4765 - mae: 0.1303 - mse: 0.0326 - val_loss: 0.1341 - val_accuracy: 0.4808 - val_mae: 0.1341 - val_mse: 0.0390\n",
            "Epoch 93/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1290 - accuracy: 0.4765 - mae: 0.1290 - mse: 0.0321 - val_loss: 0.1333 - val_accuracy: 0.4808 - val_mae: 0.1333 - val_mse: 0.0387\n",
            "Epoch 94/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1276 - accuracy: 0.4765 - mae: 0.1276 - mse: 0.0315 - val_loss: 0.1318 - val_accuracy: 0.4808 - val_mae: 0.1318 - val_mse: 0.0380\n",
            "Epoch 95/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1264 - accuracy: 0.4765 - mae: 0.1264 - mse: 0.0310 - val_loss: 0.1307 - val_accuracy: 0.4808 - val_mae: 0.1307 - val_mse: 0.0375\n",
            "Epoch 96/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1251 - accuracy: 0.4765 - mae: 0.1251 - mse: 0.0305 - val_loss: 0.1299 - val_accuracy: 0.4808 - val_mae: 0.1299 - val_mse: 0.0372\n",
            "Epoch 97/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1236 - accuracy: 0.4765 - mae: 0.1236 - mse: 0.0299 - val_loss: 0.1287 - val_accuracy: 0.4808 - val_mae: 0.1287 - val_mse: 0.0367\n",
            "Epoch 98/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1224 - accuracy: 0.4765 - mae: 0.1224 - mse: 0.0294 - val_loss: 0.1277 - val_accuracy: 0.4808 - val_mae: 0.1277 - val_mse: 0.0364\n",
            "Epoch 99/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1212 - accuracy: 0.4765 - mae: 0.1212 - mse: 0.0289 - val_loss: 0.1268 - val_accuracy: 0.4808 - val_mae: 0.1268 - val_mse: 0.0360\n",
            "Epoch 100/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1199 - accuracy: 0.4765 - mae: 0.1199 - mse: 0.0285 - val_loss: 0.1256 - val_accuracy: 0.4808 - val_mae: 0.1256 - val_mse: 0.0356\n",
            "Epoch 101/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1187 - accuracy: 0.4765 - mae: 0.1187 - mse: 0.0280 - val_loss: 0.1248 - val_accuracy: 0.4808 - val_mae: 0.1248 - val_mse: 0.0353\n",
            "Epoch 102/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1174 - accuracy: 0.4765 - mae: 0.1174 - mse: 0.0276 - val_loss: 0.1233 - val_accuracy: 0.4808 - val_mae: 0.1233 - val_mse: 0.0346\n",
            "Epoch 103/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1164 - accuracy: 0.4765 - mae: 0.1164 - mse: 0.0271 - val_loss: 0.1229 - val_accuracy: 0.4808 - val_mae: 0.1229 - val_mse: 0.0346\n",
            "Epoch 104/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1152 - accuracy: 0.4765 - mae: 0.1152 - mse: 0.0268 - val_loss: 0.1217 - val_accuracy: 0.4808 - val_mae: 0.1217 - val_mse: 0.0342\n",
            "Epoch 105/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1140 - accuracy: 0.4765 - mae: 0.1140 - mse: 0.0262 - val_loss: 0.1208 - val_accuracy: 0.4808 - val_mae: 0.1208 - val_mse: 0.0338\n",
            "Epoch 106/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1129 - accuracy: 0.4765 - mae: 0.1129 - mse: 0.0259 - val_loss: 0.1200 - val_accuracy: 0.4808 - val_mae: 0.1200 - val_mse: 0.0336\n",
            "Epoch 107/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1118 - accuracy: 0.4765 - mae: 0.1118 - mse: 0.0256 - val_loss: 0.1185 - val_accuracy: 0.4808 - val_mae: 0.1185 - val_mse: 0.0330\n",
            "Epoch 108/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1109 - accuracy: 0.4765 - mae: 0.1109 - mse: 0.0251 - val_loss: 0.1180 - val_accuracy: 0.4808 - val_mae: 0.1180 - val_mse: 0.0329\n",
            "Epoch 109/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1097 - accuracy: 0.4765 - mae: 0.1097 - mse: 0.0248 - val_loss: 0.1166 - val_accuracy: 0.4808 - val_mae: 0.1166 - val_mse: 0.0322\n",
            "Epoch 110/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1086 - accuracy: 0.4765 - mae: 0.1086 - mse: 0.0243 - val_loss: 0.1152 - val_accuracy: 0.4808 - val_mae: 0.1152 - val_mse: 0.0315\n",
            "Epoch 111/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1077 - accuracy: 0.4765 - mae: 0.1077 - mse: 0.0239 - val_loss: 0.1148 - val_accuracy: 0.4808 - val_mae: 0.1148 - val_mse: 0.0316\n",
            "Epoch 112/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1066 - accuracy: 0.4765 - mae: 0.1066 - mse: 0.0237 - val_loss: 0.1138 - val_accuracy: 0.4808 - val_mae: 0.1138 - val_mse: 0.0313\n",
            "Epoch 113/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1056 - accuracy: 0.4765 - mae: 0.1056 - mse: 0.0233 - val_loss: 0.1132 - val_accuracy: 0.4808 - val_mae: 0.1132 - val_mse: 0.0311\n",
            "Epoch 114/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1045 - accuracy: 0.4765 - mae: 0.1045 - mse: 0.0229 - val_loss: 0.1125 - val_accuracy: 0.4808 - val_mae: 0.1125 - val_mse: 0.0309\n",
            "Epoch 115/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1037 - accuracy: 0.4765 - mae: 0.1037 - mse: 0.0227 - val_loss: 0.1113 - val_accuracy: 0.4808 - val_mae: 0.1113 - val_mse: 0.0304\n",
            "Epoch 116/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1026 - accuracy: 0.4765 - mae: 0.1026 - mse: 0.0223 - val_loss: 0.1101 - val_accuracy: 0.4808 - val_mae: 0.1101 - val_mse: 0.0299\n",
            "Epoch 117/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1017 - accuracy: 0.4765 - mae: 0.1017 - mse: 0.0219 - val_loss: 0.1095 - val_accuracy: 0.4808 - val_mae: 0.1095 - val_mse: 0.0298\n",
            "Epoch 118/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1007 - accuracy: 0.4765 - mae: 0.1007 - mse: 0.0216 - val_loss: 0.1089 - val_accuracy: 0.4808 - val_mae: 0.1089 - val_mse: 0.0297\n",
            "Epoch 119/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0999 - accuracy: 0.4765 - mae: 0.0999 - mse: 0.0214 - val_loss: 0.1082 - val_accuracy: 0.4808 - val_mae: 0.1082 - val_mse: 0.0295\n",
            "Epoch 120/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0988 - accuracy: 0.4765 - mae: 0.0988 - mse: 0.0211 - val_loss: 0.1070 - val_accuracy: 0.4808 - val_mae: 0.1070 - val_mse: 0.0290\n",
            "Epoch 121/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0980 - accuracy: 0.4765 - mae: 0.0980 - mse: 0.0209 - val_loss: 0.1061 - val_accuracy: 0.4808 - val_mae: 0.1061 - val_mse: 0.0287\n",
            "Epoch 122/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0970 - accuracy: 0.4765 - mae: 0.0970 - mse: 0.0204 - val_loss: 0.1058 - val_accuracy: 0.4808 - val_mae: 0.1058 - val_mse: 0.0288\n",
            "Epoch 123/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0961 - accuracy: 0.4765 - mae: 0.0961 - mse: 0.0203 - val_loss: 0.1045 - val_accuracy: 0.4808 - val_mae: 0.1045 - val_mse: 0.0282\n",
            "Epoch 124/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0952 - accuracy: 0.4765 - mae: 0.0952 - mse: 0.0199 - val_loss: 0.1037 - val_accuracy: 0.4808 - val_mae: 0.1037 - val_mse: 0.0278\n",
            "Epoch 125/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0946 - accuracy: 0.4765 - mae: 0.0946 - mse: 0.0196 - val_loss: 0.1033 - val_accuracy: 0.4808 - val_mae: 0.1033 - val_mse: 0.0279\n",
            "Epoch 126/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0935 - accuracy: 0.4765 - mae: 0.0935 - mse: 0.0194 - val_loss: 0.1029 - val_accuracy: 0.4808 - val_mae: 0.1029 - val_mse: 0.0279\n",
            "Epoch 127/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0927 - accuracy: 0.4765 - mae: 0.0927 - mse: 0.0192 - val_loss: 0.1019 - val_accuracy: 0.4808 - val_mae: 0.1019 - val_mse: 0.0275\n",
            "Epoch 128/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0919 - accuracy: 0.4765 - mae: 0.0919 - mse: 0.0190 - val_loss: 0.1010 - val_accuracy: 0.4808 - val_mae: 0.1010 - val_mse: 0.0272\n",
            "Epoch 129/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0909 - accuracy: 0.4765 - mae: 0.0909 - mse: 0.0188 - val_loss: 0.1004 - val_accuracy: 0.4808 - val_mae: 0.1004 - val_mse: 0.0271\n",
            "Epoch 130/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0902 - accuracy: 0.4765 - mae: 0.0902 - mse: 0.0185 - val_loss: 0.0996 - val_accuracy: 0.4808 - val_mae: 0.0996 - val_mse: 0.0268\n",
            "Epoch 131/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0893 - accuracy: 0.4765 - mae: 0.0893 - mse: 0.0182 - val_loss: 0.0994 - val_accuracy: 0.4808 - val_mae: 0.0994 - val_mse: 0.0269\n",
            "Epoch 132/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0887 - accuracy: 0.4765 - mae: 0.0887 - mse: 0.0182 - val_loss: 0.0981 - val_accuracy: 0.4808 - val_mae: 0.0981 - val_mse: 0.0264\n",
            "Epoch 133/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0878 - accuracy: 0.4765 - mae: 0.0878 - mse: 0.0178 - val_loss: 0.0976 - val_accuracy: 0.4808 - val_mae: 0.0976 - val_mse: 0.0263\n",
            "Epoch 134/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0871 - accuracy: 0.4765 - mae: 0.0871 - mse: 0.0176 - val_loss: 0.0976 - val_accuracy: 0.4808 - val_mae: 0.0976 - val_mse: 0.0265\n",
            "Epoch 135/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0865 - accuracy: 0.4765 - mae: 0.0865 - mse: 0.0176 - val_loss: 0.0964 - val_accuracy: 0.4808 - val_mae: 0.0964 - val_mse: 0.0260\n",
            "Epoch 136/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0856 - accuracy: 0.4765 - mae: 0.0856 - mse: 0.0172 - val_loss: 0.0955 - val_accuracy: 0.4808 - val_mae: 0.0955 - val_mse: 0.0258\n",
            "Epoch 137/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0849 - accuracy: 0.4765 - mae: 0.0849 - mse: 0.0170 - val_loss: 0.0949 - val_accuracy: 0.4808 - val_mae: 0.0949 - val_mse: 0.0256\n",
            "Epoch 138/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0843 - accuracy: 0.4765 - mae: 0.0843 - mse: 0.0169 - val_loss: 0.0945 - val_accuracy: 0.4808 - val_mae: 0.0945 - val_mse: 0.0255\n",
            "Epoch 139/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0836 - accuracy: 0.4765 - mae: 0.0836 - mse: 0.0167 - val_loss: 0.0940 - val_accuracy: 0.4808 - val_mae: 0.0940 - val_mse: 0.0253\n",
            "Epoch 140/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0830 - accuracy: 0.4765 - mae: 0.0830 - mse: 0.0165 - val_loss: 0.0930 - val_accuracy: 0.4808 - val_mae: 0.0930 - val_mse: 0.0249\n",
            "Epoch 141/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0822 - accuracy: 0.4765 - mae: 0.0822 - mse: 0.0162 - val_loss: 0.0925 - val_accuracy: 0.4808 - val_mae: 0.0925 - val_mse: 0.0248\n",
            "Epoch 142/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0818 - accuracy: 0.4765 - mae: 0.0818 - mse: 0.0161 - val_loss: 0.0919 - val_accuracy: 0.4808 - val_mae: 0.0919 - val_mse: 0.0246\n",
            "Epoch 143/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0811 - accuracy: 0.4765 - mae: 0.0811 - mse: 0.0159 - val_loss: 0.0912 - val_accuracy: 0.4808 - val_mae: 0.0912 - val_mse: 0.0244\n",
            "Epoch 144/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0805 - accuracy: 0.4765 - mae: 0.0805 - mse: 0.0158 - val_loss: 0.0903 - val_accuracy: 0.4808 - val_mae: 0.0903 - val_mse: 0.0239\n",
            "Epoch 145/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0800 - accuracy: 0.4765 - mae: 0.0800 - mse: 0.0155 - val_loss: 0.0897 - val_accuracy: 0.4808 - val_mae: 0.0897 - val_mse: 0.0237\n",
            "Epoch 146/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0793 - accuracy: 0.4765 - mae: 0.0793 - mse: 0.0153 - val_loss: 0.0894 - val_accuracy: 0.4808 - val_mae: 0.0894 - val_mse: 0.0238\n",
            "Epoch 147/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0788 - accuracy: 0.4765 - mae: 0.0788 - mse: 0.0153 - val_loss: 0.0890 - val_accuracy: 0.4808 - val_mae: 0.0890 - val_mse: 0.0237\n",
            "Epoch 148/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0782 - accuracy: 0.4765 - mae: 0.0782 - mse: 0.0151 - val_loss: 0.0882 - val_accuracy: 0.4808 - val_mae: 0.0882 - val_mse: 0.0233\n",
            "Epoch 149/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0777 - accuracy: 0.4765 - mae: 0.0777 - mse: 0.0148 - val_loss: 0.0880 - val_accuracy: 0.4808 - val_mae: 0.0880 - val_mse: 0.0234\n",
            "Epoch 150/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0771 - accuracy: 0.4765 - mae: 0.0771 - mse: 0.0148 - val_loss: 0.0879 - val_accuracy: 0.4808 - val_mae: 0.0879 - val_mse: 0.0234\n",
            "Epoch 151/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0765 - accuracy: 0.4765 - mae: 0.0765 - mse: 0.0147 - val_loss: 0.0867 - val_accuracy: 0.4808 - val_mae: 0.0867 - val_mse: 0.0229\n",
            "Epoch 152/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0759 - accuracy: 0.4765 - mae: 0.0759 - mse: 0.0143 - val_loss: 0.0872 - val_accuracy: 0.4808 - val_mae: 0.0872 - val_mse: 0.0233\n",
            "Epoch 153/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0753 - accuracy: 0.4765 - mae: 0.0753 - mse: 0.0145 - val_loss: 0.0858 - val_accuracy: 0.4808 - val_mae: 0.0858 - val_mse: 0.0226\n",
            "Epoch 154/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0747 - accuracy: 0.4765 - mae: 0.0747 - mse: 0.0142 - val_loss: 0.0856 - val_accuracy: 0.4808 - val_mae: 0.0856 - val_mse: 0.0227\n",
            "Epoch 155/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0742 - accuracy: 0.4765 - mae: 0.0742 - mse: 0.0141 - val_loss: 0.0854 - val_accuracy: 0.4808 - val_mae: 0.0854 - val_mse: 0.0226\n",
            "Epoch 156/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0738 - accuracy: 0.4765 - mae: 0.0738 - mse: 0.0140 - val_loss: 0.0845 - val_accuracy: 0.4808 - val_mae: 0.0845 - val_mse: 0.0223\n",
            "Epoch 157/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0732 - accuracy: 0.4765 - mae: 0.0732 - mse: 0.0138 - val_loss: 0.0837 - val_accuracy: 0.4808 - val_mae: 0.0837 - val_mse: 0.0219\n",
            "Epoch 158/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0727 - accuracy: 0.4765 - mae: 0.0727 - mse: 0.0136 - val_loss: 0.0837 - val_accuracy: 0.4808 - val_mae: 0.0837 - val_mse: 0.0221\n",
            "Epoch 159/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0722 - accuracy: 0.4765 - mae: 0.0722 - mse: 0.0135 - val_loss: 0.0827 - val_accuracy: 0.4808 - val_mae: 0.0827 - val_mse: 0.0216\n",
            "Epoch 160/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0718 - accuracy: 0.4765 - mae: 0.0718 - mse: 0.0133 - val_loss: 0.0829 - val_accuracy: 0.4808 - val_mae: 0.0829 - val_mse: 0.0218\n",
            "Epoch 161/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0713 - accuracy: 0.4765 - mae: 0.0713 - mse: 0.0133 - val_loss: 0.0827 - val_accuracy: 0.4808 - val_mae: 0.0827 - val_mse: 0.0218\n",
            "Epoch 162/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0708 - accuracy: 0.4765 - mae: 0.0708 - mse: 0.0132 - val_loss: 0.0816 - val_accuracy: 0.4808 - val_mae: 0.0816 - val_mse: 0.0213\n",
            "Epoch 163/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0703 - accuracy: 0.4765 - mae: 0.0703 - mse: 0.0130 - val_loss: 0.0819 - val_accuracy: 0.4808 - val_mae: 0.0819 - val_mse: 0.0216\n",
            "Epoch 164/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0698 - accuracy: 0.4765 - mae: 0.0698 - mse: 0.0130 - val_loss: 0.0812 - val_accuracy: 0.4808 - val_mae: 0.0812 - val_mse: 0.0213\n",
            "Epoch 165/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0693 - accuracy: 0.4765 - mae: 0.0693 - mse: 0.0128 - val_loss: 0.0803 - val_accuracy: 0.4808 - val_mae: 0.0803 - val_mse: 0.0210\n",
            "Epoch 166/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0688 - accuracy: 0.4765 - mae: 0.0688 - mse: 0.0127 - val_loss: 0.0798 - val_accuracy: 0.4808 - val_mae: 0.0798 - val_mse: 0.0208\n",
            "Epoch 167/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0684 - accuracy: 0.4765 - mae: 0.0684 - mse: 0.0125 - val_loss: 0.0796 - val_accuracy: 0.4808 - val_mae: 0.0796 - val_mse: 0.0208\n",
            "Epoch 168/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0679 - accuracy: 0.4765 - mae: 0.0679 - mse: 0.0125 - val_loss: 0.0798 - val_accuracy: 0.4808 - val_mae: 0.0798 - val_mse: 0.0210\n",
            "Epoch 169/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0674 - accuracy: 0.4765 - mae: 0.0674 - mse: 0.0125 - val_loss: 0.0783 - val_accuracy: 0.4808 - val_mae: 0.0783 - val_mse: 0.0202\n",
            "Epoch 170/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0670 - accuracy: 0.4765 - mae: 0.0670 - mse: 0.0121 - val_loss: 0.0784 - val_accuracy: 0.4808 - val_mae: 0.0784 - val_mse: 0.0205\n",
            "Epoch 171/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0665 - accuracy: 0.4765 - mae: 0.0665 - mse: 0.0122 - val_loss: 0.0775 - val_accuracy: 0.4808 - val_mae: 0.0775 - val_mse: 0.0200\n",
            "Epoch 172/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0661 - accuracy: 0.4765 - mae: 0.0661 - mse: 0.0120 - val_loss: 0.0774 - val_accuracy: 0.4808 - val_mae: 0.0774 - val_mse: 0.0201\n",
            "Epoch 173/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0656 - accuracy: 0.4765 - mae: 0.0656 - mse: 0.0119 - val_loss: 0.0769 - val_accuracy: 0.4808 - val_mae: 0.0769 - val_mse: 0.0199\n",
            "Epoch 174/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0652 - accuracy: 0.4765 - mae: 0.0652 - mse: 0.0117 - val_loss: 0.0772 - val_accuracy: 0.4808 - val_mae: 0.0772 - val_mse: 0.0202\n",
            "Epoch 175/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0648 - accuracy: 0.4765 - mae: 0.0648 - mse: 0.0117 - val_loss: 0.0764 - val_accuracy: 0.4808 - val_mae: 0.0764 - val_mse: 0.0198\n",
            "Epoch 176/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0644 - accuracy: 0.4765 - mae: 0.0644 - mse: 0.0116 - val_loss: 0.0763 - val_accuracy: 0.4808 - val_mae: 0.0763 - val_mse: 0.0198\n",
            "Epoch 177/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0639 - accuracy: 0.4765 - mae: 0.0639 - mse: 0.0115 - val_loss: 0.0754 - val_accuracy: 0.4808 - val_mae: 0.0754 - val_mse: 0.0195\n",
            "Epoch 178/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0635 - accuracy: 0.4765 - mae: 0.0635 - mse: 0.0114 - val_loss: 0.0752 - val_accuracy: 0.4808 - val_mae: 0.0752 - val_mse: 0.0194\n",
            "Epoch 179/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0631 - accuracy: 0.4765 - mae: 0.0631 - mse: 0.0113 - val_loss: 0.0745 - val_accuracy: 0.4808 - val_mae: 0.0745 - val_mse: 0.0191\n",
            "Epoch 180/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0628 - accuracy: 0.4765 - mae: 0.0628 - mse: 0.0111 - val_loss: 0.0740 - val_accuracy: 0.4808 - val_mae: 0.0740 - val_mse: 0.0189\n",
            "Epoch 181/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0624 - accuracy: 0.4765 - mae: 0.0624 - mse: 0.0110 - val_loss: 0.0738 - val_accuracy: 0.4808 - val_mae: 0.0738 - val_mse: 0.0189\n",
            "Epoch 182/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0619 - accuracy: 0.4765 - mae: 0.0619 - mse: 0.0109 - val_loss: 0.0739 - val_accuracy: 0.4808 - val_mae: 0.0739 - val_mse: 0.0191\n",
            "Epoch 183/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0614 - accuracy: 0.4765 - mae: 0.0614 - mse: 0.0109 - val_loss: 0.0731 - val_accuracy: 0.4808 - val_mae: 0.0731 - val_mse: 0.0187\n",
            "Epoch 184/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0611 - accuracy: 0.4765 - mae: 0.0611 - mse: 0.0107 - val_loss: 0.0726 - val_accuracy: 0.4808 - val_mae: 0.0726 - val_mse: 0.0185\n",
            "Epoch 185/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0608 - accuracy: 0.4765 - mae: 0.0608 - mse: 0.0107 - val_loss: 0.0724 - val_accuracy: 0.4808 - val_mae: 0.0724 - val_mse: 0.0185\n",
            "Epoch 186/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0604 - accuracy: 0.4765 - mae: 0.0604 - mse: 0.0106 - val_loss: 0.0722 - val_accuracy: 0.4808 - val_mae: 0.0722 - val_mse: 0.0185\n",
            "Epoch 187/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0600 - accuracy: 0.4765 - mae: 0.0600 - mse: 0.0105 - val_loss: 0.0719 - val_accuracy: 0.4808 - val_mae: 0.0719 - val_mse: 0.0184\n",
            "Epoch 188/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0597 - accuracy: 0.4765 - mae: 0.0597 - mse: 0.0104 - val_loss: 0.0716 - val_accuracy: 0.4808 - val_mae: 0.0716 - val_mse: 0.0183\n",
            "Epoch 189/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0592 - accuracy: 0.4765 - mae: 0.0592 - mse: 0.0103 - val_loss: 0.0710 - val_accuracy: 0.4808 - val_mae: 0.0710 - val_mse: 0.0181\n",
            "Epoch 190/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0589 - accuracy: 0.4765 - mae: 0.0589 - mse: 0.0102 - val_loss: 0.0707 - val_accuracy: 0.4808 - val_mae: 0.0707 - val_mse: 0.0180\n",
            "Epoch 191/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0585 - accuracy: 0.4765 - mae: 0.0585 - mse: 0.0102 - val_loss: 0.0704 - val_accuracy: 0.4808 - val_mae: 0.0704 - val_mse: 0.0179\n",
            "Epoch 192/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0582 - accuracy: 0.4765 - mae: 0.0582 - mse: 0.0101 - val_loss: 0.0704 - val_accuracy: 0.4808 - val_mae: 0.0704 - val_mse: 0.0179\n",
            "Epoch 193/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0578 - accuracy: 0.4765 - mae: 0.0578 - mse: 0.0100 - val_loss: 0.0704 - val_accuracy: 0.4808 - val_mae: 0.0704 - val_mse: 0.0180\n",
            "Epoch 194/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0575 - accuracy: 0.4765 - mae: 0.0575 - mse: 0.0101 - val_loss: 0.0700 - val_accuracy: 0.4808 - val_mae: 0.0700 - val_mse: 0.0179\n",
            "Epoch 195/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0571 - accuracy: 0.4765 - mae: 0.0571 - mse: 0.0099 - val_loss: 0.0695 - val_accuracy: 0.4808 - val_mae: 0.0695 - val_mse: 0.0177\n",
            "Epoch 196/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0567 - accuracy: 0.4765 - mae: 0.0567 - mse: 0.0098 - val_loss: 0.0689 - val_accuracy: 0.4808 - val_mae: 0.0689 - val_mse: 0.0175\n",
            "Epoch 197/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0565 - accuracy: 0.4765 - mae: 0.0565 - mse: 0.0097 - val_loss: 0.0685 - val_accuracy: 0.4808 - val_mae: 0.0685 - val_mse: 0.0174\n",
            "Epoch 198/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0562 - accuracy: 0.4765 - mae: 0.0562 - mse: 0.0097 - val_loss: 0.0686 - val_accuracy: 0.4808 - val_mae: 0.0686 - val_mse: 0.0174\n",
            "Epoch 199/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0558 - accuracy: 0.4765 - mae: 0.0558 - mse: 0.0096 - val_loss: 0.0689 - val_accuracy: 0.4808 - val_mae: 0.0689 - val_mse: 0.0176\n",
            "Epoch 200/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0554 - accuracy: 0.4765 - mae: 0.0554 - mse: 0.0097 - val_loss: 0.0673 - val_accuracy: 0.4808 - val_mae: 0.0673 - val_mse: 0.0170\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hasil nya"
      ],
      "metadata": {
        "id": "mGLQsbPT9NTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'], label='loss')\n",
        "plt.plot(history.history['val_loss'], label='Valid')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['accuracy'], label='acc')\n",
        "plt.plot(history.history['val_accuracy'], label='Valid')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['mae'], label='mae')\n",
        "plt.plot(history.history['val_mae'], label='Valid mae')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['mse'], label='mse')\n",
        "plt.plot(history.history['val_mse'], label='Valid mse')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GUrToHgOq1J7",
        "outputId": "973476f4-793b-49c8-973e-e5a613952734"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# model 2"
      ],
      "metadata": {
        "id": "W1V-d9d6sqKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(29, input_dim=29)) \n",
        "model.add(Dense(1, activation = 'linear'))\n",
        "\n",
        "model.compile(loss=\"mean_absolute_error\", optimizer=optimizers.SGD(learning_rate=0.001), metrics=['accuracy','mae', 'mse'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89f64d02-c3e5-40b5-cfc3-9380d0995ac1",
        "id": "N-mzNXp8sqKU"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_25\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_76 (Dense)            (None, 29)                870       \n",
            "                                                                 \n",
            " dense_77 (Dense)            (None, 1)                 30        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 900\n",
            "Trainable params: 900\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=200, batch_size=32, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8c9c394-5c00-43f4-f093-9124d6d8db89",
        "id": "2P0wwQdYsqKU"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "12/12 [==============================] - 1s 18ms/step - loss: 0.3682 - accuracy: 0.3380 - mae: 0.3682 - mse: 0.1945 - val_loss: 0.3497 - val_accuracy: 0.3590 - val_mae: 0.3497 - val_mse: 0.1697\n",
            "Epoch 2/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.3201 - accuracy: 0.4017 - mae: 0.3201 - mse: 0.1493 - val_loss: 0.3116 - val_accuracy: 0.3718 - val_mae: 0.3116 - val_mse: 0.1350\n",
            "Epoch 3/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2886 - accuracy: 0.4377 - mae: 0.2886 - mse: 0.1209 - val_loss: 0.2853 - val_accuracy: 0.4167 - val_mae: 0.2853 - val_mse: 0.1153\n",
            "Epoch 4/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2667 - accuracy: 0.4460 - mae: 0.2667 - mse: 0.1033 - val_loss: 0.2641 - val_accuracy: 0.4423 - val_mae: 0.2641 - val_mse: 0.1021\n",
            "Epoch 5/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2490 - accuracy: 0.4543 - mae: 0.2490 - mse: 0.0921 - val_loss: 0.2488 - val_accuracy: 0.4551 - val_mae: 0.2488 - val_mse: 0.0943\n",
            "Epoch 6/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.2352 - accuracy: 0.4654 - mae: 0.2352 - mse: 0.0850 - val_loss: 0.2372 - val_accuracy: 0.4679 - val_mae: 0.2372 - val_mse: 0.0894\n",
            "Epoch 7/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2242 - accuracy: 0.4681 - mae: 0.2242 - mse: 0.0808 - val_loss: 0.2305 - val_accuracy: 0.4679 - val_mae: 0.2305 - val_mse: 0.0866\n",
            "Epoch 8/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.2175 - accuracy: 0.4681 - mae: 0.2175 - mse: 0.0779 - val_loss: 0.2259 - val_accuracy: 0.4679 - val_mae: 0.2259 - val_mse: 0.0843\n",
            "Epoch 9/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2126 - accuracy: 0.4681 - mae: 0.2126 - mse: 0.0758 - val_loss: 0.2215 - val_accuracy: 0.4679 - val_mae: 0.2215 - val_mse: 0.0822\n",
            "Epoch 10/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.2079 - accuracy: 0.4681 - mae: 0.2079 - mse: 0.0737 - val_loss: 0.2174 - val_accuracy: 0.4679 - val_mae: 0.2174 - val_mse: 0.0803\n",
            "Epoch 11/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2027 - accuracy: 0.4681 - mae: 0.2027 - mse: 0.0714 - val_loss: 0.2139 - val_accuracy: 0.4679 - val_mae: 0.2139 - val_mse: 0.0788\n",
            "Epoch 12/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1984 - accuracy: 0.4681 - mae: 0.1984 - mse: 0.0696 - val_loss: 0.2099 - val_accuracy: 0.4679 - val_mae: 0.2099 - val_mse: 0.0765\n",
            "Epoch 13/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1945 - accuracy: 0.4681 - mae: 0.1945 - mse: 0.0674 - val_loss: 0.2059 - val_accuracy: 0.4679 - val_mae: 0.2059 - val_mse: 0.0742\n",
            "Epoch 14/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1907 - accuracy: 0.4681 - mae: 0.1907 - mse: 0.0653 - val_loss: 0.2020 - val_accuracy: 0.4679 - val_mae: 0.2020 - val_mse: 0.0719\n",
            "Epoch 15/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1871 - accuracy: 0.4681 - mae: 0.1871 - mse: 0.0632 - val_loss: 0.1982 - val_accuracy: 0.4679 - val_mae: 0.1982 - val_mse: 0.0699\n",
            "Epoch 16/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1835 - accuracy: 0.4681 - mae: 0.1835 - mse: 0.0615 - val_loss: 0.1947 - val_accuracy: 0.4679 - val_mae: 0.1947 - val_mse: 0.0684\n",
            "Epoch 17/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1798 - accuracy: 0.4681 - mae: 0.1798 - mse: 0.0598 - val_loss: 0.1911 - val_accuracy: 0.4679 - val_mae: 0.1911 - val_mse: 0.0663\n",
            "Epoch 18/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1761 - accuracy: 0.4681 - mae: 0.1761 - mse: 0.0578 - val_loss: 0.1875 - val_accuracy: 0.4679 - val_mae: 0.1875 - val_mse: 0.0643\n",
            "Epoch 19/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1724 - accuracy: 0.4681 - mae: 0.1724 - mse: 0.0560 - val_loss: 0.1839 - val_accuracy: 0.4679 - val_mae: 0.1839 - val_mse: 0.0624\n",
            "Epoch 20/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1688 - accuracy: 0.4681 - mae: 0.1688 - mse: 0.0543 - val_loss: 0.1804 - val_accuracy: 0.4679 - val_mae: 0.1804 - val_mse: 0.0608\n",
            "Epoch 21/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1652 - accuracy: 0.4681 - mae: 0.1652 - mse: 0.0527 - val_loss: 0.1771 - val_accuracy: 0.4679 - val_mae: 0.1771 - val_mse: 0.0592\n",
            "Epoch 22/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1618 - accuracy: 0.4681 - mae: 0.1618 - mse: 0.0514 - val_loss: 0.1737 - val_accuracy: 0.4679 - val_mae: 0.1737 - val_mse: 0.0573\n",
            "Epoch 23/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1585 - accuracy: 0.4681 - mae: 0.1585 - mse: 0.0496 - val_loss: 0.1705 - val_accuracy: 0.4679 - val_mae: 0.1705 - val_mse: 0.0557\n",
            "Epoch 24/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1552 - accuracy: 0.4681 - mae: 0.1552 - mse: 0.0482 - val_loss: 0.1675 - val_accuracy: 0.4679 - val_mae: 0.1675 - val_mse: 0.0545\n",
            "Epoch 25/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1520 - accuracy: 0.4681 - mae: 0.1520 - mse: 0.0469 - val_loss: 0.1643 - val_accuracy: 0.4679 - val_mae: 0.1643 - val_mse: 0.0528\n",
            "Epoch 26/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1488 - accuracy: 0.4681 - mae: 0.1488 - mse: 0.0457 - val_loss: 0.1611 - val_accuracy: 0.4679 - val_mae: 0.1611 - val_mse: 0.0510\n",
            "Epoch 27/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1456 - accuracy: 0.4681 - mae: 0.1456 - mse: 0.0442 - val_loss: 0.1581 - val_accuracy: 0.4679 - val_mae: 0.1581 - val_mse: 0.0497\n",
            "Epoch 28/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1426 - accuracy: 0.4681 - mae: 0.1426 - mse: 0.0430 - val_loss: 0.1551 - val_accuracy: 0.4679 - val_mae: 0.1551 - val_mse: 0.0483\n",
            "Epoch 29/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1395 - accuracy: 0.4681 - mae: 0.1395 - mse: 0.0419 - val_loss: 0.1524 - val_accuracy: 0.4679 - val_mae: 0.1524 - val_mse: 0.0468\n",
            "Epoch 30/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1366 - accuracy: 0.4681 - mae: 0.1366 - mse: 0.0408 - val_loss: 0.1495 - val_accuracy: 0.4679 - val_mae: 0.1495 - val_mse: 0.0456\n",
            "Epoch 31/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1338 - accuracy: 0.4681 - mae: 0.1338 - mse: 0.0397 - val_loss: 0.1465 - val_accuracy: 0.4679 - val_mae: 0.1465 - val_mse: 0.0444\n",
            "Epoch 32/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1309 - accuracy: 0.4681 - mae: 0.1309 - mse: 0.0387 - val_loss: 0.1438 - val_accuracy: 0.4679 - val_mae: 0.1438 - val_mse: 0.0431\n",
            "Epoch 33/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1286 - accuracy: 0.4681 - mae: 0.1286 - mse: 0.0377 - val_loss: 0.1410 - val_accuracy: 0.4679 - val_mae: 0.1410 - val_mse: 0.0424\n",
            "Epoch 34/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1260 - accuracy: 0.4681 - mae: 0.1260 - mse: 0.0369 - val_loss: 0.1383 - val_accuracy: 0.4679 - val_mae: 0.1383 - val_mse: 0.0412\n",
            "Epoch 35/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1237 - accuracy: 0.4681 - mae: 0.1237 - mse: 0.0360 - val_loss: 0.1359 - val_accuracy: 0.4679 - val_mae: 0.1359 - val_mse: 0.0403\n",
            "Epoch 36/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1214 - accuracy: 0.4681 - mae: 0.1214 - mse: 0.0352 - val_loss: 0.1333 - val_accuracy: 0.4679 - val_mae: 0.1333 - val_mse: 0.0393\n",
            "Epoch 37/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1193 - accuracy: 0.4681 - mae: 0.1193 - mse: 0.0345 - val_loss: 0.1310 - val_accuracy: 0.4679 - val_mae: 0.1310 - val_mse: 0.0383\n",
            "Epoch 38/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1172 - accuracy: 0.4681 - mae: 0.1172 - mse: 0.0337 - val_loss: 0.1288 - val_accuracy: 0.4679 - val_mae: 0.1288 - val_mse: 0.0375\n",
            "Epoch 39/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1151 - accuracy: 0.4681 - mae: 0.1151 - mse: 0.0330 - val_loss: 0.1266 - val_accuracy: 0.4679 - val_mae: 0.1266 - val_mse: 0.0367\n",
            "Epoch 40/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1132 - accuracy: 0.4681 - mae: 0.1132 - mse: 0.0324 - val_loss: 0.1244 - val_accuracy: 0.4679 - val_mae: 0.1244 - val_mse: 0.0358\n",
            "Epoch 41/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1113 - accuracy: 0.4681 - mae: 0.1113 - mse: 0.0318 - val_loss: 0.1223 - val_accuracy: 0.4679 - val_mae: 0.1223 - val_mse: 0.0351\n",
            "Epoch 42/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1096 - accuracy: 0.4681 - mae: 0.1096 - mse: 0.0312 - val_loss: 0.1203 - val_accuracy: 0.4679 - val_mae: 0.1203 - val_mse: 0.0345\n",
            "Epoch 43/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1078 - accuracy: 0.4681 - mae: 0.1078 - mse: 0.0306 - val_loss: 0.1185 - val_accuracy: 0.4679 - val_mae: 0.1185 - val_mse: 0.0337\n",
            "Epoch 44/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1060 - accuracy: 0.4681 - mae: 0.1060 - mse: 0.0300 - val_loss: 0.1165 - val_accuracy: 0.4679 - val_mae: 0.1165 - val_mse: 0.0330\n",
            "Epoch 45/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1043 - accuracy: 0.4681 - mae: 0.1043 - mse: 0.0295 - val_loss: 0.1146 - val_accuracy: 0.4679 - val_mae: 0.1146 - val_mse: 0.0324\n",
            "Epoch 46/200\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.1027 - accuracy: 0.4681 - mae: 0.1027 - mse: 0.0289 - val_loss: 0.1129 - val_accuracy: 0.4679 - val_mae: 0.1129 - val_mse: 0.0317\n",
            "Epoch 47/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1012 - accuracy: 0.4681 - mae: 0.1012 - mse: 0.0285 - val_loss: 0.1114 - val_accuracy: 0.4679 - val_mae: 0.1114 - val_mse: 0.0311\n",
            "Epoch 48/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0997 - accuracy: 0.4681 - mae: 0.0997 - mse: 0.0280 - val_loss: 0.1099 - val_accuracy: 0.4679 - val_mae: 0.1099 - val_mse: 0.0307\n",
            "Epoch 49/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0984 - accuracy: 0.4681 - mae: 0.0984 - mse: 0.0275 - val_loss: 0.1085 - val_accuracy: 0.4679 - val_mae: 0.1085 - val_mse: 0.0300\n",
            "Epoch 50/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0970 - accuracy: 0.4681 - mae: 0.0970 - mse: 0.0271 - val_loss: 0.1070 - val_accuracy: 0.4679 - val_mae: 0.1070 - val_mse: 0.0296\n",
            "Epoch 51/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0957 - accuracy: 0.4681 - mae: 0.0957 - mse: 0.0267 - val_loss: 0.1059 - val_accuracy: 0.4679 - val_mae: 0.1059 - val_mse: 0.0290\n",
            "Epoch 52/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0945 - accuracy: 0.4681 - mae: 0.0945 - mse: 0.0262 - val_loss: 0.1043 - val_accuracy: 0.4679 - val_mae: 0.1043 - val_mse: 0.0286\n",
            "Epoch 53/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0933 - accuracy: 0.4681 - mae: 0.0933 - mse: 0.0259 - val_loss: 0.1032 - val_accuracy: 0.4679 - val_mae: 0.1032 - val_mse: 0.0282\n",
            "Epoch 54/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0922 - accuracy: 0.4681 - mae: 0.0922 - mse: 0.0255 - val_loss: 0.1017 - val_accuracy: 0.4679 - val_mae: 0.1017 - val_mse: 0.0278\n",
            "Epoch 55/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0910 - accuracy: 0.4681 - mae: 0.0910 - mse: 0.0251 - val_loss: 0.1004 - val_accuracy: 0.4679 - val_mae: 0.1004 - val_mse: 0.0274\n",
            "Epoch 56/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0899 - accuracy: 0.4681 - mae: 0.0899 - mse: 0.0248 - val_loss: 0.0992 - val_accuracy: 0.4679 - val_mae: 0.0992 - val_mse: 0.0270\n",
            "Epoch 57/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0888 - accuracy: 0.4681 - mae: 0.0888 - mse: 0.0244 - val_loss: 0.0980 - val_accuracy: 0.4679 - val_mae: 0.0980 - val_mse: 0.0266\n",
            "Epoch 58/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0877 - accuracy: 0.4681 - mae: 0.0877 - mse: 0.0241 - val_loss: 0.0968 - val_accuracy: 0.4679 - val_mae: 0.0968 - val_mse: 0.0261\n",
            "Epoch 59/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0867 - accuracy: 0.4681 - mae: 0.0867 - mse: 0.0237 - val_loss: 0.0957 - val_accuracy: 0.4679 - val_mae: 0.0957 - val_mse: 0.0257\n",
            "Epoch 60/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0855 - accuracy: 0.4681 - mae: 0.0855 - mse: 0.0233 - val_loss: 0.0945 - val_accuracy: 0.4679 - val_mae: 0.0945 - val_mse: 0.0253\n",
            "Epoch 61/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0847 - accuracy: 0.4681 - mae: 0.0847 - mse: 0.0230 - val_loss: 0.0936 - val_accuracy: 0.4679 - val_mae: 0.0936 - val_mse: 0.0251\n",
            "Epoch 62/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0835 - accuracy: 0.4681 - mae: 0.0835 - mse: 0.0227 - val_loss: 0.0925 - val_accuracy: 0.4679 - val_mae: 0.0925 - val_mse: 0.0248\n",
            "Epoch 63/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0827 - accuracy: 0.4681 - mae: 0.0827 - mse: 0.0223 - val_loss: 0.0915 - val_accuracy: 0.4679 - val_mae: 0.0915 - val_mse: 0.0244\n",
            "Epoch 64/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0815 - accuracy: 0.4681 - mae: 0.0815 - mse: 0.0221 - val_loss: 0.0901 - val_accuracy: 0.4679 - val_mae: 0.0901 - val_mse: 0.0240\n",
            "Epoch 65/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0805 - accuracy: 0.4681 - mae: 0.0805 - mse: 0.0218 - val_loss: 0.0894 - val_accuracy: 0.4679 - val_mae: 0.0894 - val_mse: 0.0239\n",
            "Epoch 66/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0797 - accuracy: 0.4681 - mae: 0.0797 - mse: 0.0215 - val_loss: 0.0882 - val_accuracy: 0.4679 - val_mae: 0.0882 - val_mse: 0.0235\n",
            "Epoch 67/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0787 - accuracy: 0.4681 - mae: 0.0787 - mse: 0.0212 - val_loss: 0.0873 - val_accuracy: 0.4679 - val_mae: 0.0873 - val_mse: 0.0231\n",
            "Epoch 68/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0780 - accuracy: 0.4681 - mae: 0.0780 - mse: 0.0210 - val_loss: 0.0860 - val_accuracy: 0.4679 - val_mae: 0.0860 - val_mse: 0.0228\n",
            "Epoch 69/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0768 - accuracy: 0.4681 - mae: 0.0768 - mse: 0.0206 - val_loss: 0.0850 - val_accuracy: 0.4679 - val_mae: 0.0850 - val_mse: 0.0226\n",
            "Epoch 70/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0759 - accuracy: 0.4681 - mae: 0.0759 - mse: 0.0204 - val_loss: 0.0843 - val_accuracy: 0.4679 - val_mae: 0.0843 - val_mse: 0.0224\n",
            "Epoch 71/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0752 - accuracy: 0.4681 - mae: 0.0752 - mse: 0.0201 - val_loss: 0.0837 - val_accuracy: 0.4679 - val_mae: 0.0837 - val_mse: 0.0222\n",
            "Epoch 72/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0744 - accuracy: 0.4681 - mae: 0.0744 - mse: 0.0199 - val_loss: 0.0824 - val_accuracy: 0.4679 - val_mae: 0.0824 - val_mse: 0.0218\n",
            "Epoch 73/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0735 - accuracy: 0.4681 - mae: 0.0735 - mse: 0.0196 - val_loss: 0.0814 - val_accuracy: 0.4679 - val_mae: 0.0814 - val_mse: 0.0216\n",
            "Epoch 74/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0727 - accuracy: 0.4681 - mae: 0.0727 - mse: 0.0194 - val_loss: 0.0807 - val_accuracy: 0.4679 - val_mae: 0.0807 - val_mse: 0.0214\n",
            "Epoch 75/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0719 - accuracy: 0.4681 - mae: 0.0719 - mse: 0.0192 - val_loss: 0.0797 - val_accuracy: 0.4679 - val_mae: 0.0797 - val_mse: 0.0211\n",
            "Epoch 76/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0710 - accuracy: 0.4681 - mae: 0.0710 - mse: 0.0189 - val_loss: 0.0788 - val_accuracy: 0.4679 - val_mae: 0.0788 - val_mse: 0.0209\n",
            "Epoch 77/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0701 - accuracy: 0.4681 - mae: 0.0701 - mse: 0.0187 - val_loss: 0.0782 - val_accuracy: 0.4679 - val_mae: 0.0782 - val_mse: 0.0207\n",
            "Epoch 78/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0696 - accuracy: 0.4681 - mae: 0.0696 - mse: 0.0186 - val_loss: 0.0772 - val_accuracy: 0.4679 - val_mae: 0.0772 - val_mse: 0.0205\n",
            "Epoch 79/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0687 - accuracy: 0.4681 - mae: 0.0687 - mse: 0.0183 - val_loss: 0.0765 - val_accuracy: 0.4679 - val_mae: 0.0765 - val_mse: 0.0203\n",
            "Epoch 80/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0679 - accuracy: 0.4681 - mae: 0.0679 - mse: 0.0181 - val_loss: 0.0758 - val_accuracy: 0.4679 - val_mae: 0.0758 - val_mse: 0.0201\n",
            "Epoch 81/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0672 - accuracy: 0.4681 - mae: 0.0672 - mse: 0.0179 - val_loss: 0.0749 - val_accuracy: 0.4679 - val_mae: 0.0749 - val_mse: 0.0200\n",
            "Epoch 82/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0664 - accuracy: 0.4681 - mae: 0.0664 - mse: 0.0177 - val_loss: 0.0740 - val_accuracy: 0.4679 - val_mae: 0.0740 - val_mse: 0.0198\n",
            "Epoch 83/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0657 - accuracy: 0.4681 - mae: 0.0657 - mse: 0.0175 - val_loss: 0.0731 - val_accuracy: 0.4679 - val_mae: 0.0731 - val_mse: 0.0196\n",
            "Epoch 84/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0650 - accuracy: 0.4681 - mae: 0.0650 - mse: 0.0173 - val_loss: 0.0724 - val_accuracy: 0.4679 - val_mae: 0.0724 - val_mse: 0.0194\n",
            "Epoch 85/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0644 - accuracy: 0.4681 - mae: 0.0644 - mse: 0.0171 - val_loss: 0.0719 - val_accuracy: 0.4679 - val_mae: 0.0719 - val_mse: 0.0192\n",
            "Epoch 86/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0637 - accuracy: 0.4681 - mae: 0.0637 - mse: 0.0170 - val_loss: 0.0708 - val_accuracy: 0.4679 - val_mae: 0.0708 - val_mse: 0.0191\n",
            "Epoch 87/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0630 - accuracy: 0.4681 - mae: 0.0630 - mse: 0.0168 - val_loss: 0.0700 - val_accuracy: 0.4679 - val_mae: 0.0700 - val_mse: 0.0189\n",
            "Epoch 88/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0622 - accuracy: 0.4681 - mae: 0.0622 - mse: 0.0166 - val_loss: 0.0693 - val_accuracy: 0.4679 - val_mae: 0.0693 - val_mse: 0.0187\n",
            "Epoch 89/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0616 - accuracy: 0.4681 - mae: 0.0616 - mse: 0.0165 - val_loss: 0.0686 - val_accuracy: 0.4679 - val_mae: 0.0686 - val_mse: 0.0186\n",
            "Epoch 90/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0610 - accuracy: 0.4681 - mae: 0.0610 - mse: 0.0163 - val_loss: 0.0681 - val_accuracy: 0.4679 - val_mae: 0.0681 - val_mse: 0.0185\n",
            "Epoch 91/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0604 - accuracy: 0.4681 - mae: 0.0604 - mse: 0.0161 - val_loss: 0.0676 - val_accuracy: 0.4679 - val_mae: 0.0676 - val_mse: 0.0183\n",
            "Epoch 92/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0598 - accuracy: 0.4681 - mae: 0.0598 - mse: 0.0159 - val_loss: 0.0667 - val_accuracy: 0.4679 - val_mae: 0.0667 - val_mse: 0.0182\n",
            "Epoch 93/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0593 - accuracy: 0.4681 - mae: 0.0593 - mse: 0.0158 - val_loss: 0.0663 - val_accuracy: 0.4679 - val_mae: 0.0663 - val_mse: 0.0181\n",
            "Epoch 94/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0586 - accuracy: 0.4681 - mae: 0.0586 - mse: 0.0157 - val_loss: 0.0655 - val_accuracy: 0.4679 - val_mae: 0.0655 - val_mse: 0.0179\n",
            "Epoch 95/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0580 - accuracy: 0.4681 - mae: 0.0580 - mse: 0.0156 - val_loss: 0.0649 - val_accuracy: 0.4679 - val_mae: 0.0649 - val_mse: 0.0178\n",
            "Epoch 96/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0574 - accuracy: 0.4681 - mae: 0.0574 - mse: 0.0154 - val_loss: 0.0646 - val_accuracy: 0.4679 - val_mae: 0.0646 - val_mse: 0.0177\n",
            "Epoch 97/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0569 - accuracy: 0.4681 - mae: 0.0569 - mse: 0.0152 - val_loss: 0.0638 - val_accuracy: 0.4679 - val_mae: 0.0638 - val_mse: 0.0176\n",
            "Epoch 98/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0564 - accuracy: 0.4681 - mae: 0.0564 - mse: 0.0151 - val_loss: 0.0632 - val_accuracy: 0.4679 - val_mae: 0.0632 - val_mse: 0.0175\n",
            "Epoch 99/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0560 - accuracy: 0.4681 - mae: 0.0560 - mse: 0.0150 - val_loss: 0.0628 - val_accuracy: 0.4679 - val_mae: 0.0628 - val_mse: 0.0174\n",
            "Epoch 100/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0556 - accuracy: 0.4681 - mae: 0.0556 - mse: 0.0149 - val_loss: 0.0624 - val_accuracy: 0.4679 - val_mae: 0.0624 - val_mse: 0.0173\n",
            "Epoch 101/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0551 - accuracy: 0.4681 - mae: 0.0551 - mse: 0.0148 - val_loss: 0.0618 - val_accuracy: 0.4679 - val_mae: 0.0618 - val_mse: 0.0172\n",
            "Epoch 102/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0546 - accuracy: 0.4681 - mae: 0.0546 - mse: 0.0147 - val_loss: 0.0616 - val_accuracy: 0.4679 - val_mae: 0.0616 - val_mse: 0.0171\n",
            "Epoch 103/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0541 - accuracy: 0.4681 - mae: 0.0541 - mse: 0.0146 - val_loss: 0.0610 - val_accuracy: 0.4679 - val_mae: 0.0610 - val_mse: 0.0170\n",
            "Epoch 104/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0537 - accuracy: 0.4681 - mae: 0.0537 - mse: 0.0144 - val_loss: 0.0607 - val_accuracy: 0.4679 - val_mae: 0.0607 - val_mse: 0.0169\n",
            "Epoch 105/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0535 - accuracy: 0.4681 - mae: 0.0535 - mse: 0.0143 - val_loss: 0.0601 - val_accuracy: 0.4679 - val_mae: 0.0601 - val_mse: 0.0168\n",
            "Epoch 106/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0530 - accuracy: 0.4681 - mae: 0.0530 - mse: 0.0143 - val_loss: 0.0602 - val_accuracy: 0.4679 - val_mae: 0.0602 - val_mse: 0.0168\n",
            "Epoch 107/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0530 - accuracy: 0.4681 - mae: 0.0530 - mse: 0.0142 - val_loss: 0.0597 - val_accuracy: 0.4679 - val_mae: 0.0597 - val_mse: 0.0167\n",
            "Epoch 108/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0524 - accuracy: 0.4681 - mae: 0.0524 - mse: 0.0141 - val_loss: 0.0590 - val_accuracy: 0.4679 - val_mae: 0.0590 - val_mse: 0.0166\n",
            "Epoch 109/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0520 - accuracy: 0.4681 - mae: 0.0520 - mse: 0.0139 - val_loss: 0.0588 - val_accuracy: 0.4679 - val_mae: 0.0588 - val_mse: 0.0165\n",
            "Epoch 110/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0516 - accuracy: 0.4681 - mae: 0.0516 - mse: 0.0139 - val_loss: 0.0583 - val_accuracy: 0.4679 - val_mae: 0.0583 - val_mse: 0.0164\n",
            "Epoch 111/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0512 - accuracy: 0.4681 - mae: 0.0512 - mse: 0.0137 - val_loss: 0.0580 - val_accuracy: 0.4679 - val_mae: 0.0580 - val_mse: 0.0164\n",
            "Epoch 112/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0510 - accuracy: 0.4681 - mae: 0.0510 - mse: 0.0137 - val_loss: 0.0575 - val_accuracy: 0.4679 - val_mae: 0.0575 - val_mse: 0.0163\n",
            "Epoch 113/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0506 - accuracy: 0.4681 - mae: 0.0506 - mse: 0.0136 - val_loss: 0.0572 - val_accuracy: 0.4679 - val_mae: 0.0572 - val_mse: 0.0162\n",
            "Epoch 114/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0503 - accuracy: 0.4681 - mae: 0.0503 - mse: 0.0135 - val_loss: 0.0568 - val_accuracy: 0.4679 - val_mae: 0.0568 - val_mse: 0.0161\n",
            "Epoch 115/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0500 - accuracy: 0.4681 - mae: 0.0500 - mse: 0.0134 - val_loss: 0.0566 - val_accuracy: 0.4679 - val_mae: 0.0566 - val_mse: 0.0161\n",
            "Epoch 116/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0497 - accuracy: 0.4681 - mae: 0.0497 - mse: 0.0133 - val_loss: 0.0562 - val_accuracy: 0.4679 - val_mae: 0.0562 - val_mse: 0.0160\n",
            "Epoch 117/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0494 - accuracy: 0.4681 - mae: 0.0494 - mse: 0.0132 - val_loss: 0.0560 - val_accuracy: 0.4679 - val_mae: 0.0560 - val_mse: 0.0159\n",
            "Epoch 118/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0491 - accuracy: 0.4681 - mae: 0.0491 - mse: 0.0131 - val_loss: 0.0554 - val_accuracy: 0.4679 - val_mae: 0.0554 - val_mse: 0.0158\n",
            "Epoch 119/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0487 - accuracy: 0.4681 - mae: 0.0487 - mse: 0.0130 - val_loss: 0.0555 - val_accuracy: 0.4679 - val_mae: 0.0555 - val_mse: 0.0158\n",
            "Epoch 120/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0488 - accuracy: 0.4681 - mae: 0.0488 - mse: 0.0129 - val_loss: 0.0548 - val_accuracy: 0.4679 - val_mae: 0.0548 - val_mse: 0.0157\n",
            "Epoch 121/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0484 - accuracy: 0.4681 - mae: 0.0484 - mse: 0.0128 - val_loss: 0.0544 - val_accuracy: 0.4679 - val_mae: 0.0544 - val_mse: 0.0156\n",
            "Epoch 122/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0481 - accuracy: 0.4681 - mae: 0.0481 - mse: 0.0128 - val_loss: 0.0542 - val_accuracy: 0.4679 - val_mae: 0.0542 - val_mse: 0.0156\n",
            "Epoch 123/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0478 - accuracy: 0.4681 - mae: 0.0478 - mse: 0.0127 - val_loss: 0.0539 - val_accuracy: 0.4679 - val_mae: 0.0539 - val_mse: 0.0155\n",
            "Epoch 124/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0475 - accuracy: 0.4681 - mae: 0.0475 - mse: 0.0126 - val_loss: 0.0536 - val_accuracy: 0.4679 - val_mae: 0.0536 - val_mse: 0.0154\n",
            "Epoch 125/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0473 - accuracy: 0.4681 - mae: 0.0473 - mse: 0.0126 - val_loss: 0.0535 - val_accuracy: 0.4679 - val_mae: 0.0535 - val_mse: 0.0154\n",
            "Epoch 126/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0469 - accuracy: 0.4681 - mae: 0.0469 - mse: 0.0125 - val_loss: 0.0532 - val_accuracy: 0.4679 - val_mae: 0.0532 - val_mse: 0.0153\n",
            "Epoch 127/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0467 - accuracy: 0.4681 - mae: 0.0467 - mse: 0.0124 - val_loss: 0.0528 - val_accuracy: 0.4679 - val_mae: 0.0528 - val_mse: 0.0152\n",
            "Epoch 128/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0463 - accuracy: 0.4681 - mae: 0.0463 - mse: 0.0123 - val_loss: 0.0524 - val_accuracy: 0.4679 - val_mae: 0.0524 - val_mse: 0.0151\n",
            "Epoch 129/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0461 - accuracy: 0.4681 - mae: 0.0461 - mse: 0.0122 - val_loss: 0.0521 - val_accuracy: 0.4679 - val_mae: 0.0521 - val_mse: 0.0151\n",
            "Epoch 130/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0458 - accuracy: 0.4681 - mae: 0.0458 - mse: 0.0121 - val_loss: 0.0519 - val_accuracy: 0.4679 - val_mae: 0.0519 - val_mse: 0.0150\n",
            "Epoch 131/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0455 - accuracy: 0.4681 - mae: 0.0455 - mse: 0.0120 - val_loss: 0.0516 - val_accuracy: 0.4679 - val_mae: 0.0516 - val_mse: 0.0150\n",
            "Epoch 132/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0455 - accuracy: 0.4681 - mae: 0.0455 - mse: 0.0120 - val_loss: 0.0512 - val_accuracy: 0.4679 - val_mae: 0.0512 - val_mse: 0.0149\n",
            "Epoch 133/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0449 - accuracy: 0.4681 - mae: 0.0449 - mse: 0.0119 - val_loss: 0.0510 - val_accuracy: 0.4679 - val_mae: 0.0510 - val_mse: 0.0149\n",
            "Epoch 134/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0448 - accuracy: 0.4681 - mae: 0.0448 - mse: 0.0118 - val_loss: 0.0504 - val_accuracy: 0.4679 - val_mae: 0.0504 - val_mse: 0.0148\n",
            "Epoch 135/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0446 - accuracy: 0.4681 - mae: 0.0446 - mse: 0.0117 - val_loss: 0.0503 - val_accuracy: 0.4679 - val_mae: 0.0503 - val_mse: 0.0148\n",
            "Epoch 136/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0442 - accuracy: 0.4681 - mae: 0.0442 - mse: 0.0117 - val_loss: 0.0505 - val_accuracy: 0.4679 - val_mae: 0.0505 - val_mse: 0.0147\n",
            "Epoch 137/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0441 - accuracy: 0.4681 - mae: 0.0441 - mse: 0.0116 - val_loss: 0.0498 - val_accuracy: 0.4679 - val_mae: 0.0498 - val_mse: 0.0147\n",
            "Epoch 138/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0438 - accuracy: 0.4681 - mae: 0.0438 - mse: 0.0115 - val_loss: 0.0496 - val_accuracy: 0.4679 - val_mae: 0.0496 - val_mse: 0.0146\n",
            "Epoch 139/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0437 - accuracy: 0.4681 - mae: 0.0437 - mse: 0.0115 - val_loss: 0.0492 - val_accuracy: 0.4679 - val_mae: 0.0492 - val_mse: 0.0145\n",
            "Epoch 140/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0433 - accuracy: 0.4681 - mae: 0.0433 - mse: 0.0114 - val_loss: 0.0491 - val_accuracy: 0.4679 - val_mae: 0.0491 - val_mse: 0.0145\n",
            "Epoch 141/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0432 - accuracy: 0.4681 - mae: 0.0432 - mse: 0.0114 - val_loss: 0.0486 - val_accuracy: 0.4679 - val_mae: 0.0486 - val_mse: 0.0144\n",
            "Epoch 142/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0428 - accuracy: 0.4681 - mae: 0.0428 - mse: 0.0113 - val_loss: 0.0484 - val_accuracy: 0.4679 - val_mae: 0.0484 - val_mse: 0.0144\n",
            "Epoch 143/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0427 - accuracy: 0.4681 - mae: 0.0427 - mse: 0.0112 - val_loss: 0.0482 - val_accuracy: 0.4679 - val_mae: 0.0482 - val_mse: 0.0143\n",
            "Epoch 144/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0425 - accuracy: 0.4681 - mae: 0.0425 - mse: 0.0112 - val_loss: 0.0481 - val_accuracy: 0.4679 - val_mae: 0.0481 - val_mse: 0.0143\n",
            "Epoch 145/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0425 - accuracy: 0.4681 - mae: 0.0425 - mse: 0.0111 - val_loss: 0.0476 - val_accuracy: 0.4679 - val_mae: 0.0476 - val_mse: 0.0142\n",
            "Epoch 146/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0421 - accuracy: 0.4681 - mae: 0.0421 - mse: 0.0110 - val_loss: 0.0474 - val_accuracy: 0.4679 - val_mae: 0.0474 - val_mse: 0.0142\n",
            "Epoch 147/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0420 - accuracy: 0.4681 - mae: 0.0420 - mse: 0.0110 - val_loss: 0.0470 - val_accuracy: 0.4679 - val_mae: 0.0470 - val_mse: 0.0141\n",
            "Epoch 148/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0418 - accuracy: 0.4681 - mae: 0.0418 - mse: 0.0109 - val_loss: 0.0467 - val_accuracy: 0.4679 - val_mae: 0.0467 - val_mse: 0.0141\n",
            "Epoch 149/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0415 - accuracy: 0.4681 - mae: 0.0415 - mse: 0.0108 - val_loss: 0.0466 - val_accuracy: 0.4679 - val_mae: 0.0466 - val_mse: 0.0140\n",
            "Epoch 150/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0411 - accuracy: 0.4681 - mae: 0.0411 - mse: 0.0107 - val_loss: 0.0465 - val_accuracy: 0.4679 - val_mae: 0.0465 - val_mse: 0.0139\n",
            "Epoch 151/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0412 - accuracy: 0.4681 - mae: 0.0412 - mse: 0.0107 - val_loss: 0.0459 - val_accuracy: 0.4679 - val_mae: 0.0459 - val_mse: 0.0139\n",
            "Epoch 152/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0409 - accuracy: 0.4681 - mae: 0.0409 - mse: 0.0106 - val_loss: 0.0456 - val_accuracy: 0.4679 - val_mae: 0.0456 - val_mse: 0.0138\n",
            "Epoch 153/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0406 - accuracy: 0.4681 - mae: 0.0406 - mse: 0.0105 - val_loss: 0.0458 - val_accuracy: 0.4679 - val_mae: 0.0458 - val_mse: 0.0138\n",
            "Epoch 154/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0405 - accuracy: 0.4681 - mae: 0.0405 - mse: 0.0105 - val_loss: 0.0455 - val_accuracy: 0.4679 - val_mae: 0.0455 - val_mse: 0.0138\n",
            "Epoch 155/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0403 - accuracy: 0.4681 - mae: 0.0403 - mse: 0.0105 - val_loss: 0.0452 - val_accuracy: 0.4679 - val_mae: 0.0452 - val_mse: 0.0137\n",
            "Epoch 156/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0400 - accuracy: 0.4681 - mae: 0.0400 - mse: 0.0104 - val_loss: 0.0451 - val_accuracy: 0.4679 - val_mae: 0.0451 - val_mse: 0.0137\n",
            "Epoch 157/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0399 - accuracy: 0.4681 - mae: 0.0399 - mse: 0.0103 - val_loss: 0.0447 - val_accuracy: 0.4679 - val_mae: 0.0447 - val_mse: 0.0137\n",
            "Epoch 158/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0398 - accuracy: 0.4681 - mae: 0.0398 - mse: 0.0103 - val_loss: 0.0444 - val_accuracy: 0.4679 - val_mae: 0.0444 - val_mse: 0.0136\n",
            "Epoch 159/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0395 - accuracy: 0.4681 - mae: 0.0395 - mse: 0.0102 - val_loss: 0.0441 - val_accuracy: 0.4679 - val_mae: 0.0441 - val_mse: 0.0135\n",
            "Epoch 160/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0393 - accuracy: 0.4681 - mae: 0.0393 - mse: 0.0101 - val_loss: 0.0439 - val_accuracy: 0.4679 - val_mae: 0.0439 - val_mse: 0.0135\n",
            "Epoch 161/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0391 - accuracy: 0.4681 - mae: 0.0391 - mse: 0.0101 - val_loss: 0.0436 - val_accuracy: 0.4679 - val_mae: 0.0436 - val_mse: 0.0135\n",
            "Epoch 162/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0388 - accuracy: 0.4681 - mae: 0.0388 - mse: 0.0101 - val_loss: 0.0434 - val_accuracy: 0.4679 - val_mae: 0.0434 - val_mse: 0.0134\n",
            "Epoch 163/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0386 - accuracy: 0.4681 - mae: 0.0386 - mse: 0.0100 - val_loss: 0.0443 - val_accuracy: 0.4679 - val_mae: 0.0443 - val_mse: 0.0134\n",
            "Epoch 164/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0388 - accuracy: 0.4681 - mae: 0.0388 - mse: 0.0100 - val_loss: 0.0435 - val_accuracy: 0.4679 - val_mae: 0.0435 - val_mse: 0.0133\n",
            "Epoch 165/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0385 - accuracy: 0.4681 - mae: 0.0385 - mse: 0.0099 - val_loss: 0.0432 - val_accuracy: 0.4679 - val_mae: 0.0432 - val_mse: 0.0133\n",
            "Epoch 166/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0381 - accuracy: 0.4681 - mae: 0.0381 - mse: 0.0098 - val_loss: 0.0425 - val_accuracy: 0.4679 - val_mae: 0.0425 - val_mse: 0.0133\n",
            "Epoch 167/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0380 - accuracy: 0.4681 - mae: 0.0380 - mse: 0.0098 - val_loss: 0.0428 - val_accuracy: 0.4679 - val_mae: 0.0428 - val_mse: 0.0133\n",
            "Epoch 168/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0380 - accuracy: 0.4681 - mae: 0.0380 - mse: 0.0097 - val_loss: 0.0432 - val_accuracy: 0.4679 - val_mae: 0.0432 - val_mse: 0.0132\n",
            "Epoch 169/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0378 - accuracy: 0.4681 - mae: 0.0378 - mse: 0.0097 - val_loss: 0.0419 - val_accuracy: 0.4679 - val_mae: 0.0419 - val_mse: 0.0132\n",
            "Epoch 170/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0377 - accuracy: 0.4681 - mae: 0.0377 - mse: 0.0097 - val_loss: 0.0416 - val_accuracy: 0.4679 - val_mae: 0.0416 - val_mse: 0.0131\n",
            "Epoch 171/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0372 - accuracy: 0.4681 - mae: 0.0372 - mse: 0.0096 - val_loss: 0.0426 - val_accuracy: 0.4679 - val_mae: 0.0426 - val_mse: 0.0131\n",
            "Epoch 172/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0375 - accuracy: 0.4681 - mae: 0.0375 - mse: 0.0096 - val_loss: 0.0413 - val_accuracy: 0.4679 - val_mae: 0.0413 - val_mse: 0.0131\n",
            "Epoch 173/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0372 - accuracy: 0.4681 - mae: 0.0372 - mse: 0.0095 - val_loss: 0.0414 - val_accuracy: 0.4679 - val_mae: 0.0414 - val_mse: 0.0131\n",
            "Epoch 174/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0370 - accuracy: 0.4681 - mae: 0.0370 - mse: 0.0094 - val_loss: 0.0412 - val_accuracy: 0.4679 - val_mae: 0.0412 - val_mse: 0.0130\n",
            "Epoch 175/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0368 - accuracy: 0.4681 - mae: 0.0368 - mse: 0.0095 - val_loss: 0.0411 - val_accuracy: 0.4679 - val_mae: 0.0411 - val_mse: 0.0130\n",
            "Epoch 176/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0366 - accuracy: 0.4681 - mae: 0.0366 - mse: 0.0094 - val_loss: 0.0409 - val_accuracy: 0.4679 - val_mae: 0.0409 - val_mse: 0.0130\n",
            "Epoch 177/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0365 - accuracy: 0.4681 - mae: 0.0365 - mse: 0.0093 - val_loss: 0.0409 - val_accuracy: 0.4679 - val_mae: 0.0409 - val_mse: 0.0129\n",
            "Epoch 178/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0363 - accuracy: 0.4681 - mae: 0.0363 - mse: 0.0093 - val_loss: 0.0408 - val_accuracy: 0.4679 - val_mae: 0.0408 - val_mse: 0.0129\n",
            "Epoch 179/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0362 - accuracy: 0.4681 - mae: 0.0362 - mse: 0.0093 - val_loss: 0.0404 - val_accuracy: 0.4679 - val_mae: 0.0404 - val_mse: 0.0128\n",
            "Epoch 180/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0360 - accuracy: 0.4681 - mae: 0.0360 - mse: 0.0092 - val_loss: 0.0408 - val_accuracy: 0.4679 - val_mae: 0.0408 - val_mse: 0.0128\n",
            "Epoch 181/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0359 - accuracy: 0.4681 - mae: 0.0359 - mse: 0.0092 - val_loss: 0.0401 - val_accuracy: 0.4679 - val_mae: 0.0401 - val_mse: 0.0127\n",
            "Epoch 182/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0357 - accuracy: 0.4681 - mae: 0.0357 - mse: 0.0091 - val_loss: 0.0400 - val_accuracy: 0.4679 - val_mae: 0.0400 - val_mse: 0.0127\n",
            "Epoch 183/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0356 - accuracy: 0.4681 - mae: 0.0356 - mse: 0.0091 - val_loss: 0.0399 - val_accuracy: 0.4679 - val_mae: 0.0399 - val_mse: 0.0127\n",
            "Epoch 184/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0356 - accuracy: 0.4681 - mae: 0.0356 - mse: 0.0090 - val_loss: 0.0397 - val_accuracy: 0.4679 - val_mae: 0.0397 - val_mse: 0.0127\n",
            "Epoch 185/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0354 - accuracy: 0.4681 - mae: 0.0354 - mse: 0.0090 - val_loss: 0.0398 - val_accuracy: 0.4679 - val_mae: 0.0398 - val_mse: 0.0127\n",
            "Epoch 186/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0353 - accuracy: 0.4681 - mae: 0.0353 - mse: 0.0090 - val_loss: 0.0394 - val_accuracy: 0.4679 - val_mae: 0.0394 - val_mse: 0.0126\n",
            "Epoch 187/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0351 - accuracy: 0.4681 - mae: 0.0351 - mse: 0.0089 - val_loss: 0.0397 - val_accuracy: 0.4679 - val_mae: 0.0397 - val_mse: 0.0127\n",
            "Epoch 188/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0348 - accuracy: 0.4681 - mae: 0.0348 - mse: 0.0089 - val_loss: 0.0394 - val_accuracy: 0.4679 - val_mae: 0.0394 - val_mse: 0.0126\n",
            "Epoch 189/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0348 - accuracy: 0.4681 - mae: 0.0348 - mse: 0.0088 - val_loss: 0.0399 - val_accuracy: 0.4679 - val_mae: 0.0399 - val_mse: 0.0126\n",
            "Epoch 190/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0346 - accuracy: 0.4681 - mae: 0.0346 - mse: 0.0088 - val_loss: 0.0394 - val_accuracy: 0.4679 - val_mae: 0.0394 - val_mse: 0.0126\n",
            "Epoch 191/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0345 - accuracy: 0.4681 - mae: 0.0345 - mse: 0.0087 - val_loss: 0.0388 - val_accuracy: 0.4679 - val_mae: 0.0388 - val_mse: 0.0125\n",
            "Epoch 192/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0344 - accuracy: 0.4681 - mae: 0.0344 - mse: 0.0087 - val_loss: 0.0389 - val_accuracy: 0.4679 - val_mae: 0.0389 - val_mse: 0.0125\n",
            "Epoch 193/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0342 - accuracy: 0.4681 - mae: 0.0342 - mse: 0.0087 - val_loss: 0.0386 - val_accuracy: 0.4679 - val_mae: 0.0386 - val_mse: 0.0124\n",
            "Epoch 194/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0341 - accuracy: 0.4681 - mae: 0.0341 - mse: 0.0086 - val_loss: 0.0388 - val_accuracy: 0.4679 - val_mae: 0.0388 - val_mse: 0.0124\n",
            "Epoch 195/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0341 - accuracy: 0.4681 - mae: 0.0341 - mse: 0.0086 - val_loss: 0.0389 - val_accuracy: 0.4679 - val_mae: 0.0389 - val_mse: 0.0124\n",
            "Epoch 196/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0338 - accuracy: 0.4681 - mae: 0.0338 - mse: 0.0086 - val_loss: 0.0384 - val_accuracy: 0.4679 - val_mae: 0.0384 - val_mse: 0.0123\n",
            "Epoch 197/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0338 - accuracy: 0.4681 - mae: 0.0338 - mse: 0.0085 - val_loss: 0.0385 - val_accuracy: 0.4679 - val_mae: 0.0385 - val_mse: 0.0123\n",
            "Epoch 198/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0336 - accuracy: 0.4681 - mae: 0.0336 - mse: 0.0085 - val_loss: 0.0389 - val_accuracy: 0.4679 - val_mae: 0.0389 - val_mse: 0.0124\n",
            "Epoch 199/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0336 - accuracy: 0.4681 - mae: 0.0336 - mse: 0.0085 - val_loss: 0.0383 - val_accuracy: 0.4679 - val_mae: 0.0383 - val_mse: 0.0123\n",
            "Epoch 200/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0334 - accuracy: 0.4681 - mae: 0.0334 - mse: 0.0085 - val_loss: 0.0381 - val_accuracy: 0.4679 - val_mae: 0.0381 - val_mse: 0.0122\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hasil nya"
      ],
      "metadata": {
        "id": "C3VpnDz6sqKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'], label='loss')\n",
        "plt.plot(history.history['val_loss'], label='Valid')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['accuracy'], label='acc')\n",
        "plt.plot(history.history['val_accuracy'], label='Valid')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['mae'], label='mae')\n",
        "plt.plot(history.history['val_mae'], label='Valid mae')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['mse'], label='mse')\n",
        "plt.plot(history.history['val_mse'], label='Valid mse')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a15fb252-3a55-495a-81f4-1c4dc993f363",
        "id": "FpFfv1tssqKV"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# model 3"
      ],
      "metadata": {
        "id": "G2wVye425fPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(29, input_dim=29)) \n",
        "model.add(Dense(29, input_dim=29)) \n",
        "model.add(Dense(1, activation = 'linear'))\n",
        "\n",
        "model.compile(loss=\"mean_absolute_error\", optimizer=optimizers.SGD(learning_rate=0.001), metrics=['accuracy','mae', 'mse'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fa2186d-c71d-4e71-a5e7-78da5b374cbe",
        "id": "SgBo4X3Z5fPX"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_26\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_78 (Dense)            (None, 29)                870       \n",
            "                                                                 \n",
            " dense_79 (Dense)            (None, 29)                870       \n",
            "                                                                 \n",
            " dense_80 (Dense)            (None, 1)                 30        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,770\n",
            "Trainable params: 1,770\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=200, batch_size=32, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3c5ee32-5bbd-44c3-a9b3-3cf0189ebb2a",
        "id": "WFPliH5Y5fPX"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "12/12 [==============================] - 1s 19ms/step - loss: 0.4209 - accuracy: 0.4432 - mae: 0.4209 - mse: 0.2544 - val_loss: 0.3605 - val_accuracy: 0.4423 - val_mae: 0.3605 - val_mse: 0.1975\n",
            "Epoch 2/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.3488 - accuracy: 0.4349 - mae: 0.3488 - mse: 0.1811 - val_loss: 0.3213 - val_accuracy: 0.4167 - val_mae: 0.3213 - val_mse: 0.1567\n",
            "Epoch 3/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.3120 - accuracy: 0.4238 - mae: 0.3120 - mse: 0.1528 - val_loss: 0.3047 - val_accuracy: 0.4167 - val_mae: 0.3047 - val_mse: 0.1417\n",
            "Epoch 4/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2958 - accuracy: 0.4238 - mae: 0.2958 - mse: 0.1386 - val_loss: 0.2916 - val_accuracy: 0.4167 - val_mae: 0.2916 - val_mse: 0.1318\n",
            "Epoch 5/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2829 - accuracy: 0.4183 - mae: 0.2829 - mse: 0.1295 - val_loss: 0.2806 - val_accuracy: 0.4167 - val_mae: 0.2806 - val_mse: 0.1228\n",
            "Epoch 6/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2720 - accuracy: 0.4238 - mae: 0.2720 - mse: 0.1201 - val_loss: 0.2711 - val_accuracy: 0.4295 - val_mae: 0.2711 - val_mse: 0.1156\n",
            "Epoch 7/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2613 - accuracy: 0.4294 - mae: 0.2613 - mse: 0.1130 - val_loss: 0.2604 - val_accuracy: 0.4359 - val_mae: 0.2604 - val_mse: 0.1074\n",
            "Epoch 8/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.2510 - accuracy: 0.4294 - mae: 0.2510 - mse: 0.1048 - val_loss: 0.2543 - val_accuracy: 0.4359 - val_mae: 0.2543 - val_mse: 0.1036\n",
            "Epoch 9/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.2423 - accuracy: 0.4321 - mae: 0.2423 - mse: 0.1001 - val_loss: 0.2440 - val_accuracy: 0.4359 - val_mae: 0.2440 - val_mse: 0.0953\n",
            "Epoch 10/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2341 - accuracy: 0.4321 - mae: 0.2341 - mse: 0.0929 - val_loss: 0.2364 - val_accuracy: 0.4359 - val_mae: 0.2364 - val_mse: 0.0900\n",
            "Epoch 11/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2266 - accuracy: 0.4321 - mae: 0.2266 - mse: 0.0876 - val_loss: 0.2280 - val_accuracy: 0.4423 - val_mae: 0.2280 - val_mse: 0.0837\n",
            "Epoch 12/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.2194 - accuracy: 0.4321 - mae: 0.2194 - mse: 0.0824 - val_loss: 0.2208 - val_accuracy: 0.4423 - val_mae: 0.2208 - val_mse: 0.0786\n",
            "Epoch 13/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2130 - accuracy: 0.4349 - mae: 0.2130 - mse: 0.0772 - val_loss: 0.2144 - val_accuracy: 0.4423 - val_mae: 0.2144 - val_mse: 0.0741\n",
            "Epoch 14/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2071 - accuracy: 0.4432 - mae: 0.2071 - mse: 0.0731 - val_loss: 0.2082 - val_accuracy: 0.4487 - val_mae: 0.2082 - val_mse: 0.0702\n",
            "Epoch 15/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2008 - accuracy: 0.4432 - mae: 0.2008 - mse: 0.0691 - val_loss: 0.2028 - val_accuracy: 0.4551 - val_mae: 0.2028 - val_mse: 0.0666\n",
            "Epoch 16/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1948 - accuracy: 0.4432 - mae: 0.1948 - mse: 0.0655 - val_loss: 0.1985 - val_accuracy: 0.4551 - val_mae: 0.1985 - val_mse: 0.0640\n",
            "Epoch 17/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1897 - accuracy: 0.4460 - mae: 0.1897 - mse: 0.0626 - val_loss: 0.1938 - val_accuracy: 0.4551 - val_mae: 0.1938 - val_mse: 0.0611\n",
            "Epoch 18/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1851 - accuracy: 0.4515 - mae: 0.1851 - mse: 0.0598 - val_loss: 0.1895 - val_accuracy: 0.4551 - val_mae: 0.1895 - val_mse: 0.0584\n",
            "Epoch 19/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1808 - accuracy: 0.4543 - mae: 0.1808 - mse: 0.0569 - val_loss: 0.1844 - val_accuracy: 0.4615 - val_mae: 0.1844 - val_mse: 0.0554\n",
            "Epoch 20/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1764 - accuracy: 0.4598 - mae: 0.1764 - mse: 0.0542 - val_loss: 0.1810 - val_accuracy: 0.4615 - val_mae: 0.1810 - val_mse: 0.0533\n",
            "Epoch 21/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1724 - accuracy: 0.4598 - mae: 0.1724 - mse: 0.0519 - val_loss: 0.1776 - val_accuracy: 0.4615 - val_mae: 0.1776 - val_mse: 0.0513\n",
            "Epoch 22/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1688 - accuracy: 0.4626 - mae: 0.1688 - mse: 0.0496 - val_loss: 0.1761 - val_accuracy: 0.4679 - val_mae: 0.1761 - val_mse: 0.0502\n",
            "Epoch 23/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1656 - accuracy: 0.4626 - mae: 0.1656 - mse: 0.0481 - val_loss: 0.1685 - val_accuracy: 0.4744 - val_mae: 0.1685 - val_mse: 0.0467\n",
            "Epoch 24/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1616 - accuracy: 0.4654 - mae: 0.1616 - mse: 0.0456 - val_loss: 0.1631 - val_accuracy: 0.4744 - val_mae: 0.1631 - val_mse: 0.0443\n",
            "Epoch 25/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1583 - accuracy: 0.4654 - mae: 0.1583 - mse: 0.0433 - val_loss: 0.1594 - val_accuracy: 0.4744 - val_mae: 0.1594 - val_mse: 0.0425\n",
            "Epoch 26/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1551 - accuracy: 0.4681 - mae: 0.1551 - mse: 0.0416 - val_loss: 0.1589 - val_accuracy: 0.4744 - val_mae: 0.1589 - val_mse: 0.0416\n",
            "Epoch 27/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1515 - accuracy: 0.4681 - mae: 0.1515 - mse: 0.0402 - val_loss: 0.1554 - val_accuracy: 0.4744 - val_mae: 0.1554 - val_mse: 0.0400\n",
            "Epoch 28/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1487 - accuracy: 0.4765 - mae: 0.1487 - mse: 0.0387 - val_loss: 0.1519 - val_accuracy: 0.4744 - val_mae: 0.1519 - val_mse: 0.0385\n",
            "Epoch 29/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1451 - accuracy: 0.4765 - mae: 0.1451 - mse: 0.0369 - val_loss: 0.1504 - val_accuracy: 0.4808 - val_mae: 0.1504 - val_mse: 0.0374\n",
            "Epoch 30/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1422 - accuracy: 0.4765 - mae: 0.1422 - mse: 0.0357 - val_loss: 0.1472 - val_accuracy: 0.4808 - val_mae: 0.1472 - val_mse: 0.0363\n",
            "Epoch 31/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1399 - accuracy: 0.4765 - mae: 0.1399 - mse: 0.0344 - val_loss: 0.1434 - val_accuracy: 0.4808 - val_mae: 0.1434 - val_mse: 0.0350\n",
            "Epoch 32/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1370 - accuracy: 0.4765 - mae: 0.1370 - mse: 0.0331 - val_loss: 0.1415 - val_accuracy: 0.4808 - val_mae: 0.1415 - val_mse: 0.0340\n",
            "Epoch 33/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1345 - accuracy: 0.4765 - mae: 0.1345 - mse: 0.0320 - val_loss: 0.1373 - val_accuracy: 0.4808 - val_mae: 0.1373 - val_mse: 0.0330\n",
            "Epoch 34/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1322 - accuracy: 0.4765 - mae: 0.1322 - mse: 0.0310 - val_loss: 0.1353 - val_accuracy: 0.4808 - val_mae: 0.1353 - val_mse: 0.0322\n",
            "Epoch 35/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1300 - accuracy: 0.4765 - mae: 0.1300 - mse: 0.0300 - val_loss: 0.1330 - val_accuracy: 0.4808 - val_mae: 0.1330 - val_mse: 0.0314\n",
            "Epoch 36/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1278 - accuracy: 0.4765 - mae: 0.1278 - mse: 0.0291 - val_loss: 0.1324 - val_accuracy: 0.4808 - val_mae: 0.1324 - val_mse: 0.0307\n",
            "Epoch 37/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1259 - accuracy: 0.4765 - mae: 0.1259 - mse: 0.0282 - val_loss: 0.1302 - val_accuracy: 0.4808 - val_mae: 0.1302 - val_mse: 0.0300\n",
            "Epoch 38/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1237 - accuracy: 0.4765 - mae: 0.1237 - mse: 0.0274 - val_loss: 0.1283 - val_accuracy: 0.4808 - val_mae: 0.1283 - val_mse: 0.0294\n",
            "Epoch 39/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1216 - accuracy: 0.4765 - mae: 0.1216 - mse: 0.0266 - val_loss: 0.1265 - val_accuracy: 0.4808 - val_mae: 0.1265 - val_mse: 0.0287\n",
            "Epoch 40/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1201 - accuracy: 0.4765 - mae: 0.1201 - mse: 0.0259 - val_loss: 0.1247 - val_accuracy: 0.4808 - val_mae: 0.1247 - val_mse: 0.0280\n",
            "Epoch 41/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1179 - accuracy: 0.4765 - mae: 0.1179 - mse: 0.0250 - val_loss: 0.1217 - val_accuracy: 0.4808 - val_mae: 0.1217 - val_mse: 0.0274\n",
            "Epoch 42/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1159 - accuracy: 0.4765 - mae: 0.1159 - mse: 0.0243 - val_loss: 0.1192 - val_accuracy: 0.4808 - val_mae: 0.1192 - val_mse: 0.0272\n",
            "Epoch 43/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1139 - accuracy: 0.4765 - mae: 0.1139 - mse: 0.0237 - val_loss: 0.1174 - val_accuracy: 0.4808 - val_mae: 0.1174 - val_mse: 0.0266\n",
            "Epoch 44/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1121 - accuracy: 0.4765 - mae: 0.1121 - mse: 0.0229 - val_loss: 0.1163 - val_accuracy: 0.4808 - val_mae: 0.1163 - val_mse: 0.0258\n",
            "Epoch 45/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1099 - accuracy: 0.4765 - mae: 0.1099 - mse: 0.0222 - val_loss: 0.1142 - val_accuracy: 0.4808 - val_mae: 0.1142 - val_mse: 0.0253\n",
            "Epoch 46/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1078 - accuracy: 0.4765 - mae: 0.1078 - mse: 0.0215 - val_loss: 0.1121 - val_accuracy: 0.4808 - val_mae: 0.1121 - val_mse: 0.0260\n",
            "Epoch 47/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1071 - accuracy: 0.4765 - mae: 0.1071 - mse: 0.0213 - val_loss: 0.1106 - val_accuracy: 0.4808 - val_mae: 0.1106 - val_mse: 0.0249\n",
            "Epoch 48/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.1044 - accuracy: 0.4765 - mae: 0.1044 - mse: 0.0205 - val_loss: 0.1096 - val_accuracy: 0.4808 - val_mae: 0.1096 - val_mse: 0.0241\n",
            "Epoch 49/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1029 - accuracy: 0.4765 - mae: 0.1029 - mse: 0.0199 - val_loss: 0.1075 - val_accuracy: 0.4808 - val_mae: 0.1075 - val_mse: 0.0240\n",
            "Epoch 50/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1013 - accuracy: 0.4765 - mae: 0.1013 - mse: 0.0195 - val_loss: 0.1067 - val_accuracy: 0.4808 - val_mae: 0.1067 - val_mse: 0.0232\n",
            "Epoch 51/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0999 - accuracy: 0.4765 - mae: 0.0999 - mse: 0.0189 - val_loss: 0.1045 - val_accuracy: 0.4808 - val_mae: 0.1045 - val_mse: 0.0229\n",
            "Epoch 52/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0980 - accuracy: 0.4765 - mae: 0.0980 - mse: 0.0183 - val_loss: 0.1029 - val_accuracy: 0.4808 - val_mae: 0.1029 - val_mse: 0.0225\n",
            "Epoch 53/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0965 - accuracy: 0.4765 - mae: 0.0965 - mse: 0.0179 - val_loss: 0.1018 - val_accuracy: 0.4808 - val_mae: 0.1018 - val_mse: 0.0219\n",
            "Epoch 54/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0950 - accuracy: 0.4765 - mae: 0.0950 - mse: 0.0174 - val_loss: 0.0999 - val_accuracy: 0.4808 - val_mae: 0.0999 - val_mse: 0.0220\n",
            "Epoch 55/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0939 - accuracy: 0.4765 - mae: 0.0939 - mse: 0.0172 - val_loss: 0.0985 - val_accuracy: 0.4808 - val_mae: 0.0985 - val_mse: 0.0216\n",
            "Epoch 56/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0922 - accuracy: 0.4765 - mae: 0.0922 - mse: 0.0166 - val_loss: 0.0974 - val_accuracy: 0.4808 - val_mae: 0.0974 - val_mse: 0.0213\n",
            "Epoch 57/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0901 - accuracy: 0.4765 - mae: 0.0901 - mse: 0.0161 - val_loss: 0.0964 - val_accuracy: 0.4808 - val_mae: 0.0964 - val_mse: 0.0206\n",
            "Epoch 58/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0896 - accuracy: 0.4765 - mae: 0.0896 - mse: 0.0158 - val_loss: 0.0952 - val_accuracy: 0.4808 - val_mae: 0.0952 - val_mse: 0.0202\n",
            "Epoch 59/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0879 - accuracy: 0.4765 - mae: 0.0879 - mse: 0.0154 - val_loss: 0.0936 - val_accuracy: 0.4808 - val_mae: 0.0936 - val_mse: 0.0205\n",
            "Epoch 60/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0866 - accuracy: 0.4765 - mae: 0.0866 - mse: 0.0151 - val_loss: 0.0923 - val_accuracy: 0.4808 - val_mae: 0.0923 - val_mse: 0.0200\n",
            "Epoch 61/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0854 - accuracy: 0.4765 - mae: 0.0854 - mse: 0.0147 - val_loss: 0.0925 - val_accuracy: 0.4808 - val_mae: 0.0925 - val_mse: 0.0192\n",
            "Epoch 62/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0845 - accuracy: 0.4765 - mae: 0.0845 - mse: 0.0142 - val_loss: 0.0900 - val_accuracy: 0.4808 - val_mae: 0.0900 - val_mse: 0.0195\n",
            "Epoch 63/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0828 - accuracy: 0.4765 - mae: 0.0828 - mse: 0.0141 - val_loss: 0.0887 - val_accuracy: 0.4808 - val_mae: 0.0887 - val_mse: 0.0191\n",
            "Epoch 64/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0816 - accuracy: 0.4765 - mae: 0.0816 - mse: 0.0137 - val_loss: 0.0879 - val_accuracy: 0.4808 - val_mae: 0.0879 - val_mse: 0.0192\n",
            "Epoch 65/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0805 - accuracy: 0.4765 - mae: 0.0805 - mse: 0.0134 - val_loss: 0.0870 - val_accuracy: 0.4808 - val_mae: 0.0870 - val_mse: 0.0192\n",
            "Epoch 66/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0794 - accuracy: 0.4765 - mae: 0.0794 - mse: 0.0131 - val_loss: 0.0858 - val_accuracy: 0.4808 - val_mae: 0.0858 - val_mse: 0.0188\n",
            "Epoch 67/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0780 - accuracy: 0.4765 - mae: 0.0780 - mse: 0.0129 - val_loss: 0.0846 - val_accuracy: 0.4808 - val_mae: 0.0846 - val_mse: 0.0185\n",
            "Epoch 68/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0768 - accuracy: 0.4765 - mae: 0.0768 - mse: 0.0126 - val_loss: 0.0836 - val_accuracy: 0.4808 - val_mae: 0.0836 - val_mse: 0.0182\n",
            "Epoch 69/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0757 - accuracy: 0.4765 - mae: 0.0757 - mse: 0.0123 - val_loss: 0.0832 - val_accuracy: 0.4808 - val_mae: 0.0832 - val_mse: 0.0184\n",
            "Epoch 70/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0746 - accuracy: 0.4765 - mae: 0.0746 - mse: 0.0121 - val_loss: 0.0823 - val_accuracy: 0.4808 - val_mae: 0.0823 - val_mse: 0.0183\n",
            "Epoch 71/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0733 - accuracy: 0.4765 - mae: 0.0733 - mse: 0.0118 - val_loss: 0.0810 - val_accuracy: 0.4808 - val_mae: 0.0810 - val_mse: 0.0178\n",
            "Epoch 72/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0720 - accuracy: 0.4765 - mae: 0.0720 - mse: 0.0115 - val_loss: 0.0800 - val_accuracy: 0.4808 - val_mae: 0.0800 - val_mse: 0.0175\n",
            "Epoch 73/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0710 - accuracy: 0.4765 - mae: 0.0710 - mse: 0.0113 - val_loss: 0.0791 - val_accuracy: 0.4808 - val_mae: 0.0791 - val_mse: 0.0174\n",
            "Epoch 74/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0702 - accuracy: 0.4765 - mae: 0.0702 - mse: 0.0111 - val_loss: 0.0782 - val_accuracy: 0.4808 - val_mae: 0.0782 - val_mse: 0.0172\n",
            "Epoch 75/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0686 - accuracy: 0.4765 - mae: 0.0686 - mse: 0.0108 - val_loss: 0.0773 - val_accuracy: 0.4808 - val_mae: 0.0773 - val_mse: 0.0172\n",
            "Epoch 76/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0678 - accuracy: 0.4765 - mae: 0.0678 - mse: 0.0106 - val_loss: 0.0760 - val_accuracy: 0.4808 - val_mae: 0.0760 - val_mse: 0.0166\n",
            "Epoch 77/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0670 - accuracy: 0.4765 - mae: 0.0670 - mse: 0.0104 - val_loss: 0.0758 - val_accuracy: 0.4808 - val_mae: 0.0758 - val_mse: 0.0170\n",
            "Epoch 78/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0653 - accuracy: 0.4765 - mae: 0.0653 - mse: 0.0102 - val_loss: 0.0742 - val_accuracy: 0.4808 - val_mae: 0.0742 - val_mse: 0.0162\n",
            "Epoch 79/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0644 - accuracy: 0.4765 - mae: 0.0644 - mse: 0.0100 - val_loss: 0.0735 - val_accuracy: 0.4808 - val_mae: 0.0735 - val_mse: 0.0164\n",
            "Epoch 80/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0636 - accuracy: 0.4765 - mae: 0.0636 - mse: 0.0099 - val_loss: 0.0723 - val_accuracy: 0.4808 - val_mae: 0.0723 - val_mse: 0.0158\n",
            "Epoch 81/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0625 - accuracy: 0.4765 - mae: 0.0625 - mse: 0.0096 - val_loss: 0.0716 - val_accuracy: 0.4808 - val_mae: 0.0716 - val_mse: 0.0161\n",
            "Epoch 82/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0615 - accuracy: 0.4765 - mae: 0.0615 - mse: 0.0095 - val_loss: 0.0705 - val_accuracy: 0.4808 - val_mae: 0.0705 - val_mse: 0.0155\n",
            "Epoch 83/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0610 - accuracy: 0.4765 - mae: 0.0610 - mse: 0.0093 - val_loss: 0.0701 - val_accuracy: 0.4808 - val_mae: 0.0701 - val_mse: 0.0160\n",
            "Epoch 84/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0594 - accuracy: 0.4765 - mae: 0.0594 - mse: 0.0091 - val_loss: 0.0702 - val_accuracy: 0.4808 - val_mae: 0.0702 - val_mse: 0.0163\n",
            "Epoch 85/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0590 - accuracy: 0.4765 - mae: 0.0590 - mse: 0.0091 - val_loss: 0.0674 - val_accuracy: 0.4808 - val_mae: 0.0674 - val_mse: 0.0149\n",
            "Epoch 86/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0582 - accuracy: 0.4765 - mae: 0.0582 - mse: 0.0088 - val_loss: 0.0666 - val_accuracy: 0.4808 - val_mae: 0.0666 - val_mse: 0.0148\n",
            "Epoch 87/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0570 - accuracy: 0.4765 - mae: 0.0570 - mse: 0.0086 - val_loss: 0.0669 - val_accuracy: 0.4808 - val_mae: 0.0669 - val_mse: 0.0155\n",
            "Epoch 88/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0559 - accuracy: 0.4765 - mae: 0.0559 - mse: 0.0085 - val_loss: 0.0653 - val_accuracy: 0.4808 - val_mae: 0.0653 - val_mse: 0.0150\n",
            "Epoch 89/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0555 - accuracy: 0.4765 - mae: 0.0555 - mse: 0.0084 - val_loss: 0.0639 - val_accuracy: 0.4808 - val_mae: 0.0639 - val_mse: 0.0143\n",
            "Epoch 90/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0546 - accuracy: 0.4765 - mae: 0.0546 - mse: 0.0082 - val_loss: 0.0633 - val_accuracy: 0.4808 - val_mae: 0.0633 - val_mse: 0.0147\n",
            "Epoch 91/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0537 - accuracy: 0.4765 - mae: 0.0537 - mse: 0.0081 - val_loss: 0.0619 - val_accuracy: 0.4808 - val_mae: 0.0619 - val_mse: 0.0140\n",
            "Epoch 92/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0528 - accuracy: 0.4765 - mae: 0.0528 - mse: 0.0079 - val_loss: 0.0613 - val_accuracy: 0.4808 - val_mae: 0.0613 - val_mse: 0.0140\n",
            "Epoch 93/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0524 - accuracy: 0.4765 - mae: 0.0524 - mse: 0.0078 - val_loss: 0.0608 - val_accuracy: 0.4808 - val_mae: 0.0608 - val_mse: 0.0136\n",
            "Epoch 94/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0514 - accuracy: 0.4765 - mae: 0.0514 - mse: 0.0075 - val_loss: 0.0608 - val_accuracy: 0.4808 - val_mae: 0.0608 - val_mse: 0.0144\n",
            "Epoch 95/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0505 - accuracy: 0.4765 - mae: 0.0505 - mse: 0.0075 - val_loss: 0.0595 - val_accuracy: 0.4808 - val_mae: 0.0595 - val_mse: 0.0141\n",
            "Epoch 96/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0498 - accuracy: 0.4765 - mae: 0.0498 - mse: 0.0074 - val_loss: 0.0580 - val_accuracy: 0.4808 - val_mae: 0.0580 - val_mse: 0.0136\n",
            "Epoch 97/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0491 - accuracy: 0.4765 - mae: 0.0491 - mse: 0.0073 - val_loss: 0.0582 - val_accuracy: 0.4808 - val_mae: 0.0582 - val_mse: 0.0139\n",
            "Epoch 98/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0484 - accuracy: 0.4765 - mae: 0.0484 - mse: 0.0072 - val_loss: 0.0567 - val_accuracy: 0.4808 - val_mae: 0.0567 - val_mse: 0.0135\n",
            "Epoch 99/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0477 - accuracy: 0.4765 - mae: 0.0477 - mse: 0.0070 - val_loss: 0.0560 - val_accuracy: 0.4808 - val_mae: 0.0560 - val_mse: 0.0133\n",
            "Epoch 100/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0468 - accuracy: 0.4765 - mae: 0.0468 - mse: 0.0069 - val_loss: 0.0551 - val_accuracy: 0.4808 - val_mae: 0.0551 - val_mse: 0.0132\n",
            "Epoch 101/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0464 - accuracy: 0.4765 - mae: 0.0464 - mse: 0.0069 - val_loss: 0.0540 - val_accuracy: 0.4808 - val_mae: 0.0540 - val_mse: 0.0130\n",
            "Epoch 102/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0458 - accuracy: 0.4765 - mae: 0.0458 - mse: 0.0067 - val_loss: 0.0537 - val_accuracy: 0.4808 - val_mae: 0.0537 - val_mse: 0.0131\n",
            "Epoch 103/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0451 - accuracy: 0.4765 - mae: 0.0451 - mse: 0.0067 - val_loss: 0.0526 - val_accuracy: 0.4808 - val_mae: 0.0526 - val_mse: 0.0128\n",
            "Epoch 104/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0448 - accuracy: 0.4765 - mae: 0.0448 - mse: 0.0066 - val_loss: 0.0526 - val_accuracy: 0.4808 - val_mae: 0.0526 - val_mse: 0.0130\n",
            "Epoch 105/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0439 - accuracy: 0.4765 - mae: 0.0439 - mse: 0.0064 - val_loss: 0.0528 - val_accuracy: 0.4808 - val_mae: 0.0528 - val_mse: 0.0130\n",
            "Epoch 106/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0435 - accuracy: 0.4765 - mae: 0.0435 - mse: 0.0064 - val_loss: 0.0517 - val_accuracy: 0.4808 - val_mae: 0.0517 - val_mse: 0.0128\n",
            "Epoch 107/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0429 - accuracy: 0.4765 - mae: 0.0429 - mse: 0.0063 - val_loss: 0.0500 - val_accuracy: 0.4808 - val_mae: 0.0500 - val_mse: 0.0122\n",
            "Epoch 108/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0417 - accuracy: 0.4765 - mae: 0.0417 - mse: 0.0061 - val_loss: 0.0492 - val_accuracy: 0.4808 - val_mae: 0.0492 - val_mse: 0.0121\n",
            "Epoch 109/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0417 - accuracy: 0.4765 - mae: 0.0417 - mse: 0.0061 - val_loss: 0.0485 - val_accuracy: 0.4808 - val_mae: 0.0485 - val_mse: 0.0121\n",
            "Epoch 110/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0410 - accuracy: 0.4765 - mae: 0.0410 - mse: 0.0060 - val_loss: 0.0503 - val_accuracy: 0.4808 - val_mae: 0.0503 - val_mse: 0.0126\n",
            "Epoch 111/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0404 - accuracy: 0.4765 - mae: 0.0404 - mse: 0.0059 - val_loss: 0.0473 - val_accuracy: 0.4808 - val_mae: 0.0473 - val_mse: 0.0120\n",
            "Epoch 112/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0399 - accuracy: 0.4765 - mae: 0.0399 - mse: 0.0058 - val_loss: 0.0474 - val_accuracy: 0.4808 - val_mae: 0.0474 - val_mse: 0.0121\n",
            "Epoch 113/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0392 - accuracy: 0.4765 - mae: 0.0392 - mse: 0.0057 - val_loss: 0.0461 - val_accuracy: 0.4808 - val_mae: 0.0461 - val_mse: 0.0117\n",
            "Epoch 114/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0386 - accuracy: 0.4765 - mae: 0.0386 - mse: 0.0056 - val_loss: 0.0463 - val_accuracy: 0.4808 - val_mae: 0.0463 - val_mse: 0.0120\n",
            "Epoch 115/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0386 - accuracy: 0.4765 - mae: 0.0386 - mse: 0.0056 - val_loss: 0.0452 - val_accuracy: 0.4808 - val_mae: 0.0452 - val_mse: 0.0115\n",
            "Epoch 116/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0379 - accuracy: 0.4765 - mae: 0.0379 - mse: 0.0055 - val_loss: 0.0448 - val_accuracy: 0.4808 - val_mae: 0.0448 - val_mse: 0.0117\n",
            "Epoch 117/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0375 - accuracy: 0.4765 - mae: 0.0375 - mse: 0.0054 - val_loss: 0.0452 - val_accuracy: 0.4808 - val_mae: 0.0452 - val_mse: 0.0118\n",
            "Epoch 118/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0373 - accuracy: 0.4765 - mae: 0.0373 - mse: 0.0055 - val_loss: 0.0448 - val_accuracy: 0.4808 - val_mae: 0.0448 - val_mse: 0.0112\n",
            "Epoch 119/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0369 - accuracy: 0.4765 - mae: 0.0369 - mse: 0.0054 - val_loss: 0.0430 - val_accuracy: 0.4808 - val_mae: 0.0430 - val_mse: 0.0112\n",
            "Epoch 120/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0359 - accuracy: 0.4765 - mae: 0.0359 - mse: 0.0052 - val_loss: 0.0489 - val_accuracy: 0.4808 - val_mae: 0.0489 - val_mse: 0.0122\n",
            "Epoch 121/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0362 - accuracy: 0.4765 - mae: 0.0362 - mse: 0.0053 - val_loss: 0.0440 - val_accuracy: 0.4808 - val_mae: 0.0440 - val_mse: 0.0116\n",
            "Epoch 122/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0355 - accuracy: 0.4765 - mae: 0.0355 - mse: 0.0052 - val_loss: 0.0420 - val_accuracy: 0.4808 - val_mae: 0.0420 - val_mse: 0.0110\n",
            "Epoch 123/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0352 - accuracy: 0.4765 - mae: 0.0352 - mse: 0.0051 - val_loss: 0.0418 - val_accuracy: 0.4808 - val_mae: 0.0418 - val_mse: 0.0112\n",
            "Epoch 124/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0346 - accuracy: 0.4765 - mae: 0.0346 - mse: 0.0051 - val_loss: 0.0413 - val_accuracy: 0.4808 - val_mae: 0.0413 - val_mse: 0.0111\n",
            "Epoch 125/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0343 - accuracy: 0.4765 - mae: 0.0343 - mse: 0.0050 - val_loss: 0.0435 - val_accuracy: 0.4808 - val_mae: 0.0435 - val_mse: 0.0115\n",
            "Epoch 126/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0340 - accuracy: 0.4765 - mae: 0.0340 - mse: 0.0050 - val_loss: 0.0409 - val_accuracy: 0.4808 - val_mae: 0.0409 - val_mse: 0.0111\n",
            "Epoch 127/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0339 - accuracy: 0.4765 - mae: 0.0339 - mse: 0.0049 - val_loss: 0.0406 - val_accuracy: 0.4808 - val_mae: 0.0406 - val_mse: 0.0107\n",
            "Epoch 128/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0334 - accuracy: 0.4765 - mae: 0.0334 - mse: 0.0048 - val_loss: 0.0414 - val_accuracy: 0.4808 - val_mae: 0.0414 - val_mse: 0.0112\n",
            "Epoch 129/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0332 - accuracy: 0.4765 - mae: 0.0332 - mse: 0.0048 - val_loss: 0.0402 - val_accuracy: 0.4808 - val_mae: 0.0402 - val_mse: 0.0110\n",
            "Epoch 130/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0331 - accuracy: 0.4765 - mae: 0.0331 - mse: 0.0048 - val_loss: 0.0470 - val_accuracy: 0.4808 - val_mae: 0.0470 - val_mse: 0.0117\n",
            "Epoch 131/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0334 - accuracy: 0.4765 - mae: 0.0334 - mse: 0.0047 - val_loss: 0.0391 - val_accuracy: 0.4808 - val_mae: 0.0391 - val_mse: 0.0106\n",
            "Epoch 132/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0324 - accuracy: 0.4765 - mae: 0.0324 - mse: 0.0047 - val_loss: 0.0392 - val_accuracy: 0.4808 - val_mae: 0.0392 - val_mse: 0.0108\n",
            "Epoch 133/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0320 - accuracy: 0.4765 - mae: 0.0320 - mse: 0.0046 - val_loss: 0.0388 - val_accuracy: 0.4808 - val_mae: 0.0388 - val_mse: 0.0107\n",
            "Epoch 134/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0320 - accuracy: 0.4765 - mae: 0.0320 - mse: 0.0046 - val_loss: 0.0389 - val_accuracy: 0.4808 - val_mae: 0.0389 - val_mse: 0.0107\n",
            "Epoch 135/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0316 - accuracy: 0.4765 - mae: 0.0316 - mse: 0.0045 - val_loss: 0.0376 - val_accuracy: 0.4808 - val_mae: 0.0376 - val_mse: 0.0104\n",
            "Epoch 136/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0311 - accuracy: 0.4765 - mae: 0.0311 - mse: 0.0044 - val_loss: 0.0375 - val_accuracy: 0.4808 - val_mae: 0.0375 - val_mse: 0.0104\n",
            "Epoch 137/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0311 - accuracy: 0.4765 - mae: 0.0311 - mse: 0.0045 - val_loss: 0.0373 - val_accuracy: 0.4808 - val_mae: 0.0373 - val_mse: 0.0103\n",
            "Epoch 138/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0310 - accuracy: 0.4765 - mae: 0.0310 - mse: 0.0044 - val_loss: 0.0380 - val_accuracy: 0.4808 - val_mae: 0.0380 - val_mse: 0.0106\n",
            "Epoch 139/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0309 - accuracy: 0.4765 - mae: 0.0309 - mse: 0.0044 - val_loss: 0.0368 - val_accuracy: 0.4808 - val_mae: 0.0368 - val_mse: 0.0102\n",
            "Epoch 140/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0304 - accuracy: 0.4765 - mae: 0.0304 - mse: 0.0043 - val_loss: 0.0383 - val_accuracy: 0.4808 - val_mae: 0.0383 - val_mse: 0.0106\n",
            "Epoch 141/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0305 - accuracy: 0.4765 - mae: 0.0305 - mse: 0.0043 - val_loss: 0.0370 - val_accuracy: 0.4808 - val_mae: 0.0370 - val_mse: 0.0101\n",
            "Epoch 142/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0298 - accuracy: 0.4765 - mae: 0.0298 - mse: 0.0042 - val_loss: 0.0392 - val_accuracy: 0.4808 - val_mae: 0.0392 - val_mse: 0.0106\n",
            "Epoch 143/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0298 - accuracy: 0.4765 - mae: 0.0298 - mse: 0.0042 - val_loss: 0.0360 - val_accuracy: 0.4808 - val_mae: 0.0360 - val_mse: 0.0101\n",
            "Epoch 144/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0295 - accuracy: 0.4765 - mae: 0.0295 - mse: 0.0042 - val_loss: 0.0358 - val_accuracy: 0.4808 - val_mae: 0.0358 - val_mse: 0.0100\n",
            "Epoch 145/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0295 - accuracy: 0.4765 - mae: 0.0295 - mse: 0.0041 - val_loss: 0.0370 - val_accuracy: 0.4808 - val_mae: 0.0370 - val_mse: 0.0103\n",
            "Epoch 146/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0294 - accuracy: 0.4765 - mae: 0.0294 - mse: 0.0041 - val_loss: 0.0356 - val_accuracy: 0.4808 - val_mae: 0.0356 - val_mse: 0.0101\n",
            "Epoch 147/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0291 - accuracy: 0.4765 - mae: 0.0291 - mse: 0.0041 - val_loss: 0.0355 - val_accuracy: 0.4808 - val_mae: 0.0355 - val_mse: 0.0099\n",
            "Epoch 148/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0291 - accuracy: 0.4765 - mae: 0.0291 - mse: 0.0040 - val_loss: 0.0353 - val_accuracy: 0.4808 - val_mae: 0.0353 - val_mse: 0.0101\n",
            "Epoch 149/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0284 - accuracy: 0.4765 - mae: 0.0284 - mse: 0.0040 - val_loss: 0.0387 - val_accuracy: 0.4808 - val_mae: 0.0387 - val_mse: 0.0098\n",
            "Epoch 150/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0289 - accuracy: 0.4765 - mae: 0.0289 - mse: 0.0040 - val_loss: 0.0351 - val_accuracy: 0.4808 - val_mae: 0.0351 - val_mse: 0.0098\n",
            "Epoch 151/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0282 - accuracy: 0.4765 - mae: 0.0282 - mse: 0.0039 - val_loss: 0.0349 - val_accuracy: 0.4808 - val_mae: 0.0349 - val_mse: 0.0100\n",
            "Epoch 152/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0280 - accuracy: 0.4765 - mae: 0.0280 - mse: 0.0039 - val_loss: 0.0348 - val_accuracy: 0.4808 - val_mae: 0.0348 - val_mse: 0.0097\n",
            "Epoch 153/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0276 - accuracy: 0.4765 - mae: 0.0276 - mse: 0.0038 - val_loss: 0.0341 - val_accuracy: 0.4808 - val_mae: 0.0341 - val_mse: 0.0098\n",
            "Epoch 154/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0273 - accuracy: 0.4765 - mae: 0.0273 - mse: 0.0038 - val_loss: 0.0338 - val_accuracy: 0.4808 - val_mae: 0.0338 - val_mse: 0.0097\n",
            "Epoch 155/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0274 - accuracy: 0.4765 - mae: 0.0274 - mse: 0.0038 - val_loss: 0.0337 - val_accuracy: 0.4808 - val_mae: 0.0337 - val_mse: 0.0097\n",
            "Epoch 156/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0275 - accuracy: 0.4765 - mae: 0.0275 - mse: 0.0037 - val_loss: 0.0413 - val_accuracy: 0.4808 - val_mae: 0.0413 - val_mse: 0.0105\n",
            "Epoch 157/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0276 - accuracy: 0.4765 - mae: 0.0276 - mse: 0.0038 - val_loss: 0.0338 - val_accuracy: 0.4808 - val_mae: 0.0338 - val_mse: 0.0098\n",
            "Epoch 158/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0272 - accuracy: 0.4765 - mae: 0.0272 - mse: 0.0037 - val_loss: 0.0366 - val_accuracy: 0.4808 - val_mae: 0.0366 - val_mse: 0.0100\n",
            "Epoch 159/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0267 - accuracy: 0.4765 - mae: 0.0267 - mse: 0.0037 - val_loss: 0.0337 - val_accuracy: 0.4808 - val_mae: 0.0337 - val_mse: 0.0097\n",
            "Epoch 160/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0268 - accuracy: 0.4765 - mae: 0.0268 - mse: 0.0037 - val_loss: 0.0343 - val_accuracy: 0.4808 - val_mae: 0.0343 - val_mse: 0.0098\n",
            "Epoch 161/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0267 - accuracy: 0.4765 - mae: 0.0267 - mse: 0.0036 - val_loss: 0.0353 - val_accuracy: 0.4808 - val_mae: 0.0353 - val_mse: 0.0094\n",
            "Epoch 162/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0265 - accuracy: 0.4765 - mae: 0.0265 - mse: 0.0036 - val_loss: 0.0331 - val_accuracy: 0.4808 - val_mae: 0.0331 - val_mse: 0.0096\n",
            "Epoch 163/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0263 - accuracy: 0.4765 - mae: 0.0263 - mse: 0.0036 - val_loss: 0.0341 - val_accuracy: 0.4808 - val_mae: 0.0341 - val_mse: 0.0094\n",
            "Epoch 164/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0257 - accuracy: 0.4765 - mae: 0.0257 - mse: 0.0035 - val_loss: 0.0352 - val_accuracy: 0.4808 - val_mae: 0.0352 - val_mse: 0.0093\n",
            "Epoch 165/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0259 - accuracy: 0.4765 - mae: 0.0259 - mse: 0.0035 - val_loss: 0.0353 - val_accuracy: 0.4808 - val_mae: 0.0353 - val_mse: 0.0098\n",
            "Epoch 166/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0255 - accuracy: 0.4765 - mae: 0.0255 - mse: 0.0035 - val_loss: 0.0327 - val_accuracy: 0.4808 - val_mae: 0.0327 - val_mse: 0.0095\n",
            "Epoch 167/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0253 - accuracy: 0.4765 - mae: 0.0253 - mse: 0.0035 - val_loss: 0.0321 - val_accuracy: 0.4808 - val_mae: 0.0321 - val_mse: 0.0094\n",
            "Epoch 168/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0258 - accuracy: 0.4765 - mae: 0.0258 - mse: 0.0034 - val_loss: 0.0343 - val_accuracy: 0.4808 - val_mae: 0.0343 - val_mse: 0.0097\n",
            "Epoch 169/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0249 - accuracy: 0.4765 - mae: 0.0249 - mse: 0.0034 - val_loss: 0.0390 - val_accuracy: 0.4808 - val_mae: 0.0390 - val_mse: 0.0100\n",
            "Epoch 170/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0254 - accuracy: 0.4765 - mae: 0.0254 - mse: 0.0034 - val_loss: 0.0357 - val_accuracy: 0.4808 - val_mae: 0.0357 - val_mse: 0.0097\n",
            "Epoch 171/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0252 - accuracy: 0.4765 - mae: 0.0252 - mse: 0.0033 - val_loss: 0.0320 - val_accuracy: 0.4808 - val_mae: 0.0320 - val_mse: 0.0094\n",
            "Epoch 172/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0244 - accuracy: 0.4765 - mae: 0.0244 - mse: 0.0033 - val_loss: 0.0323 - val_accuracy: 0.4808 - val_mae: 0.0323 - val_mse: 0.0092\n",
            "Epoch 173/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0244 - accuracy: 0.4765 - mae: 0.0244 - mse: 0.0033 - val_loss: 0.0315 - val_accuracy: 0.4808 - val_mae: 0.0315 - val_mse: 0.0092\n",
            "Epoch 174/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0240 - accuracy: 0.4765 - mae: 0.0240 - mse: 0.0032 - val_loss: 0.0332 - val_accuracy: 0.4808 - val_mae: 0.0332 - val_mse: 0.0091\n",
            "Epoch 175/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0244 - accuracy: 0.4765 - mae: 0.0244 - mse: 0.0032 - val_loss: 0.0316 - val_accuracy: 0.4808 - val_mae: 0.0316 - val_mse: 0.0092\n",
            "Epoch 176/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0246 - accuracy: 0.4765 - mae: 0.0246 - mse: 0.0032 - val_loss: 0.0361 - val_accuracy: 0.4808 - val_mae: 0.0361 - val_mse: 0.0097\n",
            "Epoch 177/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0241 - accuracy: 0.4765 - mae: 0.0241 - mse: 0.0032 - val_loss: 0.0335 - val_accuracy: 0.4808 - val_mae: 0.0335 - val_mse: 0.0090\n",
            "Epoch 178/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0238 - accuracy: 0.4765 - mae: 0.0238 - mse: 0.0031 - val_loss: 0.0315 - val_accuracy: 0.4808 - val_mae: 0.0315 - val_mse: 0.0090\n",
            "Epoch 179/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0239 - accuracy: 0.4765 - mae: 0.0239 - mse: 0.0031 - val_loss: 0.0323 - val_accuracy: 0.4808 - val_mae: 0.0323 - val_mse: 0.0090\n",
            "Epoch 180/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0237 - accuracy: 0.4765 - mae: 0.0237 - mse: 0.0031 - val_loss: 0.0326 - val_accuracy: 0.4808 - val_mae: 0.0326 - val_mse: 0.0089\n",
            "Epoch 181/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0241 - accuracy: 0.4765 - mae: 0.0241 - mse: 0.0031 - val_loss: 0.0327 - val_accuracy: 0.4808 - val_mae: 0.0327 - val_mse: 0.0094\n",
            "Epoch 182/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0236 - accuracy: 0.4765 - mae: 0.0236 - mse: 0.0031 - val_loss: 0.0319 - val_accuracy: 0.4808 - val_mae: 0.0319 - val_mse: 0.0093\n",
            "Epoch 183/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0229 - accuracy: 0.4765 - mae: 0.0229 - mse: 0.0030 - val_loss: 0.0304 - val_accuracy: 0.4808 - val_mae: 0.0304 - val_mse: 0.0090\n",
            "Epoch 184/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0231 - accuracy: 0.4765 - mae: 0.0231 - mse: 0.0031 - val_loss: 0.0303 - val_accuracy: 0.4808 - val_mae: 0.0303 - val_mse: 0.0091\n",
            "Epoch 185/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0227 - accuracy: 0.4765 - mae: 0.0227 - mse: 0.0030 - val_loss: 0.0315 - val_accuracy: 0.4808 - val_mae: 0.0315 - val_mse: 0.0088\n",
            "Epoch 186/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0225 - accuracy: 0.4765 - mae: 0.0225 - mse: 0.0030 - val_loss: 0.0324 - val_accuracy: 0.4808 - val_mae: 0.0324 - val_mse: 0.0093\n",
            "Epoch 187/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0230 - accuracy: 0.4765 - mae: 0.0230 - mse: 0.0030 - val_loss: 0.0357 - val_accuracy: 0.4808 - val_mae: 0.0357 - val_mse: 0.0095\n",
            "Epoch 188/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0224 - accuracy: 0.4765 - mae: 0.0224 - mse: 0.0030 - val_loss: 0.0319 - val_accuracy: 0.4808 - val_mae: 0.0319 - val_mse: 0.0088\n",
            "Epoch 189/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0221 - accuracy: 0.4765 - mae: 0.0221 - mse: 0.0029 - val_loss: 0.0294 - val_accuracy: 0.4808 - val_mae: 0.0294 - val_mse: 0.0090\n",
            "Epoch 190/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0219 - accuracy: 0.4765 - mae: 0.0219 - mse: 0.0029 - val_loss: 0.0303 - val_accuracy: 0.4808 - val_mae: 0.0303 - val_mse: 0.0091\n",
            "Epoch 191/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0217 - accuracy: 0.4765 - mae: 0.0217 - mse: 0.0029 - val_loss: 0.0290 - val_accuracy: 0.4808 - val_mae: 0.0290 - val_mse: 0.0089\n",
            "Epoch 192/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0224 - accuracy: 0.4765 - mae: 0.0224 - mse: 0.0029 - val_loss: 0.0298 - val_accuracy: 0.4808 - val_mae: 0.0298 - val_mse: 0.0087\n",
            "Epoch 193/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0220 - accuracy: 0.4765 - mae: 0.0220 - mse: 0.0029 - val_loss: 0.0291 - val_accuracy: 0.4808 - val_mae: 0.0291 - val_mse: 0.0088\n",
            "Epoch 194/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0214 - accuracy: 0.4765 - mae: 0.0214 - mse: 0.0028 - val_loss: 0.0291 - val_accuracy: 0.4808 - val_mae: 0.0291 - val_mse: 0.0089\n",
            "Epoch 195/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0211 - accuracy: 0.4765 - mae: 0.0211 - mse: 0.0028 - val_loss: 0.0316 - val_accuracy: 0.4808 - val_mae: 0.0316 - val_mse: 0.0087\n",
            "Epoch 196/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0213 - accuracy: 0.4765 - mae: 0.0213 - mse: 0.0028 - val_loss: 0.0289 - val_accuracy: 0.4808 - val_mae: 0.0289 - val_mse: 0.0089\n",
            "Epoch 197/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0212 - accuracy: 0.4765 - mae: 0.0212 - mse: 0.0028 - val_loss: 0.0303 - val_accuracy: 0.4808 - val_mae: 0.0303 - val_mse: 0.0086\n",
            "Epoch 198/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0217 - accuracy: 0.4765 - mae: 0.0217 - mse: 0.0028 - val_loss: 0.0295 - val_accuracy: 0.4808 - val_mae: 0.0295 - val_mse: 0.0089\n",
            "Epoch 199/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0225 - accuracy: 0.4765 - mae: 0.0225 - mse: 0.0028 - val_loss: 0.0330 - val_accuracy: 0.4808 - val_mae: 0.0330 - val_mse: 0.0092\n",
            "Epoch 200/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0213 - accuracy: 0.4765 - mae: 0.0213 - mse: 0.0028 - val_loss: 0.0282 - val_accuracy: 0.4808 - val_mae: 0.0282 - val_mse: 0.0088\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hasil nya"
      ],
      "metadata": {
        "id": "1iuupICo5fPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'], label='loss')\n",
        "plt.plot(history.history['val_loss'], label='Valid')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['accuracy'], label='acc')\n",
        "plt.plot(history.history['val_accuracy'], label='Valid')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['mae'], label='mae')\n",
        "plt.plot(history.history['val_mae'], label='Valid mae')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['mse'], label='mse')\n",
        "plt.plot(history.history['val_mse'], label='Valid mse')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "21a23ad8-8cc2-429c-88d2-83fcc141e994",
        "id": "-vt7TrvV5fPX"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# model 4"
      ],
      "metadata": {
        "id": "L4H28PUOI7qi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(29, input_dim=29, activation = 'relu')) \n",
        "model.add(Dense(1, activation = 'relu'))\n",
        "\n",
        "model.compile(loss=\"mean_absolute_error\", optimizer=optimizers.SGD(learning_rate=0.001), metrics=['accuracy','mae', 'mse'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3e790e6-0649-4ebb-9f5e-9f7bbd3b1cf1",
        "id": "dfbA4W40I7qj"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_28\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_83 (Dense)            (None, 29)                870       \n",
            "                                                                 \n",
            " dense_84 (Dense)            (None, 1)                 30        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 900\n",
            "Trainable params: 900\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=200, batch_size=32, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5018542-6f43-40a2-8a8f-1af2bc97f4a0",
        "id": "u91JzoXHI7qj"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "12/12 [==============================] - 1s 34ms/step - loss: 0.3718 - accuracy: 0.3269 - mae: 0.3718 - mse: 0.1872 - val_loss: 0.3392 - val_accuracy: 0.4167 - val_mae: 0.3392 - val_mse: 0.1523\n",
            "Epoch 2/200\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.3261 - accuracy: 0.3546 - mae: 0.3261 - mse: 0.1513 - val_loss: 0.2940 - val_accuracy: 0.4295 - val_mae: 0.2940 - val_mse: 0.1205\n",
            "Epoch 3/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.2827 - accuracy: 0.3684 - mae: 0.2827 - mse: 0.1208 - val_loss: 0.2519 - val_accuracy: 0.4423 - val_mae: 0.2519 - val_mse: 0.0945\n",
            "Epoch 4/200\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.2422 - accuracy: 0.4072 - mae: 0.2422 - mse: 0.0959 - val_loss: 0.2141 - val_accuracy: 0.4551 - val_mae: 0.2141 - val_mse: 0.0737\n",
            "Epoch 5/200\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.2068 - accuracy: 0.4294 - mae: 0.2068 - mse: 0.0760 - val_loss: 0.1816 - val_accuracy: 0.4615 - val_mae: 0.1816 - val_mse: 0.0575\n",
            "Epoch 6/200\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.1771 - accuracy: 0.4515 - mae: 0.1771 - mse: 0.0607 - val_loss: 0.1550 - val_accuracy: 0.4744 - val_mae: 0.1550 - val_mse: 0.0455\n",
            "Epoch 7/200\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.1529 - accuracy: 0.4543 - mae: 0.1529 - mse: 0.0488 - val_loss: 0.1323 - val_accuracy: 0.4808 - val_mae: 0.1323 - val_mse: 0.0362\n",
            "Epoch 8/200\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.1316 - accuracy: 0.4626 - mae: 0.1316 - mse: 0.0391 - val_loss: 0.1121 - val_accuracy: 0.4808 - val_mae: 0.1121 - val_mse: 0.0290\n",
            "Epoch 9/200\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.1135 - accuracy: 0.4654 - mae: 0.1135 - mse: 0.0314 - val_loss: 0.0956 - val_accuracy: 0.4808 - val_mae: 0.0956 - val_mse: 0.0237\n",
            "Epoch 10/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0983 - accuracy: 0.4681 - mae: 0.0983 - mse: 0.0257 - val_loss: 0.0811 - val_accuracy: 0.4808 - val_mae: 0.0811 - val_mse: 0.0198\n",
            "Epoch 11/200\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0851 - accuracy: 0.4709 - mae: 0.0851 - mse: 0.0211 - val_loss: 0.0687 - val_accuracy: 0.4808 - val_mae: 0.0687 - val_mse: 0.0167\n",
            "Epoch 12/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0736 - accuracy: 0.4765 - mae: 0.0736 - mse: 0.0173 - val_loss: 0.0601 - val_accuracy: 0.4808 - val_mae: 0.0601 - val_mse: 0.0146\n",
            "Epoch 13/200\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0648 - accuracy: 0.4765 - mae: 0.0648 - mse: 0.0146 - val_loss: 0.0524 - val_accuracy: 0.4808 - val_mae: 0.0524 - val_mse: 0.0130\n",
            "Epoch 14/200\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.0568 - accuracy: 0.4765 - mae: 0.0568 - mse: 0.0123 - val_loss: 0.0463 - val_accuracy: 0.4808 - val_mae: 0.0463 - val_mse: 0.0119\n",
            "Epoch 15/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0507 - accuracy: 0.4765 - mae: 0.0507 - mse: 0.0106 - val_loss: 0.0419 - val_accuracy: 0.4808 - val_mae: 0.0419 - val_mse: 0.0110\n",
            "Epoch 16/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0459 - accuracy: 0.4765 - mae: 0.0459 - mse: 0.0093 - val_loss: 0.0387 - val_accuracy: 0.4808 - val_mae: 0.0387 - val_mse: 0.0105\n",
            "Epoch 17/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0424 - accuracy: 0.4765 - mae: 0.0424 - mse: 0.0084 - val_loss: 0.0360 - val_accuracy: 0.4808 - val_mae: 0.0360 - val_mse: 0.0101\n",
            "Epoch 18/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0398 - accuracy: 0.4765 - mae: 0.0398 - mse: 0.0077 - val_loss: 0.0338 - val_accuracy: 0.4808 - val_mae: 0.0338 - val_mse: 0.0098\n",
            "Epoch 19/200\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0374 - accuracy: 0.4765 - mae: 0.0374 - mse: 0.0070 - val_loss: 0.0318 - val_accuracy: 0.4808 - val_mae: 0.0318 - val_mse: 0.0095\n",
            "Epoch 20/200\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0353 - accuracy: 0.4765 - mae: 0.0353 - mse: 0.0065 - val_loss: 0.0299 - val_accuracy: 0.4808 - val_mae: 0.0299 - val_mse: 0.0093\n",
            "Epoch 21/200\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.0331 - accuracy: 0.4765 - mae: 0.0331 - mse: 0.0060 - val_loss: 0.0285 - val_accuracy: 0.4808 - val_mae: 0.0285 - val_mse: 0.0091\n",
            "Epoch 22/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0313 - accuracy: 0.4765 - mae: 0.0313 - mse: 0.0056 - val_loss: 0.0272 - val_accuracy: 0.4808 - val_mae: 0.0272 - val_mse: 0.0090\n",
            "Epoch 23/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0293 - accuracy: 0.4765 - mae: 0.0293 - mse: 0.0051 - val_loss: 0.0261 - val_accuracy: 0.4808 - val_mae: 0.0261 - val_mse: 0.0089\n",
            "Epoch 24/200\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0273 - accuracy: 0.4765 - mae: 0.0273 - mse: 0.0047 - val_loss: 0.0252 - val_accuracy: 0.4808 - val_mae: 0.0252 - val_mse: 0.0087\n",
            "Epoch 25/200\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.0258 - accuracy: 0.4765 - mae: 0.0258 - mse: 0.0044 - val_loss: 0.0246 - val_accuracy: 0.4808 - val_mae: 0.0246 - val_mse: 0.0086\n",
            "Epoch 26/200\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0244 - accuracy: 0.4765 - mae: 0.0244 - mse: 0.0041 - val_loss: 0.0239 - val_accuracy: 0.4808 - val_mae: 0.0239 - val_mse: 0.0085\n",
            "Epoch 27/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0231 - accuracy: 0.4765 - mae: 0.0231 - mse: 0.0038 - val_loss: 0.0233 - val_accuracy: 0.4808 - val_mae: 0.0233 - val_mse: 0.0084\n",
            "Epoch 28/200\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0218 - accuracy: 0.4765 - mae: 0.0218 - mse: 0.0036 - val_loss: 0.0228 - val_accuracy: 0.4808 - val_mae: 0.0228 - val_mse: 0.0083\n",
            "Epoch 29/200\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0206 - accuracy: 0.4765 - mae: 0.0206 - mse: 0.0034 - val_loss: 0.0224 - val_accuracy: 0.4808 - val_mae: 0.0224 - val_mse: 0.0082\n",
            "Epoch 30/200\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.0196 - accuracy: 0.4765 - mae: 0.0196 - mse: 0.0032 - val_loss: 0.0221 - val_accuracy: 0.4808 - val_mae: 0.0221 - val_mse: 0.0081\n",
            "Epoch 31/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0189 - accuracy: 0.4765 - mae: 0.0189 - mse: 0.0031 - val_loss: 0.0218 - val_accuracy: 0.4808 - val_mae: 0.0218 - val_mse: 0.0081\n",
            "Epoch 32/200\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0182 - accuracy: 0.4765 - mae: 0.0182 - mse: 0.0030 - val_loss: 0.0216 - val_accuracy: 0.4808 - val_mae: 0.0216 - val_mse: 0.0080\n",
            "Epoch 33/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0176 - accuracy: 0.4765 - mae: 0.0176 - mse: 0.0029 - val_loss: 0.0213 - val_accuracy: 0.4808 - val_mae: 0.0213 - val_mse: 0.0080\n",
            "Epoch 34/200\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.0171 - accuracy: 0.4765 - mae: 0.0171 - mse: 0.0028 - val_loss: 0.0211 - val_accuracy: 0.4808 - val_mae: 0.0211 - val_mse: 0.0079\n",
            "Epoch 35/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0166 - accuracy: 0.4765 - mae: 0.0166 - mse: 0.0027 - val_loss: 0.0208 - val_accuracy: 0.4808 - val_mae: 0.0208 - val_mse: 0.0079\n",
            "Epoch 36/200\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0162 - accuracy: 0.4765 - mae: 0.0162 - mse: 0.0026 - val_loss: 0.0206 - val_accuracy: 0.4808 - val_mae: 0.0206 - val_mse: 0.0079\n",
            "Epoch 37/200\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0159 - accuracy: 0.4765 - mae: 0.0159 - mse: 0.0026 - val_loss: 0.0204 - val_accuracy: 0.4808 - val_mae: 0.0204 - val_mse: 0.0078\n",
            "Epoch 38/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0155 - accuracy: 0.4765 - mae: 0.0155 - mse: 0.0025 - val_loss: 0.0203 - val_accuracy: 0.4808 - val_mae: 0.0203 - val_mse: 0.0078\n",
            "Epoch 39/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0152 - accuracy: 0.4765 - mae: 0.0152 - mse: 0.0025 - val_loss: 0.0202 - val_accuracy: 0.4808 - val_mae: 0.0202 - val_mse: 0.0078\n",
            "Epoch 40/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0149 - accuracy: 0.4765 - mae: 0.0149 - mse: 0.0024 - val_loss: 0.0200 - val_accuracy: 0.4808 - val_mae: 0.0200 - val_mse: 0.0078\n",
            "Epoch 41/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0146 - accuracy: 0.4765 - mae: 0.0146 - mse: 0.0024 - val_loss: 0.0199 - val_accuracy: 0.4808 - val_mae: 0.0199 - val_mse: 0.0077\n",
            "Epoch 42/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0143 - accuracy: 0.4765 - mae: 0.0143 - mse: 0.0023 - val_loss: 0.0197 - val_accuracy: 0.4808 - val_mae: 0.0197 - val_mse: 0.0077\n",
            "Epoch 43/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0139 - accuracy: 0.4765 - mae: 0.0139 - mse: 0.0023 - val_loss: 0.0195 - val_accuracy: 0.4808 - val_mae: 0.0195 - val_mse: 0.0077\n",
            "Epoch 44/200\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.0135 - accuracy: 0.4765 - mae: 0.0135 - mse: 0.0022 - val_loss: 0.0194 - val_accuracy: 0.4808 - val_mae: 0.0194 - val_mse: 0.0077\n",
            "Epoch 45/200\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.0133 - accuracy: 0.4765 - mae: 0.0133 - mse: 0.0022 - val_loss: 0.0193 - val_accuracy: 0.4808 - val_mae: 0.0193 - val_mse: 0.0077\n",
            "Epoch 46/200\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0130 - accuracy: 0.4765 - mae: 0.0130 - mse: 0.0022 - val_loss: 0.0192 - val_accuracy: 0.4808 - val_mae: 0.0192 - val_mse: 0.0077\n",
            "Epoch 47/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0127 - accuracy: 0.4765 - mae: 0.0127 - mse: 0.0021 - val_loss: 0.0191 - val_accuracy: 0.4808 - val_mae: 0.0191 - val_mse: 0.0077\n",
            "Epoch 48/200\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.0125 - accuracy: 0.4765 - mae: 0.0125 - mse: 0.0021 - val_loss: 0.0190 - val_accuracy: 0.4808 - val_mae: 0.0190 - val_mse: 0.0077\n",
            "Epoch 49/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0123 - accuracy: 0.4765 - mae: 0.0123 - mse: 0.0021 - val_loss: 0.0189 - val_accuracy: 0.4808 - val_mae: 0.0189 - val_mse: 0.0076\n",
            "Epoch 50/200\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.0122 - accuracy: 0.4765 - mae: 0.0122 - mse: 0.0021 - val_loss: 0.0188 - val_accuracy: 0.4808 - val_mae: 0.0188 - val_mse: 0.0076\n",
            "Epoch 51/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0120 - accuracy: 0.4765 - mae: 0.0120 - mse: 0.0020 - val_loss: 0.0187 - val_accuracy: 0.4808 - val_mae: 0.0187 - val_mse: 0.0076\n",
            "Epoch 52/200\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0119 - accuracy: 0.4765 - mae: 0.0119 - mse: 0.0020 - val_loss: 0.0186 - val_accuracy: 0.4808 - val_mae: 0.0186 - val_mse: 0.0076\n",
            "Epoch 53/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.0117 - accuracy: 0.4765 - mae: 0.0117 - mse: 0.0020 - val_loss: 0.0185 - val_accuracy: 0.4808 - val_mae: 0.0185 - val_mse: 0.0076\n",
            "Epoch 54/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0116 - accuracy: 0.4765 - mae: 0.0116 - mse: 0.0020 - val_loss: 0.0185 - val_accuracy: 0.4808 - val_mae: 0.0185 - val_mse: 0.0076\n",
            "Epoch 55/200\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0114 - accuracy: 0.4765 - mae: 0.0114 - mse: 0.0020 - val_loss: 0.0184 - val_accuracy: 0.4808 - val_mae: 0.0184 - val_mse: 0.0076\n",
            "Epoch 56/200\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.0113 - accuracy: 0.4765 - mae: 0.0113 - mse: 0.0020 - val_loss: 0.0184 - val_accuracy: 0.4808 - val_mae: 0.0184 - val_mse: 0.0076\n",
            "Epoch 57/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0112 - accuracy: 0.4765 - mae: 0.0112 - mse: 0.0020 - val_loss: 0.0184 - val_accuracy: 0.4808 - val_mae: 0.0184 - val_mse: 0.0076\n",
            "Epoch 58/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0111 - accuracy: 0.4765 - mae: 0.0111 - mse: 0.0019 - val_loss: 0.0183 - val_accuracy: 0.4808 - val_mae: 0.0183 - val_mse: 0.0076\n",
            "Epoch 59/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0111 - accuracy: 0.4765 - mae: 0.0111 - mse: 0.0019 - val_loss: 0.0183 - val_accuracy: 0.4808 - val_mae: 0.0183 - val_mse: 0.0076\n",
            "Epoch 60/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0110 - accuracy: 0.4765 - mae: 0.0110 - mse: 0.0019 - val_loss: 0.0183 - val_accuracy: 0.4808 - val_mae: 0.0183 - val_mse: 0.0076\n",
            "Epoch 61/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0109 - accuracy: 0.4765 - mae: 0.0109 - mse: 0.0019 - val_loss: 0.0183 - val_accuracy: 0.4808 - val_mae: 0.0183 - val_mse: 0.0076\n",
            "Epoch 62/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0109 - accuracy: 0.4765 - mae: 0.0109 - mse: 0.0019 - val_loss: 0.0183 - val_accuracy: 0.4808 - val_mae: 0.0183 - val_mse: 0.0076\n",
            "Epoch 63/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0109 - accuracy: 0.4765 - mae: 0.0109 - mse: 0.0019 - val_loss: 0.0183 - val_accuracy: 0.4808 - val_mae: 0.0183 - val_mse: 0.0076\n",
            "Epoch 64/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0108 - accuracy: 0.4765 - mae: 0.0108 - mse: 0.0019 - val_loss: 0.0183 - val_accuracy: 0.4808 - val_mae: 0.0183 - val_mse: 0.0076\n",
            "Epoch 65/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0108 - accuracy: 0.4765 - mae: 0.0108 - mse: 0.0019 - val_loss: 0.0182 - val_accuracy: 0.4808 - val_mae: 0.0182 - val_mse: 0.0076\n",
            "Epoch 66/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0107 - accuracy: 0.4765 - mae: 0.0107 - mse: 0.0019 - val_loss: 0.0182 - val_accuracy: 0.4808 - val_mae: 0.0182 - val_mse: 0.0076\n",
            "Epoch 67/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0107 - accuracy: 0.4765 - mae: 0.0107 - mse: 0.0019 - val_loss: 0.0182 - val_accuracy: 0.4808 - val_mae: 0.0182 - val_mse: 0.0076\n",
            "Epoch 68/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0107 - accuracy: 0.4765 - mae: 0.0107 - mse: 0.0019 - val_loss: 0.0182 - val_accuracy: 0.4808 - val_mae: 0.0182 - val_mse: 0.0076\n",
            "Epoch 69/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0106 - accuracy: 0.4765 - mae: 0.0106 - mse: 0.0019 - val_loss: 0.0182 - val_accuracy: 0.4808 - val_mae: 0.0182 - val_mse: 0.0076\n",
            "Epoch 70/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0106 - accuracy: 0.4765 - mae: 0.0106 - mse: 0.0019 - val_loss: 0.0182 - val_accuracy: 0.4808 - val_mae: 0.0182 - val_mse: 0.0076\n",
            "Epoch 71/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0106 - accuracy: 0.4765 - mae: 0.0106 - mse: 0.0019 - val_loss: 0.0181 - val_accuracy: 0.4808 - val_mae: 0.0181 - val_mse: 0.0076\n",
            "Epoch 72/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0105 - accuracy: 0.4765 - mae: 0.0105 - mse: 0.0019 - val_loss: 0.0181 - val_accuracy: 0.4808 - val_mae: 0.0181 - val_mse: 0.0076\n",
            "Epoch 73/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0105 - accuracy: 0.4765 - mae: 0.0105 - mse: 0.0019 - val_loss: 0.0181 - val_accuracy: 0.4808 - val_mae: 0.0181 - val_mse: 0.0076\n",
            "Epoch 74/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0105 - accuracy: 0.4765 - mae: 0.0105 - mse: 0.0019 - val_loss: 0.0181 - val_accuracy: 0.4808 - val_mae: 0.0181 - val_mse: 0.0076\n",
            "Epoch 75/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0105 - accuracy: 0.4765 - mae: 0.0105 - mse: 0.0019 - val_loss: 0.0181 - val_accuracy: 0.4808 - val_mae: 0.0181 - val_mse: 0.0076\n",
            "Epoch 76/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0105 - accuracy: 0.4765 - mae: 0.0105 - mse: 0.0019 - val_loss: 0.0181 - val_accuracy: 0.4808 - val_mae: 0.0181 - val_mse: 0.0076\n",
            "Epoch 77/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0105 - accuracy: 0.4765 - mae: 0.0105 - mse: 0.0019 - val_loss: 0.0181 - val_accuracy: 0.4808 - val_mae: 0.0181 - val_mse: 0.0076\n",
            "Epoch 78/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0104 - accuracy: 0.4765 - mae: 0.0104 - mse: 0.0019 - val_loss: 0.0181 - val_accuracy: 0.4808 - val_mae: 0.0181 - val_mse: 0.0076\n",
            "Epoch 79/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0104 - accuracy: 0.4765 - mae: 0.0104 - mse: 0.0019 - val_loss: 0.0181 - val_accuracy: 0.4808 - val_mae: 0.0181 - val_mse: 0.0076\n",
            "Epoch 80/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0104 - accuracy: 0.4765 - mae: 0.0104 - mse: 0.0019 - val_loss: 0.0180 - val_accuracy: 0.4808 - val_mae: 0.0180 - val_mse: 0.0076\n",
            "Epoch 81/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0104 - accuracy: 0.4765 - mae: 0.0104 - mse: 0.0019 - val_loss: 0.0180 - val_accuracy: 0.4808 - val_mae: 0.0180 - val_mse: 0.0076\n",
            "Epoch 82/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0104 - accuracy: 0.4765 - mae: 0.0104 - mse: 0.0019 - val_loss: 0.0180 - val_accuracy: 0.4808 - val_mae: 0.0180 - val_mse: 0.0076\n",
            "Epoch 83/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0104 - accuracy: 0.4765 - mae: 0.0104 - mse: 0.0019 - val_loss: 0.0180 - val_accuracy: 0.4808 - val_mae: 0.0180 - val_mse: 0.0076\n",
            "Epoch 84/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0104 - accuracy: 0.4765 - mae: 0.0104 - mse: 0.0018 - val_loss: 0.0180 - val_accuracy: 0.4808 - val_mae: 0.0180 - val_mse: 0.0076\n",
            "Epoch 85/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0103 - accuracy: 0.4765 - mae: 0.0103 - mse: 0.0018 - val_loss: 0.0180 - val_accuracy: 0.4808 - val_mae: 0.0180 - val_mse: 0.0076\n",
            "Epoch 86/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0103 - accuracy: 0.4765 - mae: 0.0103 - mse: 0.0018 - val_loss: 0.0180 - val_accuracy: 0.4808 - val_mae: 0.0180 - val_mse: 0.0076\n",
            "Epoch 87/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0103 - accuracy: 0.4765 - mae: 0.0103 - mse: 0.0018 - val_loss: 0.0179 - val_accuracy: 0.4808 - val_mae: 0.0179 - val_mse: 0.0076\n",
            "Epoch 88/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0103 - accuracy: 0.4765 - mae: 0.0103 - mse: 0.0018 - val_loss: 0.0179 - val_accuracy: 0.4808 - val_mae: 0.0179 - val_mse: 0.0076\n",
            "Epoch 89/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0103 - accuracy: 0.4765 - mae: 0.0103 - mse: 0.0018 - val_loss: 0.0179 - val_accuracy: 0.4808 - val_mae: 0.0179 - val_mse: 0.0076\n",
            "Epoch 90/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0102 - accuracy: 0.4765 - mae: 0.0102 - mse: 0.0018 - val_loss: 0.0179 - val_accuracy: 0.4808 - val_mae: 0.0179 - val_mse: 0.0076\n",
            "Epoch 91/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0102 - accuracy: 0.4765 - mae: 0.0102 - mse: 0.0018 - val_loss: 0.0179 - val_accuracy: 0.4808 - val_mae: 0.0179 - val_mse: 0.0076\n",
            "Epoch 92/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0102 - accuracy: 0.4765 - mae: 0.0102 - mse: 0.0018 - val_loss: 0.0178 - val_accuracy: 0.4808 - val_mae: 0.0178 - val_mse: 0.0076\n",
            "Epoch 93/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0102 - accuracy: 0.4765 - mae: 0.0102 - mse: 0.0018 - val_loss: 0.0178 - val_accuracy: 0.4808 - val_mae: 0.0178 - val_mse: 0.0076\n",
            "Epoch 94/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0102 - accuracy: 0.4765 - mae: 0.0102 - mse: 0.0018 - val_loss: 0.0178 - val_accuracy: 0.4808 - val_mae: 0.0178 - val_mse: 0.0076\n",
            "Epoch 95/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0102 - accuracy: 0.4765 - mae: 0.0102 - mse: 0.0018 - val_loss: 0.0178 - val_accuracy: 0.4808 - val_mae: 0.0178 - val_mse: 0.0076\n",
            "Epoch 96/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0101 - accuracy: 0.4765 - mae: 0.0101 - mse: 0.0018 - val_loss: 0.0178 - val_accuracy: 0.4808 - val_mae: 0.0178 - val_mse: 0.0076\n",
            "Epoch 97/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0101 - accuracy: 0.4765 - mae: 0.0101 - mse: 0.0018 - val_loss: 0.0178 - val_accuracy: 0.4808 - val_mae: 0.0178 - val_mse: 0.0076\n",
            "Epoch 98/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0101 - accuracy: 0.4765 - mae: 0.0101 - mse: 0.0018 - val_loss: 0.0177 - val_accuracy: 0.4808 - val_mae: 0.0177 - val_mse: 0.0076\n",
            "Epoch 99/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0101 - accuracy: 0.4765 - mae: 0.0101 - mse: 0.0018 - val_loss: 0.0177 - val_accuracy: 0.4808 - val_mae: 0.0177 - val_mse: 0.0076\n",
            "Epoch 100/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0101 - accuracy: 0.4765 - mae: 0.0101 - mse: 0.0018 - val_loss: 0.0177 - val_accuracy: 0.4808 - val_mae: 0.0177 - val_mse: 0.0076\n",
            "Epoch 101/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0101 - accuracy: 0.4765 - mae: 0.0101 - mse: 0.0018 - val_loss: 0.0177 - val_accuracy: 0.4808 - val_mae: 0.0177 - val_mse: 0.0076\n",
            "Epoch 102/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0101 - accuracy: 0.4765 - mae: 0.0101 - mse: 0.0018 - val_loss: 0.0177 - val_accuracy: 0.4808 - val_mae: 0.0177 - val_mse: 0.0076\n",
            "Epoch 103/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0101 - accuracy: 0.4765 - mae: 0.0101 - mse: 0.0018 - val_loss: 0.0177 - val_accuracy: 0.4808 - val_mae: 0.0177 - val_mse: 0.0076\n",
            "Epoch 104/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0101 - accuracy: 0.4765 - mae: 0.0101 - mse: 0.0018 - val_loss: 0.0177 - val_accuracy: 0.4808 - val_mae: 0.0177 - val_mse: 0.0076\n",
            "Epoch 105/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0100 - accuracy: 0.4765 - mae: 0.0100 - mse: 0.0018 - val_loss: 0.0177 - val_accuracy: 0.4808 - val_mae: 0.0177 - val_mse: 0.0076\n",
            "Epoch 106/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0100 - accuracy: 0.4765 - mae: 0.0100 - mse: 0.0018 - val_loss: 0.0177 - val_accuracy: 0.4808 - val_mae: 0.0177 - val_mse: 0.0076\n",
            "Epoch 107/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0100 - accuracy: 0.4765 - mae: 0.0100 - mse: 0.0018 - val_loss: 0.0176 - val_accuracy: 0.4808 - val_mae: 0.0176 - val_mse: 0.0076\n",
            "Epoch 108/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0100 - accuracy: 0.4765 - mae: 0.0100 - mse: 0.0018 - val_loss: 0.0176 - val_accuracy: 0.4808 - val_mae: 0.0176 - val_mse: 0.0076\n",
            "Epoch 109/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0100 - accuracy: 0.4765 - mae: 0.0100 - mse: 0.0018 - val_loss: 0.0176 - val_accuracy: 0.4808 - val_mae: 0.0176 - val_mse: 0.0076\n",
            "Epoch 110/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0100 - accuracy: 0.4765 - mae: 0.0100 - mse: 0.0018 - val_loss: 0.0176 - val_accuracy: 0.4808 - val_mae: 0.0176 - val_mse: 0.0076\n",
            "Epoch 111/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0100 - accuracy: 0.4765 - mae: 0.0100 - mse: 0.0018 - val_loss: 0.0176 - val_accuracy: 0.4808 - val_mae: 0.0176 - val_mse: 0.0076\n",
            "Epoch 112/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0099 - accuracy: 0.4765 - mae: 0.0099 - mse: 0.0018 - val_loss: 0.0176 - val_accuracy: 0.4808 - val_mae: 0.0176 - val_mse: 0.0076\n",
            "Epoch 113/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0099 - accuracy: 0.4765 - mae: 0.0099 - mse: 0.0018 - val_loss: 0.0176 - val_accuracy: 0.4808 - val_mae: 0.0176 - val_mse: 0.0076\n",
            "Epoch 114/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0099 - accuracy: 0.4765 - mae: 0.0099 - mse: 0.0018 - val_loss: 0.0176 - val_accuracy: 0.4808 - val_mae: 0.0176 - val_mse: 0.0076\n",
            "Epoch 115/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0099 - accuracy: 0.4765 - mae: 0.0099 - mse: 0.0018 - val_loss: 0.0176 - val_accuracy: 0.4808 - val_mae: 0.0176 - val_mse: 0.0076\n",
            "Epoch 116/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0099 - accuracy: 0.4765 - mae: 0.0099 - mse: 0.0018 - val_loss: 0.0176 - val_accuracy: 0.4808 - val_mae: 0.0176 - val_mse: 0.0076\n",
            "Epoch 117/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0099 - accuracy: 0.4765 - mae: 0.0099 - mse: 0.0018 - val_loss: 0.0176 - val_accuracy: 0.4808 - val_mae: 0.0176 - val_mse: 0.0076\n",
            "Epoch 118/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0099 - accuracy: 0.4765 - mae: 0.0099 - mse: 0.0018 - val_loss: 0.0176 - val_accuracy: 0.4808 - val_mae: 0.0176 - val_mse: 0.0076\n",
            "Epoch 119/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0099 - accuracy: 0.4765 - mae: 0.0099 - mse: 0.0018 - val_loss: 0.0176 - val_accuracy: 0.4808 - val_mae: 0.0176 - val_mse: 0.0076\n",
            "Epoch 120/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0099 - accuracy: 0.4765 - mae: 0.0099 - mse: 0.0018 - val_loss: 0.0176 - val_accuracy: 0.4808 - val_mae: 0.0176 - val_mse: 0.0076\n",
            "Epoch 121/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0099 - accuracy: 0.4765 - mae: 0.0099 - mse: 0.0018 - val_loss: 0.0176 - val_accuracy: 0.4808 - val_mae: 0.0176 - val_mse: 0.0076\n",
            "Epoch 122/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0099 - accuracy: 0.4765 - mae: 0.0099 - mse: 0.0018 - val_loss: 0.0176 - val_accuracy: 0.4808 - val_mae: 0.0176 - val_mse: 0.0076\n",
            "Epoch 123/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0099 - accuracy: 0.4765 - mae: 0.0099 - mse: 0.0018 - val_loss: 0.0176 - val_accuracy: 0.4808 - val_mae: 0.0176 - val_mse: 0.0076\n",
            "Epoch 124/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0099 - accuracy: 0.4765 - mae: 0.0099 - mse: 0.0018 - val_loss: 0.0176 - val_accuracy: 0.4808 - val_mae: 0.0176 - val_mse: 0.0076\n",
            "Epoch 125/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0099 - accuracy: 0.4765 - mae: 0.0099 - mse: 0.0018 - val_loss: 0.0176 - val_accuracy: 0.4808 - val_mae: 0.0176 - val_mse: 0.0076\n",
            "Epoch 126/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0099 - accuracy: 0.4765 - mae: 0.0099 - mse: 0.0018 - val_loss: 0.0176 - val_accuracy: 0.4808 - val_mae: 0.0176 - val_mse: 0.0076\n",
            "Epoch 127/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0099 - accuracy: 0.4765 - mae: 0.0099 - mse: 0.0018 - val_loss: 0.0176 - val_accuracy: 0.4808 - val_mae: 0.0176 - val_mse: 0.0076\n",
            "Epoch 128/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0099 - accuracy: 0.4765 - mae: 0.0099 - mse: 0.0018 - val_loss: 0.0176 - val_accuracy: 0.4808 - val_mae: 0.0176 - val_mse: 0.0076\n",
            "Epoch 129/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0098 - accuracy: 0.4765 - mae: 0.0098 - mse: 0.0018 - val_loss: 0.0176 - val_accuracy: 0.4808 - val_mae: 0.0176 - val_mse: 0.0076\n",
            "Epoch 130/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0098 - accuracy: 0.4765 - mae: 0.0098 - mse: 0.0018 - val_loss: 0.0176 - val_accuracy: 0.4808 - val_mae: 0.0176 - val_mse: 0.0076\n",
            "Epoch 131/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0098 - accuracy: 0.4765 - mae: 0.0098 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 132/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0098 - accuracy: 0.4765 - mae: 0.0098 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 133/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0098 - accuracy: 0.4765 - mae: 0.0098 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 134/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0098 - accuracy: 0.4765 - mae: 0.0098 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 135/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0098 - accuracy: 0.4765 - mae: 0.0098 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 136/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0098 - accuracy: 0.4765 - mae: 0.0098 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 137/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0098 - accuracy: 0.4765 - mae: 0.0098 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 138/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0098 - accuracy: 0.4765 - mae: 0.0098 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 139/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0098 - accuracy: 0.4765 - mae: 0.0098 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 140/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0098 - accuracy: 0.4765 - mae: 0.0098 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 141/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0097 - accuracy: 0.4765 - mae: 0.0097 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 142/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0097 - accuracy: 0.4765 - mae: 0.0097 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 143/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0097 - accuracy: 0.4765 - mae: 0.0097 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 144/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0097 - accuracy: 0.4765 - mae: 0.0097 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 145/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0097 - accuracy: 0.4765 - mae: 0.0097 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 146/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0097 - accuracy: 0.4765 - mae: 0.0097 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 147/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0097 - accuracy: 0.4765 - mae: 0.0097 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 148/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0097 - accuracy: 0.4765 - mae: 0.0097 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 149/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0097 - accuracy: 0.4765 - mae: 0.0097 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 150/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0097 - accuracy: 0.4765 - mae: 0.0097 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 151/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0097 - accuracy: 0.4765 - mae: 0.0097 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 152/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0097 - accuracy: 0.4765 - mae: 0.0097 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 153/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0097 - accuracy: 0.4765 - mae: 0.0097 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 154/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0097 - accuracy: 0.4765 - mae: 0.0097 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 155/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0097 - accuracy: 0.4765 - mae: 0.0097 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 156/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0097 - accuracy: 0.4765 - mae: 0.0097 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 157/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0097 - accuracy: 0.4765 - mae: 0.0097 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 158/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0097 - accuracy: 0.4765 - mae: 0.0097 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 159/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0097 - accuracy: 0.4765 - mae: 0.0097 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 160/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0097 - accuracy: 0.4765 - mae: 0.0097 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 161/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0097 - accuracy: 0.4765 - mae: 0.0097 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 162/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0097 - accuracy: 0.4765 - mae: 0.0097 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 163/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 164/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 165/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 166/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 167/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 168/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 169/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 170/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 171/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 172/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 173/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 174/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 175/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 176/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 177/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 178/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 179/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 180/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 181/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 182/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 183/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 184/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 185/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 186/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 187/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 188/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 189/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 190/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0095 - accuracy: 0.4765 - mae: 0.0095 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 191/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0095 - accuracy: 0.4765 - mae: 0.0095 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 192/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0095 - accuracy: 0.4765 - mae: 0.0095 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 193/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0095 - accuracy: 0.4765 - mae: 0.0095 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 194/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0095 - accuracy: 0.4765 - mae: 0.0095 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 195/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0095 - accuracy: 0.4765 - mae: 0.0095 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 196/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0095 - accuracy: 0.4765 - mae: 0.0095 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 197/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0095 - accuracy: 0.4765 - mae: 0.0095 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 198/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0095 - accuracy: 0.4765 - mae: 0.0095 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 199/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0095 - accuracy: 0.4765 - mae: 0.0095 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 200/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0095 - accuracy: 0.4765 - mae: 0.0095 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hasil nya"
      ],
      "metadata": {
        "id": "HblkSjAQI7qj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'], label='loss')\n",
        "plt.plot(history.history['val_loss'], label='Valid')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['accuracy'], label='acc')\n",
        "plt.plot(history.history['val_accuracy'], label='Valid')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['mae'], label='mae')\n",
        "plt.plot(history.history['val_mae'], label='Valid mae')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['mse'], label='mse')\n",
        "plt.plot(history.history['val_mse'], label='Valid mse')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fe46671e-33ac-4e21-8a6a-e833e6661f92",
        "id": "OxHorOxbI7qj"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxc5X3v8c9vtI0ka7VlvGMBBi8YbCM7C2G7BGzIjR1amuCmvfiSXAKv0DTJbVNSbkMufdE2S9O0eZEEGnyzNNQQUhKnMSWBBgoEsGVwDLYxXjBY4EXetG8j/e4fcyRGQrJG1kgzPvN9v17zmnOe8zxzfjqSf+fxc848x9wdEREJr0i6AxARkbGlRC8iEnJK9CIiIadELyISckr0IiIhl5vuAAaaNGmSz549O91hiIicVjZv3nzE3asG25ZxiX727NnU1tamOwwRkdOKmb0x1DYN3YiIhJwSvYhIyCnRi4iEXMaN0YtIuHR1dVFXV0d7e3u6QwmFaDTKjBkzyMvLS7qNEr2IjKm6ujpKSkqYPXs2ZpbucE5r7s7Ro0epq6ujuro66XYauhGRMdXe3s7EiROV5FPAzJg4ceKI/3ekRC8iY05JPnVO5ViGJtE3tXfxD79+jS37T6Q7FBGRjBKaRN/d4/zjE7t48Y3j6Q5FRCSjhCbRTyiIX1duaOtKcyQiIpklNIk+NyfChIJcGtuV6EWkv3379jF37lzWrFnDueeey8c//nEef/xxLr74YubMmcPGjRvZuHEj73vf+1i8eDHvf//72blzJwDd3d38+Z//OUuXLuWCCy7g3nvvTfNPM3Khur2yNJpLY1ss3WGIyBD+7y+2sf3txpR+5vxppdz54QXD1tu9ezc/+clPWLt2LUuXLuWBBx7gmWeeYf369fzN3/wNP/zhD3n66afJzc3l8ccf5y//8i/56U9/yv33309ZWRmbNm2io6ODiy++mKuvvnpEtzemW7gSfWGeevQiMqjq6moWLlwIwIIFC7jyyisxMxYuXMi+fftoaGjgxhtvZNeuXZgZXV3xXPKrX/2KrVu38vDDDwPQ0NDArl27lOjTpTSaR6PG6EUyVjI977FSUFDQtxyJRPrWI5EIsViMv/qrv+KKK67gkUceYd++fVx++eVA/EtK3/rWt1i+fHk6wk6J0IzRQ2+PXkM3IjJyDQ0NTJ8+HYDvf//7feXLly/nO9/5Tl8P/7XXXqOlpSUdIZ6ykCX6XPXoReSUfOELX+CLX/wiixcvJhZ7p8P4yU9+kvnz57NkyRLOP/98PvWpT/Xbfjowd093DP3U1NT4qT545Mvrt/HTF+t4+cun73+xRMJmx44dzJs3L91hhMpgx9TMNrt7zWD1k+rRm9kKM9tpZrvN7PZBtt9iZi+b2RYze8bM5gfls82sLSjfYmbfPYWfKWmlhXk0d8To6cmsk5eISDoNezHWzHKAe4CrgDpgk5mtd/ftCdUecPfvBvVXAt8AVgTb9rj7otSGPbjSaC7u0NQRo6ww+Sk8RUTCLJke/TJgt7vvdfdOYB2wKrGCuyfeGFsMpKVLXRokd43Ti4i8I5lEPx3Yn7BeF5T1Y2afNrM9wFeBzyRsqjazl8zsKTO7ZLAdmNnNZlZrZrX19fUjCL+/0mg80WsaBBGRd6Tsrht3v8fdzwb+Avg/QfEBYJa7LwY+DzxgZqWDtL3P3WvcvaaqquqUYygtjI9E6UtTIiLvSCbRvwXMTFifEZQNZR3wEQB373D3o8HyZmAPcO6phTq83h69pkEQEXlHMol+EzDHzKrNLB+4AVifWMHM5iSsfgjYFZRXBRdzMbOzgDnA3lQEPpjeC7Dq0YtIryuuuILHHnusX9k3v/lNbr311iHbXH755fTe5n3ttddy4sS7n3Px5S9/ma9//eupDXaMDJvo3T0G3AY8BuwAHnL3bWZ2V3CHDcBtZrbNzLYQH6K5MSi/FNgalD8M3OLux1L+UwR0MVZEBlq9ejXr1q3rV7Zu3TpWr16dVPsNGzZQXl4+FqGNm6TG6N19g7uf6+5nu/vdQdmX3H19sPyn7r7A3Re5+xXuvi0o/2lC+RJ3/8WY/SStxyh9+AauzHlR0yCISJ/rr7+eX/7yl3R2dgLxKYvffvttLrnkEm699VZqampYsGABd95556DtZ8+ezZEjRwC4++67Offcc/nABz7QN43xQGvWrOHWW2/lve99L2eddRZPPvkkN910E/PmzWPNmjV99Yba9+bNm7nsssu46KKLWL58OQcOHBj1MQjPpGaRXGzP48zNm6oevUimevR2OPhyaj9zykK45u+G3FxZWcmyZct49NFHWbVqFevWreOjH/0oZsbdd99NZWUl3d3dXHnllWzdupULLrhg0M/ZvHkz69atY8uWLcRiMZYsWcJFF100aN3jx4/z3HPPsX79elauXMmzzz7L9773PZYuXcqWLVtYtGjRoPueN28ef/Inf8LPf/5zqqqqePDBB7njjjtYu3btqA5ReBJ9/gSwCFWRdg5ojF5EEvQO3/Qm+vvvvx+Ahx56iPvuu49YLMaBAwfYvn37kIn+6aef5rrrrqOoqAiAlStXDloP4MMf/nDfFMhnnHFGv+mR9+3bx6JFiwbddyQS4ZVXXuGqq64C4g89mTp16qh//vAk+kgECkqp9FbddSOSqU7S8x5Lq1at4nOf+xwvvvgira2tXHTRRbz++ut8/etfZ9OmTVRUVLBmzRra29tTsr/EKZAHTo8ci8WG3Le7s2DBAp577rmUxNG335R+WroVllNhrbrrRkT6mTBhAldccQU33XRT30XYxsZGiouLKSsr49ChQzz66KMn/YxLL72Un/3sZ7S1tdHU1MQvfnHqlxyH2vd5551HfX19X6Lv6upi27Ztp7yfXuHp0QNEyyjraNUYvYi8y+rVq7nuuuv67sC58MILWbx4MXPnzmXmzJlcfPHFJ22/ZMkSPvaxj3HhhRcyefJkli5desqxDLXv/Px8Hn74YT7zmc/Q0NBALBbjs5/9LAsWjO6BLaGappgfrOT1g0f5eM9d/PaLV6Y2MBE5JZqmOPXGZJri00a0jGJv0e2VIiIJwpXoC8sp6mmiuSNGrLsn3dGIiGSEcCX6aBnRWDMAzR3q1YtkikwbIj6dncqxDFmiLye3p518unSLpUiGiEajHD16VMk+Bdydo0ePEo1GR9QudHfdAJSiWyxFMsWMGTOoq6tjNM+akHdEo1FmzJgxojbhSvSFFQCUWbNusRTJEHl5eVRXV6c7jKwWsqEb9ehFRAYKZ6I3TYMgItIrZIk+Pmd0GS3q0YuIBEKW6OM9+jJr0QPCRUQCoUz0VXntuhgrIhIIV6LPi0JulEm5bZoGQUQkkFSiN7MVZrbTzHab2e2DbL/FzF42sy1m9oyZzU/Y9sWg3U4zW57K4AcVLacyRzNYioj0GjbRm1kOcA9wDTAfWJ2YyAMPuPtCd18EfBX4RtB2PnADsABYAXw7+LyxEy2j3Np0MVZEJJBMj34ZsNvd97p7J7AOWJVYwd0bE1aLgd7vOq8C1rl7h7u/DuwOPm/sFJZTZi26vVJEJJBMop8O7E9YrwvK+jGzT5vZHuI9+s+MsO3NZlZrZrWj/pp0tIwJrtsrRUR6pexirLvf4+5nA38B/J8Rtr3P3Wvcvaaqqmp0gUTLKHZNgSAi0iuZRP8WMDNhfUZQNpR1wEdOse3oRcsp7G6mpbNbc9KLiJBcot8EzDGzajPLJ35xdX1iBTObk7D6IWBXsLweuMHMCsysGpgDbBx92CcRLaMg1gQ4TbrFUkRk+Nkr3T1mZrcBjwE5wFp332ZmdwG17r4euM3MPgh0AceBG4O228zsIWA7EAM+7e7dY/SzxBWWE6GHCcTvvKkozh/T3YmIZLqkpil29w3AhgFlX0pY/tOTtL0buPtUAxyxhBksNQ2CiEjYvhkL70xsplssRUSAUCZ6zUkvIpIofIm+sLdHr1ssRUQgjIk+4eEjGqMXEQlloo/36MtNQzciIhDGRF9QChiT89o50apELyISvkQfiUBBKZNyWzmhoRsRkRAmeoDCMipy2nQxVkSEsCb6aBnl1qqhGxERQpvoyymjhRNtnemOREQk7UKa6Mso9hb16EVECGuiLyynqKeZpvYY3T0+fH0RkRALZ6KPlhPtbgLQBVkRyXqhTfR53W3kEtMtliKS9UKa6N+Z2OxEqy7Iikh2C3WiL7MWzXcjIlkvnIk+mMGyFCV6EZFwJvqEHr1usRSRbBfSRN/bo9e3Y0VEkkr0ZrbCzHaa2W4zu32Q7Z83s+1mttXMnjCzMxO2dZvZluC1PpXBDyno0VfltWvoRkSy3rAPBzezHOAe4CqgDthkZuvdfXtCtZeAGndvNbNbga8CHwu2tbn7ohTHfXLBGP3kvDZ2aRoEEclyyfTolwG73X2vu3cC64BViRXc/Tfu3hqsPg/MSG2YI5RXCLlRJue20qChGxHJcskk+unA/oT1uqBsKJ8AHk1Yj5pZrZk9b2YfGayBmd0c1Kmtr69PIqQkFFYyMdKsL0yJSNYbduhmJMzsj4Aa4LKE4jPd/S0zOwv4TzN72d33JLZz9/uA+wBqampSMzlNUSUVzc0aoxeRrJdMj/4tYGbC+oygrB8z+yBwB7DS3Tt6y939reB9L/AksHgU8SavsIIyb9JdNyKS9ZJJ9JuAOWZWbWb5wA1Av7tnzGwxcC/xJH84obzCzAqC5UnAxUDiRdyxU1RJSU8TDW2duGsGSxHJXsMO3bh7zMxuAx4DcoC17r7NzO4Cat19PfA1YALwEzMDeNPdVwLzgHvNrIf4SeXvBtytM3YKKynsbqSr22np7GZCQUpHqUREThtJZT933wBsGFD2pYTlDw7R7rfAwtEEeMoKK4jGGgHneEunEr2IZK1wfjMWoKiSiMcooY2jLbqXXkSyV3gTfWElAOXWxHElehHJYuFN9EXxRF9BM8eU6EUki4U30Qc9+gpr5rgePiIiWSy8iT7o0U+MNGuMXkSyWngTfdCjn5bfpjF6EclqIU705YAxJa9VY/QiktXCe3N5JAeiZVRFWjRGLyJZLbw9eoCiSiZGWjRGLyJZLdyJvrBS99GLSNYLd6IPJjY70dZFd48mNhOR7BTuRF9YSXF3A+5oXnoRyVohT/QVRGMNABxr6RimsohIOIU70RdNJDfWSj5dHGtRj15EslO4E33xJAAqadS99CKStUKe6KsAmGiNupdeRLJWuBP9hMkATDL16EUke4U70QdDN9NymzjSrIuxIpKdkkr0ZrbCzHaa2W4zu32Q7Z83s+1mttXMnjCzMxO23Whmu4LXjakMfljB0M2saCtHmtWjF5HsNGyiN7Mc4B7gGmA+sNrM5g+o9hJQ4+4XAA8DXw3aVgJ3Au8BlgF3mllF6sIfRv4EyI0yPbeJ+qb2cdutiEgmSaZHvwzY7e573b0TWAesSqzg7r9x99Zg9XlgRrC8HPi1ux9z9+PAr4EVqQk9CWZQXMXknCYON2noRkSyUzKJfjqwP2G9LigbyieAR0fS1sxuNrNaM6utr69PIqQRKJ7ERGukXoleRLJUSi/GmtkfATXA10bSzt3vc/cad6+pqqpKZUhQXEVZzwma2mO0d3Wn9rNFRE4DyST6t4CZCeszgrJ+zOyDwB3ASnfvGEnbMVU8mQmx4wDq1YtIVkom0W8C5phZtZnlAzcA6xMrmNli4F7iSf5wwqbHgKvNrCK4CHt1UDZ+iicR7TwGOPW6xVJEstCwT5hy95iZ3UY8QecAa919m5ndBdS6+3riQzUTgJ+YGcCb7r7S3Y+Z2V8TP1kA3OXux8bkJxlKcRWRni5KaONwoxK9iGSfpB4l6O4bgA0Dyr6UsPzBk7RdC6w91QBHrW8ahAb16EUkK4X7m7HQ9+3YSbrzRkSyVBYk+niPfna0VYleRLJS1iT6WQUtSvQikpWyINHHh25m5DdrGgQRyUrhT/Q5eVBYyZRIg3r0IpKVwp/oAUqnUeXHqG/uwN3THY2IyLjKjkRfMpWK7iN0dTvHW/XsWBHJLtmR6EunUtIZnyztQENbmoMRERlf2ZHoS6aR33GUXGK8fUIXZEUku2RHoi+diuFM5gRvn1CPXkSyS3Yk+pJpAMzIPcHbGroRkSyTHYm+dCoAc4ubNXQjIlknOxJ90KM/u6CRAxq6EZEskx2JvqgScgqYmacxehHJPtmR6M2gZApT7TgHG9uJdfekOyIRkXGTHYkeoHQald1H6HE4rKkQRCSLZE+iL5lKaVf8S1MavhGRbJI9ib50GgVthwDn7QbdeSMi2SOpRG9mK8xsp5ntNrPbB9l+qZm9aGYxM7t+wLZuM9sSvNYPbDtuSqcR6e6gnGb16EUkqwz7zFgzywHuAa4C6oBNZrbe3bcnVHsTWAP82SAf0ebui1IQ6+iUzQDg3KjuvBGR7JLMw8GXAbvdfS+Ama0DVgF9id7d9wXbMvd2lvJZACwsbmDvsdY0ByMiMn6SGbqZDuxPWK8LypIVNbNaM3vezD4yWAUzuzmoU1tfXz+Cjx6BsniiPy96gv3H1aMXkewxHhdjz3T3GuAPgW+a2dkDK7j7fe5e4+41VVVVYxNFUSXkFTM79yj7j7XS06MHkIhIdkgm0b8FzExYnxGUJcXd3wre9wJPAotHEF/qmEH5TKZ4PR2xHuqbdS+9iGSHZBL9JmCOmVWbWT5wA5DU3TNmVmFmBcHyJOBiEsb2x135LCo6DwDwpsbpRSRLDJvo3T0G3AY8BuwAHnL3bWZ2l5mtBDCzpWZWB/wBcK+ZbQuazwNqzex3wG+Avxtwt874Kp9FUVs80e9XoheRLJHMXTe4+wZgw4CyLyUsbyI+pDOw3W+BhaOMMXXKZpLTcYISa1WPXkSyRvZ8Mxb6brG8cEKTEr2IZI2sTPQLJzRo6EZEskZWJvpzC46rRy8iWSO7En1xFeRGOTPnKIcaO2jv6k53RCIiYy67Er0ZlJ/JlJ6DgO68EZHskF2JHqCymor2OgD21LekORgRkbGXhYn+LKLNbwLOnvrmdEcjIjLmsjLRW1cr80valOhFJCtkX6KvqAbgPWUn2KuhGxHJAtmX6Cvjif78wmPsqW/GXbNYiki4ZV+iL58FlsM5uYdpao9xpLkz3RGJiIyp7Ev0OXlQPpOpwS2WGqcXkbDLvkQPUHkW5W3xh2ZpnF5Ewi47E31FNXmN+4jmRdSjF5HQy85EX3kW1t7Aook9vHaoKd3RiIiMqexM9JPmAPCBiuPsOKBELyLhlp2JvmouAIsKDnCkuYPDTe1pDkhEZOxkZ6Ivmwl5xZzt8Quy6tWLSJgllejNbIWZ7TSz3WZ2+yDbLzWzF80sZmbXD9h2o5ntCl43pirwUYlEYPJcqtr2ArDjQGOaAxIRGTvDJnozywHuAa4B5gOrzWz+gGpvAmuABwa0rQTuBN4DLAPuNLOK0YedAlXzyD26k+nlhWx/W4leRMIrmR79MmC3u+91905gHbAqsYK773P3rUDPgLbLgV+7+zF3Pw78GliRgrhHb/JcaDnM0sk96tGLSKglk+inA/sT1uuCsmSMpu3YqpoHwPtL6tl7pEVPmxKR0MqIi7FmdrOZ1ZpZbX19/fjsdHI80S/If5vuHufVg7ogKyLhlEyifwuYmbA+IyhLRlJt3f0+d69x95qqqqokP3qUSqdBQSmzu98EYGvdifHZr4jIOEsm0W8C5phZtZnlAzcA65P8/MeAq82sIrgIe3VQln5mcMYCio6/SlVJAVv2K9GLSDgNm+jdPQbcRjxB7wAecvdtZnaXma0EMLOlZlYH/AFwr5ltC9oeA/6a+MliE3BXUJYZplyAHXyZRdNLlOhFJLRyk6nk7huADQPKvpSwvIn4sMxgbdcCa0cR49iZeiFsvJfLJjXy61c7aWjroqwwL91RiYikVEZcjE2bqRcCUFMQvzHo5bqGdEYjIjImsjvRV50HOQVUd+4G4He6ICsiIZTdiT4nD86YT8GRlzlrUjEvvXk83RGJiKRcdid6iA/fHNjKRbPKqX3jOD09eli4iISLEv2UC6D9BFdMbedEaxevHdYXp0QkXJTop18EwLK8+EyWG1/PnLs/RURSQYn+jPMhr4iJx15iWlmUF/Yq0YtIuCjR5+TC9Iuw/RtZVl3JC68fw13j9CISHkr0ADPfAwdf5v2zCjnS3MGe+pZ0RyQikjJK9BBP9N7N5cXxL0795tXDaQ5IRCR1lOgBZtQAMLlhC/OnlvIf2w6mOSARkdRRogcoqoSqubDvWVacP4XNbxzncGN7uqMSEUkJJfpec66GfU9zzTlRAH61/VCaAxIRSQ0l+l7zV0FPjHOOP031pGL+4xUN34hIOCjR95q2BEqnYzt+wbULp/DbPUeob+pId1QiIqOmRN8rEoF5K2H3E3xkfhk9DhtePpDuqERERk2JPtH8VdDdwZxjTzF3Sgk/35Lso3FFRDKXEn2ime+Bslmw9UFWLprGi2+eYP+x1nRHJSIyKkr0iSIRuOCjsPdJrpuTS07E+Jfn30h3VCIio5JUojezFWa208x2m9ntg2wvMLMHg+0vmNnsoHy2mbWZ2Zbg9d3Uhj8GLrwBvIepb/w71y6cyo9feJPG9q50RyUicsqGTfRmlgPcA1wDzAdWm9n8AdU+ARx393OAfwC+krBtj7svCl63pCjusTNpTnzq4pd+xKcuqaa5I8a/vvBmuqMSETllyfTolwG73X2vu3cC64BVA+qsAn4QLD8MXGlmlrowx9myT0H9q5zfupGLz5nIPz+9V716ETltJZPopwP7E9brgrJB67h7DGgAJgbbqs3sJTN7yswuGWwHZnazmdWaWW19ff2IfoAxcf7vQekMePYfuX3FPI62dPJPj+9Kd1QiIqdkrC/GHgBmufti4PPAA2ZWOrCSu9/n7jXuXlNVVTXGISUhJw/eeyu88QwLe3Zww9KZfP+3+9h5UI8ZFJHTTzKJ/i1gZsL6jKBs0DpmlguUAUfdvcPdjwK4+2ZgD3DuaIMeFxetgdLp8O+f48+urKa8KI9b/2WzhnBE5LSTTKLfBMwxs2ozywduANYPqLMeuDFYvh74T3d3M6sKLuZiZmcBc4C9qQl9jBVMgA/9PRzezsQt3+HbH7+IN4+1ctsDL9He1Z3u6EREkjZsog/G3G8DHgN2AA+5+zYzu8vMVgbV7gcmmtlu4kM0vbdgXgpsNbMtxC/S3uLup89DWc+7Bs7/fXjyb1nWuZG7rzufp3fV80ffe4ETrZ3pjk5EJCmWac9Hramp8dra2nSH8Y7OFvh/18KR1+CPH+GXJ87kcw9uYdbEIn5w0zKmlxemO0IREcxss7vXDLZN34wdTn4x/OFDUDIVfnQdHyrewQ9uWsahxnY+cs+zPLPrSLojFBE5KSX6ZJScATf9B1SeBT/+A9537BEevuX9lEZz+eO1L/C3j+6gM9aT7ihFRAalRJ+sCZPhf26As/8b/PJ/c94zn+UXn1zADUtnce9Te7n+u79l35GWdEcpIvIuSvQjES2D1evgijtg+88o+t4l/O3CQ3zn40vYd6SFD/3T0/zbi3XpjlJEpB8l+pGK5MBlX4BPPhFP/D/+fa7Z/uc8sbqMBdPL+PxDv+Oz617S/fYikjGU6E/VtEVw81Nw2e2w97+o+tflPFj0Nb5Rc4JfbH2ba//xaZ56rZ5Mu6tJRLKPbq9MhfYG2PQ9eO7b0HqE9pLZ/Hvb+Tzaeh5dM97HLcsX8/6zJ6U7ShEJsZPdXqlEn0pd7bDtEXj5IfyN57BYG91E2NEzi7oJC5m+8DIWLHovkUlzIC+a7mhFJESU6NMh1gH7NxLb8ySHt/8X5ce2UkQ7AD1EoHwWkZIzoGgSFE8M3qugeBIUTQzeJ8XfcwvS/MOISKY7WaLPHe9gskZuAVRfQm71JUz7IMS6OnnyuWfZuPFZCk7sYW7DQc6NtXNG0y4KuzZhrUfBh5hDJ7/knZNBUSXkT4h/kavvPVguGLA+cDmvCE7jxwSIyKlRoh8nuXn5XH7pFVx2yeU8v/cYP3xuH0+8epjOWA8FuRFqZpVx8Yw8FlZ0ce6EDibnNMWTf+sRaOl9PwLNh6Dz9fjUDJ3N8Zcn+2Utg5z84JUXf8/Nf3fZwOVILlgkfpKwSPxz3rVswXLk5MujasMQ+x+q/WBxnqw972yDQcrsJGWJbYYqY+h6w+2r369x4Mk61dsZZvtY7z/Ttg+sPob7z8mPf0EzxTR0k0ZN7V08v/cYz+05ynN7j7LzYCM9wa9jQkEuc6eUUD2pmJmVRcysLGRmRRGzKouoKimg7wFe7hBrT0j8LfFXR9M7y4nl3Z0DXl2DLAfvsY74ck9XfD/eAwTv7sGyJyz3DKg3sA2DtD9JG5FsM70G/tcTp9RUQzcZqiSax1Xzz+Cq+fEzeFtnNzsPNbHjQCPb327k1YONPPVaPYebOvq1K8iNMK28kCmlUaaWRZlS1vteyNSyCqZURKksyicSOc2HaQY7iSR1okg8CQ12chpsmf7tGEFZX2cpsYxBygarl8y++h2Udx+jk25/10EdYfuwbR9YPcPiKx6bu/OU6DNIYX4Oi2aWs2hmeb/y9q5u6o63sf94K3XHWtl/vI23T7RxsKGdF14/xqHGdmI9/f9g8nMinFFWwNTSQqaURZk0oYCywjzKCnMpK8qjNJrHhIJcigtyKcrP6Xsvys8lJ1NOEH3DOwA5aQ1F5HSmRH8aiOblcM7kCZwzecKg23t6nCMtHRxsaOdAQzsHG9p5u6Gtb/2l/cc51txJS2dyD0yJ5kWYUJBLUX7/k0BBboSciJEbiZCbY8GykROJBO9GXk7/9dyIkZNj5EUiRCJGjoGZETEgeI+YYcTfCdYjQY6PBIk+Yta3HgnGuiMJn5XYtvezIsFQd19Z5N1tgzCwYJw0cXjesHeG+XuX++okrvf/nH6fMcQ+eofr7SSf03cpYZh99HrXz5DwO+0d6usb9bd3b5PwUqIPgUjEmFwSZXJJlAtmDF2vq7uHpvYYDW1dNLR10doRo6Wzm9bOGC0dA947Y7R2dNMSlDV3xDje2kOs24n1ON09TqxnwHp3D909Tlew3t0zzH+bJSP1P3n0lvU/UdgDTVAAAAbPSURBVCTW6z3BMGi7AXV494ko8URjAxYGPVmN8ETGgBPgaOJ7V51TPNEOfqyMeVNL+dbqxe/a32gp0WeRvJwIlcX5VBbnj8v+3HtPCMGruwd36HHHib/j0JNYFpwcetzp8fhn9AQXcfvqJbzHP6L/Nk/4rJ5ge2/d+Od6X7u+IX3io6e9bT3YZxBi//pBPQaWe//PYECbd9UPCvo+P3F5qH0MWB/smPe26StL2HfwUyXU718nseFI2vXfX/+NiWEOjK//tv7tT/rzJdlusPgYUG+w2JOJj0H3c7L4+tfpVy9YmFkxNg8yUqKXMWNm5OYYuRpeF0mrpCY1M7MVZrbTzHab2e2DbC8wsweD7S+Y2eyEbV8Mynea2fLUhS4iIskYNtGbWQ5wD3ANMB9YbWbzB1T7BHDc3c8B/gH4StB2PnADsABYAXw7+DwRERknyfTolwG73X2vu3cC64BVA+qsAn4QLD8MXGnxqw+rgHXu3uHurwO7g88TEZFxkkyinw7sT1ivC8oGrePuMaABmJhkW8zsZjOrNbPa+vr65KMXEZFhZcSDR9z9PnevcfeaqqqqdIcjIhIqyST6t4CZCeszgrJB65hZLlAGHE2yrYiIjKFkEv0mYI6ZVZtZPvGLq+sH1FkP3BgsXw/8p8dvFl0P3BDclVMNzAE2piZ0ERFJxrD30bt7zMxuAx4jPuHIWnffZmZ3AbXuvh64H/iRme0GjhE/GRDUewjYDsSAT7sPNem6iIiMhYybptjM6oE3RvERk4AjKQonlRTXyGRqXJC5sSmukcnUuODUYjvT3Qe9yJlxiX60zKx2qDmZ00lxjUymxgWZG5viGplMjQtSH1tG3HUjIiJjR4leRCTkwpjo70t3AENQXCOTqXFB5samuEYmU+OCFMcWujF6ERHpL4w9ehERSaBELyIScqFJ9MPNmT+Occw0s9+Y2XYz22ZmfxqUf9nM3jKzLcHr2jTFt8/MXg5iqA3KKs3s12a2K3ivGOeYzks4LlvMrNHMPpuOY2Zma83ssJm9klA26PGxuH8K/ua2mtmScY7ra2b2arDvR8ysPCifbWZtCcftu2MV10liG/J3N17PqBgirgcTYtpnZluC8nE7ZifJEWP3d+bup/2L+Dd29wBnAfnA74D5aYplKrAkWC4BXiM+j/+XgT/LgGO1D5g0oOyrwO3B8u3AV9L8uzwInJmOYwZcCiwBXhnu+ADXAo8Sf+zne4EXxjmuq4HcYPkrCXHNTqyXpmM26O8u+LfwO6AAqA7+3eaMV1wDtv898KXxPmYnyRFj9ncWlh59MnPmjwt3P+DuLwbLTcAOBpmaOcMkPk/gB8BH0hjLlcAedx/Nt6NPmbv/F/FpPBINdXxWAT/0uOeBcjObOl5xufuvPD4tOMDzxCcNHHdDHLOhjNszKk4Wl5kZ8FHgX8di3ydzkhwxZn9nYUn0Sc17P94s/kjFxcALQdFtwX+91o738EgCB35lZpvN7Oag7Ax3PxAsHwTOSE9oQHyepMR/fJlwzIY6Ppn0d3cT8V5fr2oze8nMnjKzS9IU02C/u0w5ZpcAh9x9V0LZuB+zATlizP7OwpLoM46ZTQB+CnzW3RuB7wBnA4uAA8T/25gOH3D3JcQfDflpM7s0caPH/6+YlntuLT476krgJ0FRphyzPuk8PkMxszuITxr446DoADDL3RcDnwceMLPScQ4r4353A6ymf4di3I/ZIDmiT6r/zsKS6DNq3nszyyP+C/yxu/8bgLsfcvdud+8B/pk0PVLR3d8K3g8DjwRxHOr9r2DwfjgdsRE/+bzo7oeCGDPimDH08Un7352ZrQH+O/DxIDkQDIscDZY3Ex8HP3c84zrJ7y4Tjlku8HvAg71l433MBssRjOHfWVgSfTJz5o+LYOzvfmCHu38joTxxTO064JWBbcchtmIzK+ldJn4x7xX6P0/gRuDn4x1boF8vKxOOWWCo47Me+B/BXRHvBRoS/us95sxsBfAFYKW7tyaUV5lZTrB8FvHnQOwdr7iC/Q71u8uEZ1R8EHjV3et6C8bzmA2VIxjLv7PxuMo8Hi/iV6ZfI34mviONcXyA+H+5tgJbgte1wI+Al4Py9cDUNMR2FvE7Hn4HbOs9TsSf7/sEsAt4HKhMQ2zFxJ9KVpZQNu7HjPiJ5gDQRXws9BNDHR/id0HcE/zNvQzUjHNcu4mP3fb+nX03qPv7we93C/Ai8OE0HLMhf3fAHcEx2wlcM55xBeXfB24ZUHfcjtlJcsSY/Z1pCgQRkZALy9CNiIgMQYleRCTklOhFREJOiV5EJOSU6EVEQk6JXkQk5JToRURC7v8D9zpYO5CKFcgAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# model 5"
      ],
      "metadata": {
        "id": "CK3MLeRwZi2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(29, input_dim=29, activation = 'linear')) \n",
        "model.add(Dense(1, activation = 'linear'))\n",
        "\n",
        "model.compile(loss=\"mean_absolute_error\", optimizer=optimizers.SGD(learning_rate=0.001), metrics=['accuracy','mae', 'mse'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db768aa5-60ad-4fa6-c55c-db30b9250c07",
        "id": "osdj1Z1qZi2P"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_30\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_87 (Dense)            (None, 29)                870       \n",
            "                                                                 \n",
            " dense_88 (Dense)            (None, 1)                 30        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 900\n",
            "Trainable params: 900\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=200, batch_size=32, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5018542-6f43-40a2-8a8f-1af2bc97f4a0",
        "id": "xW1evat6Zi2P"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "12/12 [==============================] - 1s 34ms/step - loss: 0.3718 - accuracy: 0.3269 - mae: 0.3718 - mse: 0.1872 - val_loss: 0.3392 - val_accuracy: 0.4167 - val_mae: 0.3392 - val_mse: 0.1523\n",
            "Epoch 2/200\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.3261 - accuracy: 0.3546 - mae: 0.3261 - mse: 0.1513 - val_loss: 0.2940 - val_accuracy: 0.4295 - val_mae: 0.2940 - val_mse: 0.1205\n",
            "Epoch 3/200\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.2827 - accuracy: 0.3684 - mae: 0.2827 - mse: 0.1208 - val_loss: 0.2519 - val_accuracy: 0.4423 - val_mae: 0.2519 - val_mse: 0.0945\n",
            "Epoch 4/200\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.2422 - accuracy: 0.4072 - mae: 0.2422 - mse: 0.0959 - val_loss: 0.2141 - val_accuracy: 0.4551 - val_mae: 0.2141 - val_mse: 0.0737\n",
            "Epoch 5/200\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.2068 - accuracy: 0.4294 - mae: 0.2068 - mse: 0.0760 - val_loss: 0.1816 - val_accuracy: 0.4615 - val_mae: 0.1816 - val_mse: 0.0575\n",
            "Epoch 6/200\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.1771 - accuracy: 0.4515 - mae: 0.1771 - mse: 0.0607 - val_loss: 0.1550 - val_accuracy: 0.4744 - val_mae: 0.1550 - val_mse: 0.0455\n",
            "Epoch 7/200\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.1529 - accuracy: 0.4543 - mae: 0.1529 - mse: 0.0488 - val_loss: 0.1323 - val_accuracy: 0.4808 - val_mae: 0.1323 - val_mse: 0.0362\n",
            "Epoch 8/200\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.1316 - accuracy: 0.4626 - mae: 0.1316 - mse: 0.0391 - val_loss: 0.1121 - val_accuracy: 0.4808 - val_mae: 0.1121 - val_mse: 0.0290\n",
            "Epoch 9/200\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.1135 - accuracy: 0.4654 - mae: 0.1135 - mse: 0.0314 - val_loss: 0.0956 - val_accuracy: 0.4808 - val_mae: 0.0956 - val_mse: 0.0237\n",
            "Epoch 10/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0983 - accuracy: 0.4681 - mae: 0.0983 - mse: 0.0257 - val_loss: 0.0811 - val_accuracy: 0.4808 - val_mae: 0.0811 - val_mse: 0.0198\n",
            "Epoch 11/200\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0851 - accuracy: 0.4709 - mae: 0.0851 - mse: 0.0211 - val_loss: 0.0687 - val_accuracy: 0.4808 - val_mae: 0.0687 - val_mse: 0.0167\n",
            "Epoch 12/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0736 - accuracy: 0.4765 - mae: 0.0736 - mse: 0.0173 - val_loss: 0.0601 - val_accuracy: 0.4808 - val_mae: 0.0601 - val_mse: 0.0146\n",
            "Epoch 13/200\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0648 - accuracy: 0.4765 - mae: 0.0648 - mse: 0.0146 - val_loss: 0.0524 - val_accuracy: 0.4808 - val_mae: 0.0524 - val_mse: 0.0130\n",
            "Epoch 14/200\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.0568 - accuracy: 0.4765 - mae: 0.0568 - mse: 0.0123 - val_loss: 0.0463 - val_accuracy: 0.4808 - val_mae: 0.0463 - val_mse: 0.0119\n",
            "Epoch 15/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0507 - accuracy: 0.4765 - mae: 0.0507 - mse: 0.0106 - val_loss: 0.0419 - val_accuracy: 0.4808 - val_mae: 0.0419 - val_mse: 0.0110\n",
            "Epoch 16/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0459 - accuracy: 0.4765 - mae: 0.0459 - mse: 0.0093 - val_loss: 0.0387 - val_accuracy: 0.4808 - val_mae: 0.0387 - val_mse: 0.0105\n",
            "Epoch 17/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0424 - accuracy: 0.4765 - mae: 0.0424 - mse: 0.0084 - val_loss: 0.0360 - val_accuracy: 0.4808 - val_mae: 0.0360 - val_mse: 0.0101\n",
            "Epoch 18/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0398 - accuracy: 0.4765 - mae: 0.0398 - mse: 0.0077 - val_loss: 0.0338 - val_accuracy: 0.4808 - val_mae: 0.0338 - val_mse: 0.0098\n",
            "Epoch 19/200\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0374 - accuracy: 0.4765 - mae: 0.0374 - mse: 0.0070 - val_loss: 0.0318 - val_accuracy: 0.4808 - val_mae: 0.0318 - val_mse: 0.0095\n",
            "Epoch 20/200\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0353 - accuracy: 0.4765 - mae: 0.0353 - mse: 0.0065 - val_loss: 0.0299 - val_accuracy: 0.4808 - val_mae: 0.0299 - val_mse: 0.0093\n",
            "Epoch 21/200\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.0331 - accuracy: 0.4765 - mae: 0.0331 - mse: 0.0060 - val_loss: 0.0285 - val_accuracy: 0.4808 - val_mae: 0.0285 - val_mse: 0.0091\n",
            "Epoch 22/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0313 - accuracy: 0.4765 - mae: 0.0313 - mse: 0.0056 - val_loss: 0.0272 - val_accuracy: 0.4808 - val_mae: 0.0272 - val_mse: 0.0090\n",
            "Epoch 23/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0293 - accuracy: 0.4765 - mae: 0.0293 - mse: 0.0051 - val_loss: 0.0261 - val_accuracy: 0.4808 - val_mae: 0.0261 - val_mse: 0.0089\n",
            "Epoch 24/200\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0273 - accuracy: 0.4765 - mae: 0.0273 - mse: 0.0047 - val_loss: 0.0252 - val_accuracy: 0.4808 - val_mae: 0.0252 - val_mse: 0.0087\n",
            "Epoch 25/200\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.0258 - accuracy: 0.4765 - mae: 0.0258 - mse: 0.0044 - val_loss: 0.0246 - val_accuracy: 0.4808 - val_mae: 0.0246 - val_mse: 0.0086\n",
            "Epoch 26/200\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0244 - accuracy: 0.4765 - mae: 0.0244 - mse: 0.0041 - val_loss: 0.0239 - val_accuracy: 0.4808 - val_mae: 0.0239 - val_mse: 0.0085\n",
            "Epoch 27/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0231 - accuracy: 0.4765 - mae: 0.0231 - mse: 0.0038 - val_loss: 0.0233 - val_accuracy: 0.4808 - val_mae: 0.0233 - val_mse: 0.0084\n",
            "Epoch 28/200\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0218 - accuracy: 0.4765 - mae: 0.0218 - mse: 0.0036 - val_loss: 0.0228 - val_accuracy: 0.4808 - val_mae: 0.0228 - val_mse: 0.0083\n",
            "Epoch 29/200\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0206 - accuracy: 0.4765 - mae: 0.0206 - mse: 0.0034 - val_loss: 0.0224 - val_accuracy: 0.4808 - val_mae: 0.0224 - val_mse: 0.0082\n",
            "Epoch 30/200\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.0196 - accuracy: 0.4765 - mae: 0.0196 - mse: 0.0032 - val_loss: 0.0221 - val_accuracy: 0.4808 - val_mae: 0.0221 - val_mse: 0.0081\n",
            "Epoch 31/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0189 - accuracy: 0.4765 - mae: 0.0189 - mse: 0.0031 - val_loss: 0.0218 - val_accuracy: 0.4808 - val_mae: 0.0218 - val_mse: 0.0081\n",
            "Epoch 32/200\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0182 - accuracy: 0.4765 - mae: 0.0182 - mse: 0.0030 - val_loss: 0.0216 - val_accuracy: 0.4808 - val_mae: 0.0216 - val_mse: 0.0080\n",
            "Epoch 33/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0176 - accuracy: 0.4765 - mae: 0.0176 - mse: 0.0029 - val_loss: 0.0213 - val_accuracy: 0.4808 - val_mae: 0.0213 - val_mse: 0.0080\n",
            "Epoch 34/200\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.0171 - accuracy: 0.4765 - mae: 0.0171 - mse: 0.0028 - val_loss: 0.0211 - val_accuracy: 0.4808 - val_mae: 0.0211 - val_mse: 0.0079\n",
            "Epoch 35/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0166 - accuracy: 0.4765 - mae: 0.0166 - mse: 0.0027 - val_loss: 0.0208 - val_accuracy: 0.4808 - val_mae: 0.0208 - val_mse: 0.0079\n",
            "Epoch 36/200\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0162 - accuracy: 0.4765 - mae: 0.0162 - mse: 0.0026 - val_loss: 0.0206 - val_accuracy: 0.4808 - val_mae: 0.0206 - val_mse: 0.0079\n",
            "Epoch 37/200\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0159 - accuracy: 0.4765 - mae: 0.0159 - mse: 0.0026 - val_loss: 0.0204 - val_accuracy: 0.4808 - val_mae: 0.0204 - val_mse: 0.0078\n",
            "Epoch 38/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0155 - accuracy: 0.4765 - mae: 0.0155 - mse: 0.0025 - val_loss: 0.0203 - val_accuracy: 0.4808 - val_mae: 0.0203 - val_mse: 0.0078\n",
            "Epoch 39/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0152 - accuracy: 0.4765 - mae: 0.0152 - mse: 0.0025 - val_loss: 0.0202 - val_accuracy: 0.4808 - val_mae: 0.0202 - val_mse: 0.0078\n",
            "Epoch 40/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0149 - accuracy: 0.4765 - mae: 0.0149 - mse: 0.0024 - val_loss: 0.0200 - val_accuracy: 0.4808 - val_mae: 0.0200 - val_mse: 0.0078\n",
            "Epoch 41/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0146 - accuracy: 0.4765 - mae: 0.0146 - mse: 0.0024 - val_loss: 0.0199 - val_accuracy: 0.4808 - val_mae: 0.0199 - val_mse: 0.0077\n",
            "Epoch 42/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0143 - accuracy: 0.4765 - mae: 0.0143 - mse: 0.0023 - val_loss: 0.0197 - val_accuracy: 0.4808 - val_mae: 0.0197 - val_mse: 0.0077\n",
            "Epoch 43/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0139 - accuracy: 0.4765 - mae: 0.0139 - mse: 0.0023 - val_loss: 0.0195 - val_accuracy: 0.4808 - val_mae: 0.0195 - val_mse: 0.0077\n",
            "Epoch 44/200\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.0135 - accuracy: 0.4765 - mae: 0.0135 - mse: 0.0022 - val_loss: 0.0194 - val_accuracy: 0.4808 - val_mae: 0.0194 - val_mse: 0.0077\n",
            "Epoch 45/200\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.0133 - accuracy: 0.4765 - mae: 0.0133 - mse: 0.0022 - val_loss: 0.0193 - val_accuracy: 0.4808 - val_mae: 0.0193 - val_mse: 0.0077\n",
            "Epoch 46/200\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0130 - accuracy: 0.4765 - mae: 0.0130 - mse: 0.0022 - val_loss: 0.0192 - val_accuracy: 0.4808 - val_mae: 0.0192 - val_mse: 0.0077\n",
            "Epoch 47/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0127 - accuracy: 0.4765 - mae: 0.0127 - mse: 0.0021 - val_loss: 0.0191 - val_accuracy: 0.4808 - val_mae: 0.0191 - val_mse: 0.0077\n",
            "Epoch 48/200\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.0125 - accuracy: 0.4765 - mae: 0.0125 - mse: 0.0021 - val_loss: 0.0190 - val_accuracy: 0.4808 - val_mae: 0.0190 - val_mse: 0.0077\n",
            "Epoch 49/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0123 - accuracy: 0.4765 - mae: 0.0123 - mse: 0.0021 - val_loss: 0.0189 - val_accuracy: 0.4808 - val_mae: 0.0189 - val_mse: 0.0076\n",
            "Epoch 50/200\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.0122 - accuracy: 0.4765 - mae: 0.0122 - mse: 0.0021 - val_loss: 0.0188 - val_accuracy: 0.4808 - val_mae: 0.0188 - val_mse: 0.0076\n",
            "Epoch 51/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0120 - accuracy: 0.4765 - mae: 0.0120 - mse: 0.0020 - val_loss: 0.0187 - val_accuracy: 0.4808 - val_mae: 0.0187 - val_mse: 0.0076\n",
            "Epoch 52/200\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0119 - accuracy: 0.4765 - mae: 0.0119 - mse: 0.0020 - val_loss: 0.0186 - val_accuracy: 0.4808 - val_mae: 0.0186 - val_mse: 0.0076\n",
            "Epoch 53/200\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.0117 - accuracy: 0.4765 - mae: 0.0117 - mse: 0.0020 - val_loss: 0.0185 - val_accuracy: 0.4808 - val_mae: 0.0185 - val_mse: 0.0076\n",
            "Epoch 54/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0116 - accuracy: 0.4765 - mae: 0.0116 - mse: 0.0020 - val_loss: 0.0185 - val_accuracy: 0.4808 - val_mae: 0.0185 - val_mse: 0.0076\n",
            "Epoch 55/200\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0114 - accuracy: 0.4765 - mae: 0.0114 - mse: 0.0020 - val_loss: 0.0184 - val_accuracy: 0.4808 - val_mae: 0.0184 - val_mse: 0.0076\n",
            "Epoch 56/200\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.0113 - accuracy: 0.4765 - mae: 0.0113 - mse: 0.0020 - val_loss: 0.0184 - val_accuracy: 0.4808 - val_mae: 0.0184 - val_mse: 0.0076\n",
            "Epoch 57/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0112 - accuracy: 0.4765 - mae: 0.0112 - mse: 0.0020 - val_loss: 0.0184 - val_accuracy: 0.4808 - val_mae: 0.0184 - val_mse: 0.0076\n",
            "Epoch 58/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0111 - accuracy: 0.4765 - mae: 0.0111 - mse: 0.0019 - val_loss: 0.0183 - val_accuracy: 0.4808 - val_mae: 0.0183 - val_mse: 0.0076\n",
            "Epoch 59/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0111 - accuracy: 0.4765 - mae: 0.0111 - mse: 0.0019 - val_loss: 0.0183 - val_accuracy: 0.4808 - val_mae: 0.0183 - val_mse: 0.0076\n",
            "Epoch 60/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0110 - accuracy: 0.4765 - mae: 0.0110 - mse: 0.0019 - val_loss: 0.0183 - val_accuracy: 0.4808 - val_mae: 0.0183 - val_mse: 0.0076\n",
            "Epoch 61/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0109 - accuracy: 0.4765 - mae: 0.0109 - mse: 0.0019 - val_loss: 0.0183 - val_accuracy: 0.4808 - val_mae: 0.0183 - val_mse: 0.0076\n",
            "Epoch 62/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0109 - accuracy: 0.4765 - mae: 0.0109 - mse: 0.0019 - val_loss: 0.0183 - val_accuracy: 0.4808 - val_mae: 0.0183 - val_mse: 0.0076\n",
            "Epoch 63/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0109 - accuracy: 0.4765 - mae: 0.0109 - mse: 0.0019 - val_loss: 0.0183 - val_accuracy: 0.4808 - val_mae: 0.0183 - val_mse: 0.0076\n",
            "Epoch 64/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0108 - accuracy: 0.4765 - mae: 0.0108 - mse: 0.0019 - val_loss: 0.0183 - val_accuracy: 0.4808 - val_mae: 0.0183 - val_mse: 0.0076\n",
            "Epoch 65/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0108 - accuracy: 0.4765 - mae: 0.0108 - mse: 0.0019 - val_loss: 0.0182 - val_accuracy: 0.4808 - val_mae: 0.0182 - val_mse: 0.0076\n",
            "Epoch 66/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0107 - accuracy: 0.4765 - mae: 0.0107 - mse: 0.0019 - val_loss: 0.0182 - val_accuracy: 0.4808 - val_mae: 0.0182 - val_mse: 0.0076\n",
            "Epoch 67/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0107 - accuracy: 0.4765 - mae: 0.0107 - mse: 0.0019 - val_loss: 0.0182 - val_accuracy: 0.4808 - val_mae: 0.0182 - val_mse: 0.0076\n",
            "Epoch 68/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0107 - accuracy: 0.4765 - mae: 0.0107 - mse: 0.0019 - val_loss: 0.0182 - val_accuracy: 0.4808 - val_mae: 0.0182 - val_mse: 0.0076\n",
            "Epoch 69/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0106 - accuracy: 0.4765 - mae: 0.0106 - mse: 0.0019 - val_loss: 0.0182 - val_accuracy: 0.4808 - val_mae: 0.0182 - val_mse: 0.0076\n",
            "Epoch 70/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0106 - accuracy: 0.4765 - mae: 0.0106 - mse: 0.0019 - val_loss: 0.0182 - val_accuracy: 0.4808 - val_mae: 0.0182 - val_mse: 0.0076\n",
            "Epoch 71/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0106 - accuracy: 0.4765 - mae: 0.0106 - mse: 0.0019 - val_loss: 0.0181 - val_accuracy: 0.4808 - val_mae: 0.0181 - val_mse: 0.0076\n",
            "Epoch 72/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0105 - accuracy: 0.4765 - mae: 0.0105 - mse: 0.0019 - val_loss: 0.0181 - val_accuracy: 0.4808 - val_mae: 0.0181 - val_mse: 0.0076\n",
            "Epoch 73/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0105 - accuracy: 0.4765 - mae: 0.0105 - mse: 0.0019 - val_loss: 0.0181 - val_accuracy: 0.4808 - val_mae: 0.0181 - val_mse: 0.0076\n",
            "Epoch 74/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0105 - accuracy: 0.4765 - mae: 0.0105 - mse: 0.0019 - val_loss: 0.0181 - val_accuracy: 0.4808 - val_mae: 0.0181 - val_mse: 0.0076\n",
            "Epoch 75/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0105 - accuracy: 0.4765 - mae: 0.0105 - mse: 0.0019 - val_loss: 0.0181 - val_accuracy: 0.4808 - val_mae: 0.0181 - val_mse: 0.0076\n",
            "Epoch 76/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0105 - accuracy: 0.4765 - mae: 0.0105 - mse: 0.0019 - val_loss: 0.0181 - val_accuracy: 0.4808 - val_mae: 0.0181 - val_mse: 0.0076\n",
            "Epoch 77/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0105 - accuracy: 0.4765 - mae: 0.0105 - mse: 0.0019 - val_loss: 0.0181 - val_accuracy: 0.4808 - val_mae: 0.0181 - val_mse: 0.0076\n",
            "Epoch 78/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0104 - accuracy: 0.4765 - mae: 0.0104 - mse: 0.0019 - val_loss: 0.0181 - val_accuracy: 0.4808 - val_mae: 0.0181 - val_mse: 0.0076\n",
            "Epoch 79/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0104 - accuracy: 0.4765 - mae: 0.0104 - mse: 0.0019 - val_loss: 0.0181 - val_accuracy: 0.4808 - val_mae: 0.0181 - val_mse: 0.0076\n",
            "Epoch 80/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0104 - accuracy: 0.4765 - mae: 0.0104 - mse: 0.0019 - val_loss: 0.0180 - val_accuracy: 0.4808 - val_mae: 0.0180 - val_mse: 0.0076\n",
            "Epoch 81/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0104 - accuracy: 0.4765 - mae: 0.0104 - mse: 0.0019 - val_loss: 0.0180 - val_accuracy: 0.4808 - val_mae: 0.0180 - val_mse: 0.0076\n",
            "Epoch 82/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0104 - accuracy: 0.4765 - mae: 0.0104 - mse: 0.0019 - val_loss: 0.0180 - val_accuracy: 0.4808 - val_mae: 0.0180 - val_mse: 0.0076\n",
            "Epoch 83/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0104 - accuracy: 0.4765 - mae: 0.0104 - mse: 0.0019 - val_loss: 0.0180 - val_accuracy: 0.4808 - val_mae: 0.0180 - val_mse: 0.0076\n",
            "Epoch 84/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0104 - accuracy: 0.4765 - mae: 0.0104 - mse: 0.0018 - val_loss: 0.0180 - val_accuracy: 0.4808 - val_mae: 0.0180 - val_mse: 0.0076\n",
            "Epoch 85/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0103 - accuracy: 0.4765 - mae: 0.0103 - mse: 0.0018 - val_loss: 0.0180 - val_accuracy: 0.4808 - val_mae: 0.0180 - val_mse: 0.0076\n",
            "Epoch 86/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0103 - accuracy: 0.4765 - mae: 0.0103 - mse: 0.0018 - val_loss: 0.0180 - val_accuracy: 0.4808 - val_mae: 0.0180 - val_mse: 0.0076\n",
            "Epoch 87/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0103 - accuracy: 0.4765 - mae: 0.0103 - mse: 0.0018 - val_loss: 0.0179 - val_accuracy: 0.4808 - val_mae: 0.0179 - val_mse: 0.0076\n",
            "Epoch 88/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0103 - accuracy: 0.4765 - mae: 0.0103 - mse: 0.0018 - val_loss: 0.0179 - val_accuracy: 0.4808 - val_mae: 0.0179 - val_mse: 0.0076\n",
            "Epoch 89/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0103 - accuracy: 0.4765 - mae: 0.0103 - mse: 0.0018 - val_loss: 0.0179 - val_accuracy: 0.4808 - val_mae: 0.0179 - val_mse: 0.0076\n",
            "Epoch 90/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0102 - accuracy: 0.4765 - mae: 0.0102 - mse: 0.0018 - val_loss: 0.0179 - val_accuracy: 0.4808 - val_mae: 0.0179 - val_mse: 0.0076\n",
            "Epoch 91/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0102 - accuracy: 0.4765 - mae: 0.0102 - mse: 0.0018 - val_loss: 0.0179 - val_accuracy: 0.4808 - val_mae: 0.0179 - val_mse: 0.0076\n",
            "Epoch 92/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0102 - accuracy: 0.4765 - mae: 0.0102 - mse: 0.0018 - val_loss: 0.0178 - val_accuracy: 0.4808 - val_mae: 0.0178 - val_mse: 0.0076\n",
            "Epoch 93/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0102 - accuracy: 0.4765 - mae: 0.0102 - mse: 0.0018 - val_loss: 0.0178 - val_accuracy: 0.4808 - val_mae: 0.0178 - val_mse: 0.0076\n",
            "Epoch 94/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0102 - accuracy: 0.4765 - mae: 0.0102 - mse: 0.0018 - val_loss: 0.0178 - val_accuracy: 0.4808 - val_mae: 0.0178 - val_mse: 0.0076\n",
            "Epoch 95/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0102 - accuracy: 0.4765 - mae: 0.0102 - mse: 0.0018 - val_loss: 0.0178 - val_accuracy: 0.4808 - val_mae: 0.0178 - val_mse: 0.0076\n",
            "Epoch 96/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0101 - accuracy: 0.4765 - mae: 0.0101 - mse: 0.0018 - val_loss: 0.0178 - val_accuracy: 0.4808 - val_mae: 0.0178 - val_mse: 0.0076\n",
            "Epoch 97/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0101 - accuracy: 0.4765 - mae: 0.0101 - mse: 0.0018 - val_loss: 0.0178 - val_accuracy: 0.4808 - val_mae: 0.0178 - val_mse: 0.0076\n",
            "Epoch 98/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0101 - accuracy: 0.4765 - mae: 0.0101 - mse: 0.0018 - val_loss: 0.0177 - val_accuracy: 0.4808 - val_mae: 0.0177 - val_mse: 0.0076\n",
            "Epoch 99/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0101 - accuracy: 0.4765 - mae: 0.0101 - mse: 0.0018 - val_loss: 0.0177 - val_accuracy: 0.4808 - val_mae: 0.0177 - val_mse: 0.0076\n",
            "Epoch 100/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0101 - accuracy: 0.4765 - mae: 0.0101 - mse: 0.0018 - val_loss: 0.0177 - val_accuracy: 0.4808 - val_mae: 0.0177 - val_mse: 0.0076\n",
            "Epoch 101/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0101 - accuracy: 0.4765 - mae: 0.0101 - mse: 0.0018 - val_loss: 0.0177 - val_accuracy: 0.4808 - val_mae: 0.0177 - val_mse: 0.0076\n",
            "Epoch 102/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0101 - accuracy: 0.4765 - mae: 0.0101 - mse: 0.0018 - val_loss: 0.0177 - val_accuracy: 0.4808 - val_mae: 0.0177 - val_mse: 0.0076\n",
            "Epoch 103/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0101 - accuracy: 0.4765 - mae: 0.0101 - mse: 0.0018 - val_loss: 0.0177 - val_accuracy: 0.4808 - val_mae: 0.0177 - val_mse: 0.0076\n",
            "Epoch 104/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0101 - accuracy: 0.4765 - mae: 0.0101 - mse: 0.0018 - val_loss: 0.0177 - val_accuracy: 0.4808 - val_mae: 0.0177 - val_mse: 0.0076\n",
            "Epoch 105/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0100 - accuracy: 0.4765 - mae: 0.0100 - mse: 0.0018 - val_loss: 0.0177 - val_accuracy: 0.4808 - val_mae: 0.0177 - val_mse: 0.0076\n",
            "Epoch 106/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0100 - accuracy: 0.4765 - mae: 0.0100 - mse: 0.0018 - val_loss: 0.0177 - val_accuracy: 0.4808 - val_mae: 0.0177 - val_mse: 0.0076\n",
            "Epoch 107/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0100 - accuracy: 0.4765 - mae: 0.0100 - mse: 0.0018 - val_loss: 0.0176 - val_accuracy: 0.4808 - val_mae: 0.0176 - val_mse: 0.0076\n",
            "Epoch 108/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0100 - accuracy: 0.4765 - mae: 0.0100 - mse: 0.0018 - val_loss: 0.0176 - val_accuracy: 0.4808 - val_mae: 0.0176 - val_mse: 0.0076\n",
            "Epoch 109/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0100 - accuracy: 0.4765 - mae: 0.0100 - mse: 0.0018 - val_loss: 0.0176 - val_accuracy: 0.4808 - val_mae: 0.0176 - val_mse: 0.0076\n",
            "Epoch 110/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0100 - accuracy: 0.4765 - mae: 0.0100 - mse: 0.0018 - val_loss: 0.0176 - val_accuracy: 0.4808 - val_mae: 0.0176 - val_mse: 0.0076\n",
            "Epoch 111/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0100 - accuracy: 0.4765 - mae: 0.0100 - mse: 0.0018 - val_loss: 0.0176 - val_accuracy: 0.4808 - val_mae: 0.0176 - val_mse: 0.0076\n",
            "Epoch 112/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0099 - accuracy: 0.4765 - mae: 0.0099 - mse: 0.0018 - val_loss: 0.0176 - val_accuracy: 0.4808 - val_mae: 0.0176 - val_mse: 0.0076\n",
            "Epoch 113/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0099 - accuracy: 0.4765 - mae: 0.0099 - mse: 0.0018 - val_loss: 0.0176 - val_accuracy: 0.4808 - val_mae: 0.0176 - val_mse: 0.0076\n",
            "Epoch 114/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0099 - accuracy: 0.4765 - mae: 0.0099 - mse: 0.0018 - val_loss: 0.0176 - val_accuracy: 0.4808 - val_mae: 0.0176 - val_mse: 0.0076\n",
            "Epoch 115/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0099 - accuracy: 0.4765 - mae: 0.0099 - mse: 0.0018 - val_loss: 0.0176 - val_accuracy: 0.4808 - val_mae: 0.0176 - val_mse: 0.0076\n",
            "Epoch 116/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0099 - accuracy: 0.4765 - mae: 0.0099 - mse: 0.0018 - val_loss: 0.0176 - val_accuracy: 0.4808 - val_mae: 0.0176 - val_mse: 0.0076\n",
            "Epoch 117/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0099 - accuracy: 0.4765 - mae: 0.0099 - mse: 0.0018 - val_loss: 0.0176 - val_accuracy: 0.4808 - val_mae: 0.0176 - val_mse: 0.0076\n",
            "Epoch 118/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0099 - accuracy: 0.4765 - mae: 0.0099 - mse: 0.0018 - val_loss: 0.0176 - val_accuracy: 0.4808 - val_mae: 0.0176 - val_mse: 0.0076\n",
            "Epoch 119/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0099 - accuracy: 0.4765 - mae: 0.0099 - mse: 0.0018 - val_loss: 0.0176 - val_accuracy: 0.4808 - val_mae: 0.0176 - val_mse: 0.0076\n",
            "Epoch 120/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0099 - accuracy: 0.4765 - mae: 0.0099 - mse: 0.0018 - val_loss: 0.0176 - val_accuracy: 0.4808 - val_mae: 0.0176 - val_mse: 0.0076\n",
            "Epoch 121/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0099 - accuracy: 0.4765 - mae: 0.0099 - mse: 0.0018 - val_loss: 0.0176 - val_accuracy: 0.4808 - val_mae: 0.0176 - val_mse: 0.0076\n",
            "Epoch 122/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0099 - accuracy: 0.4765 - mae: 0.0099 - mse: 0.0018 - val_loss: 0.0176 - val_accuracy: 0.4808 - val_mae: 0.0176 - val_mse: 0.0076\n",
            "Epoch 123/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0099 - accuracy: 0.4765 - mae: 0.0099 - mse: 0.0018 - val_loss: 0.0176 - val_accuracy: 0.4808 - val_mae: 0.0176 - val_mse: 0.0076\n",
            "Epoch 124/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0099 - accuracy: 0.4765 - mae: 0.0099 - mse: 0.0018 - val_loss: 0.0176 - val_accuracy: 0.4808 - val_mae: 0.0176 - val_mse: 0.0076\n",
            "Epoch 125/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0099 - accuracy: 0.4765 - mae: 0.0099 - mse: 0.0018 - val_loss: 0.0176 - val_accuracy: 0.4808 - val_mae: 0.0176 - val_mse: 0.0076\n",
            "Epoch 126/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0099 - accuracy: 0.4765 - mae: 0.0099 - mse: 0.0018 - val_loss: 0.0176 - val_accuracy: 0.4808 - val_mae: 0.0176 - val_mse: 0.0076\n",
            "Epoch 127/200\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0099 - accuracy: 0.4765 - mae: 0.0099 - mse: 0.0018 - val_loss: 0.0176 - val_accuracy: 0.4808 - val_mae: 0.0176 - val_mse: 0.0076\n",
            "Epoch 128/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0099 - accuracy: 0.4765 - mae: 0.0099 - mse: 0.0018 - val_loss: 0.0176 - val_accuracy: 0.4808 - val_mae: 0.0176 - val_mse: 0.0076\n",
            "Epoch 129/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0098 - accuracy: 0.4765 - mae: 0.0098 - mse: 0.0018 - val_loss: 0.0176 - val_accuracy: 0.4808 - val_mae: 0.0176 - val_mse: 0.0076\n",
            "Epoch 130/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0098 - accuracy: 0.4765 - mae: 0.0098 - mse: 0.0018 - val_loss: 0.0176 - val_accuracy: 0.4808 - val_mae: 0.0176 - val_mse: 0.0076\n",
            "Epoch 131/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0098 - accuracy: 0.4765 - mae: 0.0098 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 132/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0098 - accuracy: 0.4765 - mae: 0.0098 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 133/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0098 - accuracy: 0.4765 - mae: 0.0098 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 134/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0098 - accuracy: 0.4765 - mae: 0.0098 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 135/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0098 - accuracy: 0.4765 - mae: 0.0098 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 136/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0098 - accuracy: 0.4765 - mae: 0.0098 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 137/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0098 - accuracy: 0.4765 - mae: 0.0098 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 138/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0098 - accuracy: 0.4765 - mae: 0.0098 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 139/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0098 - accuracy: 0.4765 - mae: 0.0098 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 140/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0098 - accuracy: 0.4765 - mae: 0.0098 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 141/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0097 - accuracy: 0.4765 - mae: 0.0097 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 142/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0097 - accuracy: 0.4765 - mae: 0.0097 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 143/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0097 - accuracy: 0.4765 - mae: 0.0097 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 144/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0097 - accuracy: 0.4765 - mae: 0.0097 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 145/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0097 - accuracy: 0.4765 - mae: 0.0097 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 146/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0097 - accuracy: 0.4765 - mae: 0.0097 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 147/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0097 - accuracy: 0.4765 - mae: 0.0097 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 148/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0097 - accuracy: 0.4765 - mae: 0.0097 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 149/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0097 - accuracy: 0.4765 - mae: 0.0097 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 150/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0097 - accuracy: 0.4765 - mae: 0.0097 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 151/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0097 - accuracy: 0.4765 - mae: 0.0097 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 152/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0097 - accuracy: 0.4765 - mae: 0.0097 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 153/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0097 - accuracy: 0.4765 - mae: 0.0097 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 154/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0097 - accuracy: 0.4765 - mae: 0.0097 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 155/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0097 - accuracy: 0.4765 - mae: 0.0097 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 156/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0097 - accuracy: 0.4765 - mae: 0.0097 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 157/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0097 - accuracy: 0.4765 - mae: 0.0097 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 158/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0097 - accuracy: 0.4765 - mae: 0.0097 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 159/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0097 - accuracy: 0.4765 - mae: 0.0097 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 160/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0097 - accuracy: 0.4765 - mae: 0.0097 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 161/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0097 - accuracy: 0.4765 - mae: 0.0097 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 162/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0097 - accuracy: 0.4765 - mae: 0.0097 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 163/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 164/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 165/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0175 - val_accuracy: 0.4808 - val_mae: 0.0175 - val_mse: 0.0076\n",
            "Epoch 166/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 167/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 168/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 169/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 170/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 171/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 172/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 173/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 174/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 175/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 176/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 177/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 178/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 179/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 180/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 181/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 182/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 183/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 184/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 185/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 186/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 187/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 188/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 189/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0096 - accuracy: 0.4765 - mae: 0.0096 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 190/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0095 - accuracy: 0.4765 - mae: 0.0095 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 191/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0095 - accuracy: 0.4765 - mae: 0.0095 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 192/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0095 - accuracy: 0.4765 - mae: 0.0095 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 193/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0095 - accuracy: 0.4765 - mae: 0.0095 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 194/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0095 - accuracy: 0.4765 - mae: 0.0095 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 195/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0095 - accuracy: 0.4765 - mae: 0.0095 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 196/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0095 - accuracy: 0.4765 - mae: 0.0095 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 197/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0095 - accuracy: 0.4765 - mae: 0.0095 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 198/200\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0095 - accuracy: 0.4765 - mae: 0.0095 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 199/200\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0095 - accuracy: 0.4765 - mae: 0.0095 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n",
            "Epoch 200/200\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0095 - accuracy: 0.4765 - mae: 0.0095 - mse: 0.0018 - val_loss: 0.0174 - val_accuracy: 0.4808 - val_mae: 0.0174 - val_mse: 0.0076\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hasil nya"
      ],
      "metadata": {
        "id": "4M21E-l3Zi2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'], label='loss')\n",
        "plt.plot(history.history['val_loss'], label='Valid')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['accuracy'], label='acc')\n",
        "plt.plot(history.history['val_accuracy'], label='Valid')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['mae'], label='mae')\n",
        "plt.plot(history.history['val_mae'], label='Valid mae')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['mse'], label='mse')\n",
        "plt.plot(history.history['val_mse'], label='Valid mse')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fe46671e-33ac-4e21-8a6a-e833e6661f92",
        "id": "hVzwXRdEZi2P"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# callback"
      ],
      "metadata": {
        "id": "IVxKbmo59HQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "es = EarlyStopping(monitor=('mae'),mode='min', verbose=1, patience=40)\n",
        "mc = ModelCheckpoint('best.h5', monitor='mae', mode='min', verbose=1, save_best_only=True)"
      ],
      "metadata": {
        "id": "R7J9rQlsmx_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history3 = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=1000, batch_size=32, verbose=1, callbacks=[es, mc])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YkDXm-Nm0GF",
        "outputId": "6469e3a3-5b2a-43ba-a69e-f7d7bca60517"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            " 1/12 [=>............................] - ETA: 4s - loss: 0.5118 - accuracy: 0.1875 - mae: 0.5118 - mse: 0.3458\n",
            "Epoch 00001: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 1s 19ms/step - loss: 0.5976 - accuracy: 0.2271 - mae: 0.5976 - mse: 0.4510 - val_loss: 0.5130 - val_accuracy: 0.2436 - val_mae: 0.5130 - val_mse: 0.3571\n",
            "Epoch 2/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.5931 - accuracy: 0.1562 - mae: 0.5931 - mse: 0.4298\n",
            "Epoch 00002: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.4897 - accuracy: 0.2742 - mae: 0.4897 - mse: 0.3249 - val_loss: 0.4160 - val_accuracy: 0.3077 - val_mae: 0.4160 - val_mse: 0.2520\n",
            "Epoch 3/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.3990 - accuracy: 0.3125 - mae: 0.3990 - mse: 0.2281\n",
            "Epoch 00003: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.3940 - accuracy: 0.3047 - mae: 0.3940 - mse: 0.2294 - val_loss: 0.3402 - val_accuracy: 0.3590 - val_mae: 0.3402 - val_mse: 0.1791\n",
            "Epoch 4/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.3366 - accuracy: 0.1875 - mae: 0.3366 - mse: 0.1758\n",
            "Epoch 00004: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.3220 - accuracy: 0.3684 - mae: 0.3220 - mse: 0.1679 - val_loss: 0.2887 - val_accuracy: 0.3782 - val_mae: 0.2887 - val_mse: 0.1358\n",
            "Epoch 5/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2653 - accuracy: 0.4688 - mae: 0.2653 - mse: 0.1285\n",
            "Epoch 00005: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2795 - accuracy: 0.3934 - mae: 0.2795 - mse: 0.1316 - val_loss: 0.2617 - val_accuracy: 0.3974 - val_mae: 0.2617 - val_mse: 0.1146\n",
            "Epoch 6/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2963 - accuracy: 0.3438 - mae: 0.2963 - mse: 0.1595\n",
            "Epoch 00006: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.2591 - accuracy: 0.4044 - mae: 0.2591 - mse: 0.1132 - val_loss: 0.2469 - val_accuracy: 0.4103 - val_mae: 0.2469 - val_mse: 0.1025\n",
            "Epoch 7/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2370 - accuracy: 0.4375 - mae: 0.2370 - mse: 0.1099\n",
            "Epoch 00007: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2474 - accuracy: 0.4100 - mae: 0.2474 - mse: 0.1028 - val_loss: 0.2374 - val_accuracy: 0.4167 - val_mae: 0.2374 - val_mse: 0.0945\n",
            "Epoch 8/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2456 - accuracy: 0.3750 - mae: 0.2456 - mse: 0.1059\n",
            "Epoch 00008: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2394 - accuracy: 0.4238 - mae: 0.2394 - mse: 0.0952 - val_loss: 0.2316 - val_accuracy: 0.4167 - val_mae: 0.2316 - val_mse: 0.0894\n",
            "Epoch 9/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2649 - accuracy: 0.3750 - mae: 0.2649 - mse: 0.1066\n",
            "Epoch 00009: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2343 - accuracy: 0.4266 - mae: 0.2343 - mse: 0.0904 - val_loss: 0.2270 - val_accuracy: 0.4359 - val_mae: 0.2270 - val_mse: 0.0857\n",
            "Epoch 10/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2857 - accuracy: 0.3125 - mae: 0.2857 - mse: 0.1201\n",
            "Epoch 00010: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2299 - accuracy: 0.4377 - mae: 0.2299 - mse: 0.0860 - val_loss: 0.2230 - val_accuracy: 0.4487 - val_mae: 0.2230 - val_mse: 0.0827\n",
            "Epoch 11/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1895 - accuracy: 0.5000 - mae: 0.1895 - mse: 0.0562\n",
            "Epoch 00011: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2254 - accuracy: 0.4404 - mae: 0.2254 - mse: 0.0827 - val_loss: 0.2193 - val_accuracy: 0.4551 - val_mae: 0.2193 - val_mse: 0.0800\n",
            "Epoch 12/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1956 - accuracy: 0.5312 - mae: 0.1956 - mse: 0.0508\n",
            "Epoch 00012: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2214 - accuracy: 0.4404 - mae: 0.2214 - mse: 0.0799 - val_loss: 0.2157 - val_accuracy: 0.4551 - val_mae: 0.2157 - val_mse: 0.0778\n",
            "Epoch 13/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1807 - accuracy: 0.3750 - mae: 0.1807 - mse: 0.0611\n",
            "Epoch 00013: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.2174 - accuracy: 0.4432 - mae: 0.2174 - mse: 0.0775 - val_loss: 0.2122 - val_accuracy: 0.4615 - val_mae: 0.2122 - val_mse: 0.0756\n",
            "Epoch 14/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2085 - accuracy: 0.5625 - mae: 0.2085 - mse: 0.0851\n",
            "Epoch 00014: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2135 - accuracy: 0.4432 - mae: 0.2135 - mse: 0.0751 - val_loss: 0.2087 - val_accuracy: 0.4615 - val_mae: 0.2087 - val_mse: 0.0732\n",
            "Epoch 15/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1678 - accuracy: 0.5000 - mae: 0.1678 - mse: 0.0513\n",
            "Epoch 00015: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2096 - accuracy: 0.4543 - mae: 0.2096 - mse: 0.0719 - val_loss: 0.2052 - val_accuracy: 0.4615 - val_mae: 0.2052 - val_mse: 0.0714\n",
            "Epoch 16/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2348 - accuracy: 0.5938 - mae: 0.2348 - mse: 0.0841\n",
            "Epoch 00016: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2059 - accuracy: 0.4543 - mae: 0.2059 - mse: 0.0699 - val_loss: 0.2018 - val_accuracy: 0.4679 - val_mae: 0.2018 - val_mse: 0.0696\n",
            "Epoch 17/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1948 - accuracy: 0.3750 - mae: 0.1948 - mse: 0.0547\n",
            "Epoch 00017: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.2023 - accuracy: 0.4571 - mae: 0.2023 - mse: 0.0679 - val_loss: 0.1983 - val_accuracy: 0.4679 - val_mae: 0.1983 - val_mse: 0.0678\n",
            "Epoch 18/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2476 - accuracy: 0.5312 - mae: 0.2476 - mse: 0.0875\n",
            "Epoch 00018: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1988 - accuracy: 0.4571 - mae: 0.1988 - mse: 0.0660 - val_loss: 0.1950 - val_accuracy: 0.4679 - val_mae: 0.1950 - val_mse: 0.0663\n",
            "Epoch 19/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2146 - accuracy: 0.3750 - mae: 0.2146 - mse: 0.0728\n",
            "Epoch 00019: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1954 - accuracy: 0.4598 - mae: 0.1954 - mse: 0.0643 - val_loss: 0.1917 - val_accuracy: 0.4679 - val_mae: 0.1917 - val_mse: 0.0648\n",
            "Epoch 20/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1655 - accuracy: 0.3750 - mae: 0.1655 - mse: 0.0464\n",
            "Epoch 00020: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1919 - accuracy: 0.4571 - mae: 0.1919 - mse: 0.0629 - val_loss: 0.1885 - val_accuracy: 0.4679 - val_mae: 0.1885 - val_mse: 0.0630\n",
            "Epoch 21/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1742 - accuracy: 0.4375 - mae: 0.1742 - mse: 0.0502\n",
            "Epoch 00021: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1884 - accuracy: 0.4626 - mae: 0.1884 - mse: 0.0606 - val_loss: 0.1853 - val_accuracy: 0.4679 - val_mae: 0.1853 - val_mse: 0.0614\n",
            "Epoch 22/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1954 - accuracy: 0.5625 - mae: 0.1954 - mse: 0.0730\n",
            "Epoch 00022: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1851 - accuracy: 0.4626 - mae: 0.1851 - mse: 0.0587 - val_loss: 0.1821 - val_accuracy: 0.4679 - val_mae: 0.1821 - val_mse: 0.0600\n",
            "Epoch 23/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1591 - accuracy: 0.5625 - mae: 0.1591 - mse: 0.0440\n",
            "Epoch 00023: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1817 - accuracy: 0.4626 - mae: 0.1817 - mse: 0.0575 - val_loss: 0.1791 - val_accuracy: 0.4679 - val_mae: 0.1791 - val_mse: 0.0586\n",
            "Epoch 24/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1553 - accuracy: 0.5625 - mae: 0.1553 - mse: 0.0408\n",
            "Epoch 00024: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1785 - accuracy: 0.4626 - mae: 0.1785 - mse: 0.0556 - val_loss: 0.1763 - val_accuracy: 0.4679 - val_mae: 0.1763 - val_mse: 0.0571\n",
            "Epoch 25/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1589 - accuracy: 0.5000 - mae: 0.1589 - mse: 0.0501\n",
            "Epoch 00025: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1753 - accuracy: 0.4626 - mae: 0.1753 - mse: 0.0539 - val_loss: 0.1737 - val_accuracy: 0.4679 - val_mae: 0.1737 - val_mse: 0.0558\n",
            "Epoch 26/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1872 - accuracy: 0.3438 - mae: 0.1872 - mse: 0.0551\n",
            "Epoch 00026: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1723 - accuracy: 0.4626 - mae: 0.1723 - mse: 0.0524 - val_loss: 0.1713 - val_accuracy: 0.4679 - val_mae: 0.1713 - val_mse: 0.0544\n",
            "Epoch 27/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1697 - accuracy: 0.4375 - mae: 0.1697 - mse: 0.0445\n",
            "Epoch 00027: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1694 - accuracy: 0.4626 - mae: 0.1694 - mse: 0.0508 - val_loss: 0.1681 - val_accuracy: 0.4679 - val_mae: 0.1681 - val_mse: 0.0532\n",
            "Epoch 28/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2233 - accuracy: 0.4688 - mae: 0.2233 - mse: 0.0924\n",
            "Epoch 00028: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1662 - accuracy: 0.4626 - mae: 0.1662 - mse: 0.0495 - val_loss: 0.1654 - val_accuracy: 0.4679 - val_mae: 0.1654 - val_mse: 0.0520\n",
            "Epoch 29/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1789 - accuracy: 0.4688 - mae: 0.1789 - mse: 0.0469\n",
            "Epoch 00029: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1633 - accuracy: 0.4626 - mae: 0.1633 - mse: 0.0480 - val_loss: 0.1626 - val_accuracy: 0.4679 - val_mae: 0.1626 - val_mse: 0.0508\n",
            "Epoch 30/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1385 - accuracy: 0.4688 - mae: 0.1385 - mse: 0.0416\n",
            "Epoch 00030: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1605 - accuracy: 0.4626 - mae: 0.1605 - mse: 0.0469 - val_loss: 0.1601 - val_accuracy: 0.4679 - val_mae: 0.1601 - val_mse: 0.0497\n",
            "Epoch 31/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1269 - accuracy: 0.4688 - mae: 0.1269 - mse: 0.0415\n",
            "Epoch 00031: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1577 - accuracy: 0.4654 - mae: 0.1577 - mse: 0.0455 - val_loss: 0.1577 - val_accuracy: 0.4679 - val_mae: 0.1577 - val_mse: 0.0486\n",
            "Epoch 32/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1359 - accuracy: 0.4062 - mae: 0.1359 - mse: 0.0345\n",
            "Epoch 00032: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1550 - accuracy: 0.4654 - mae: 0.1550 - mse: 0.0442 - val_loss: 0.1555 - val_accuracy: 0.4679 - val_mae: 0.1555 - val_mse: 0.0476\n",
            "Epoch 33/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1229 - accuracy: 0.6562 - mae: 0.1229 - mse: 0.0223\n",
            "Epoch 00033: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1523 - accuracy: 0.4654 - mae: 0.1523 - mse: 0.0430 - val_loss: 0.1536 - val_accuracy: 0.4679 - val_mae: 0.1536 - val_mse: 0.0466\n",
            "Epoch 34/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1747 - accuracy: 0.4375 - mae: 0.1747 - mse: 0.0541\n",
            "Epoch 00034: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1496 - accuracy: 0.4681 - mae: 0.1496 - mse: 0.0415 - val_loss: 0.1506 - val_accuracy: 0.4679 - val_mae: 0.1506 - val_mse: 0.0456\n",
            "Epoch 35/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1748 - accuracy: 0.4375 - mae: 0.1748 - mse: 0.0550\n",
            "Epoch 00035: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1470 - accuracy: 0.4681 - mae: 0.1470 - mse: 0.0406 - val_loss: 0.1483 - val_accuracy: 0.4679 - val_mae: 0.1483 - val_mse: 0.0446\n",
            "Epoch 36/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.2125 - accuracy: 0.3125 - mae: 0.2125 - mse: 0.0801\n",
            "Epoch 00036: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1444 - accuracy: 0.4681 - mae: 0.1444 - mse: 0.0395 - val_loss: 0.1463 - val_accuracy: 0.4679 - val_mae: 0.1463 - val_mse: 0.0438\n",
            "Epoch 37/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1538 - accuracy: 0.3438 - mae: 0.1538 - mse: 0.0389\n",
            "Epoch 00037: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1418 - accuracy: 0.4709 - mae: 0.1418 - mse: 0.0383 - val_loss: 0.1440 - val_accuracy: 0.4679 - val_mae: 0.1440 - val_mse: 0.0429\n",
            "Epoch 38/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1231 - accuracy: 0.5312 - mae: 0.1231 - mse: 0.0278\n",
            "Epoch 00038: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1394 - accuracy: 0.4709 - mae: 0.1394 - mse: 0.0373 - val_loss: 0.1417 - val_accuracy: 0.4679 - val_mae: 0.1417 - val_mse: 0.0420\n",
            "Epoch 39/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1324 - accuracy: 0.4062 - mae: 0.1324 - mse: 0.0297\n",
            "Epoch 00039: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1370 - accuracy: 0.4709 - mae: 0.1370 - mse: 0.0363 - val_loss: 0.1394 - val_accuracy: 0.4679 - val_mae: 0.1394 - val_mse: 0.0412\n",
            "Epoch 40/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1240 - accuracy: 0.5312 - mae: 0.1240 - mse: 0.0316\n",
            "Epoch 00040: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1347 - accuracy: 0.4709 - mae: 0.1347 - mse: 0.0354 - val_loss: 0.1373 - val_accuracy: 0.4679 - val_mae: 0.1373 - val_mse: 0.0404\n",
            "Epoch 41/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1139 - accuracy: 0.3750 - mae: 0.1139 - mse: 0.0227\n",
            "Epoch 00041: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1324 - accuracy: 0.4709 - mae: 0.1324 - mse: 0.0345 - val_loss: 0.1352 - val_accuracy: 0.4679 - val_mae: 0.1352 - val_mse: 0.0397\n",
            "Epoch 42/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1459 - accuracy: 0.5938 - mae: 0.1459 - mse: 0.0389\n",
            "Epoch 00042: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1300 - accuracy: 0.4709 - mae: 0.1300 - mse: 0.0336 - val_loss: 0.1338 - val_accuracy: 0.4679 - val_mae: 0.1338 - val_mse: 0.0391\n",
            "Epoch 43/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1074 - accuracy: 0.3750 - mae: 0.1074 - mse: 0.0172\n",
            "Epoch 00043: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1277 - accuracy: 0.4709 - mae: 0.1277 - mse: 0.0327 - val_loss: 0.1315 - val_accuracy: 0.4679 - val_mae: 0.1315 - val_mse: 0.0384\n",
            "Epoch 44/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1140 - accuracy: 0.5000 - mae: 0.1140 - mse: 0.0242\n",
            "Epoch 00044: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1256 - accuracy: 0.4709 - mae: 0.1256 - mse: 0.0319 - val_loss: 0.1290 - val_accuracy: 0.4679 - val_mae: 0.1290 - val_mse: 0.0376\n",
            "Epoch 45/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0760 - accuracy: 0.4375 - mae: 0.0760 - mse: 0.0113\n",
            "Epoch 00045: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1233 - accuracy: 0.4709 - mae: 0.1233 - mse: 0.0311 - val_loss: 0.1279 - val_accuracy: 0.4679 - val_mae: 0.1279 - val_mse: 0.0372\n",
            "Epoch 46/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1197 - accuracy: 0.3750 - mae: 0.1197 - mse: 0.0266\n",
            "Epoch 00046: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1213 - accuracy: 0.4709 - mae: 0.1213 - mse: 0.0303 - val_loss: 0.1256 - val_accuracy: 0.4679 - val_mae: 0.1256 - val_mse: 0.0365\n",
            "Epoch 47/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1393 - accuracy: 0.4375 - mae: 0.1393 - mse: 0.0443\n",
            "Epoch 00047: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1191 - accuracy: 0.4709 - mae: 0.1191 - mse: 0.0295 - val_loss: 0.1231 - val_accuracy: 0.4679 - val_mae: 0.1231 - val_mse: 0.0357\n",
            "Epoch 48/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1254 - accuracy: 0.5312 - mae: 0.1254 - mse: 0.0263\n",
            "Epoch 00048: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1169 - accuracy: 0.4709 - mae: 0.1169 - mse: 0.0288 - val_loss: 0.1213 - val_accuracy: 0.4679 - val_mae: 0.1213 - val_mse: 0.0351\n",
            "Epoch 49/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1052 - accuracy: 0.4375 - mae: 0.1052 - mse: 0.0218\n",
            "Epoch 00049: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1148 - accuracy: 0.4709 - mae: 0.1148 - mse: 0.0281 - val_loss: 0.1195 - val_accuracy: 0.4679 - val_mae: 0.1195 - val_mse: 0.0347\n",
            "Epoch 50/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1079 - accuracy: 0.3750 - mae: 0.1079 - mse: 0.0230\n",
            "Epoch 00050: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1128 - accuracy: 0.4709 - mae: 0.1128 - mse: 0.0274 - val_loss: 0.1184 - val_accuracy: 0.4679 - val_mae: 0.1184 - val_mse: 0.0344\n",
            "Epoch 51/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1073 - accuracy: 0.3438 - mae: 0.1073 - mse: 0.0284\n",
            "Epoch 00051: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1109 - accuracy: 0.4709 - mae: 0.1109 - mse: 0.0268 - val_loss: 0.1169 - val_accuracy: 0.4679 - val_mae: 0.1169 - val_mse: 0.0340\n",
            "Epoch 52/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0931 - accuracy: 0.3750 - mae: 0.0931 - mse: 0.0171\n",
            "Epoch 00052: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1089 - accuracy: 0.4709 - mae: 0.1089 - mse: 0.0261 - val_loss: 0.1152 - val_accuracy: 0.4679 - val_mae: 0.1152 - val_mse: 0.0335\n",
            "Epoch 53/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0959 - accuracy: 0.4062 - mae: 0.0959 - mse: 0.0217\n",
            "Epoch 00053: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1071 - accuracy: 0.4709 - mae: 0.1071 - mse: 0.0256 - val_loss: 0.1131 - val_accuracy: 0.4679 - val_mae: 0.1131 - val_mse: 0.0329\n",
            "Epoch 54/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1306 - accuracy: 0.4375 - mae: 0.1306 - mse: 0.0406\n",
            "Epoch 00054: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.1052 - accuracy: 0.4709 - mae: 0.1052 - mse: 0.0250 - val_loss: 0.1118 - val_accuracy: 0.4679 - val_mae: 0.1118 - val_mse: 0.0327\n",
            "Epoch 55/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1103 - accuracy: 0.4375 - mae: 0.1103 - mse: 0.0335\n",
            "Epoch 00055: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1033 - accuracy: 0.4709 - mae: 0.1033 - mse: 0.0245 - val_loss: 0.1099 - val_accuracy: 0.4679 - val_mae: 0.1099 - val_mse: 0.0321\n",
            "Epoch 56/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1186 - accuracy: 0.3750 - mae: 0.1186 - mse: 0.0281\n",
            "Epoch 00056: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1017 - accuracy: 0.4709 - mae: 0.1017 - mse: 0.0240 - val_loss: 0.1083 - val_accuracy: 0.4679 - val_mae: 0.1083 - val_mse: 0.0317\n",
            "Epoch 57/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1127 - accuracy: 0.6562 - mae: 0.1127 - mse: 0.0274\n",
            "Epoch 00057: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0997 - accuracy: 0.4737 - mae: 0.0997 - mse: 0.0234 - val_loss: 0.1068 - val_accuracy: 0.4679 - val_mae: 0.1068 - val_mse: 0.0314\n",
            "Epoch 58/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0893 - accuracy: 0.5000 - mae: 0.0893 - mse: 0.0127\n",
            "Epoch 00058: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0981 - accuracy: 0.4737 - mae: 0.0981 - mse: 0.0230 - val_loss: 0.1055 - val_accuracy: 0.4679 - val_mae: 0.1055 - val_mse: 0.0311\n",
            "Epoch 59/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1046 - accuracy: 0.4688 - mae: 0.1046 - mse: 0.0235\n",
            "Epoch 00059: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0964 - accuracy: 0.4737 - mae: 0.0964 - mse: 0.0225 - val_loss: 0.1035 - val_accuracy: 0.4679 - val_mae: 0.1035 - val_mse: 0.0304\n",
            "Epoch 60/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0741 - accuracy: 0.4062 - mae: 0.0741 - mse: 0.0121\n",
            "Epoch 00060: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0948 - accuracy: 0.4737 - mae: 0.0948 - mse: 0.0220 - val_loss: 0.1021 - val_accuracy: 0.4679 - val_mae: 0.1021 - val_mse: 0.0301\n",
            "Epoch 61/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1217 - accuracy: 0.3125 - mae: 0.1217 - mse: 0.0336\n",
            "Epoch 00061: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0930 - accuracy: 0.4737 - mae: 0.0930 - mse: 0.0215 - val_loss: 0.1015 - val_accuracy: 0.4744 - val_mae: 0.1015 - val_mse: 0.0301\n",
            "Epoch 62/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0567 - accuracy: 0.3750 - mae: 0.0567 - mse: 0.0055\n",
            "Epoch 00062: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0918 - accuracy: 0.4737 - mae: 0.0918 - mse: 0.0212 - val_loss: 0.0999 - val_accuracy: 0.4744 - val_mae: 0.0999 - val_mse: 0.0296\n",
            "Epoch 63/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0756 - accuracy: 0.4062 - mae: 0.0756 - mse: 0.0126\n",
            "Epoch 00063: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0903 - accuracy: 0.4737 - mae: 0.0903 - mse: 0.0208 - val_loss: 0.0987 - val_accuracy: 0.4744 - val_mae: 0.0987 - val_mse: 0.0291\n",
            "Epoch 64/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0730 - accuracy: 0.3125 - mae: 0.0730 - mse: 0.0131\n",
            "Epoch 00064: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0892 - accuracy: 0.4737 - mae: 0.0892 - mse: 0.0205 - val_loss: 0.0979 - val_accuracy: 0.4744 - val_mae: 0.0979 - val_mse: 0.0290\n",
            "Epoch 65/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0591 - accuracy: 0.5312 - mae: 0.0591 - mse: 0.0094\n",
            "Epoch 00065: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0880 - accuracy: 0.4737 - mae: 0.0880 - mse: 0.0201 - val_loss: 0.0969 - val_accuracy: 0.4744 - val_mae: 0.0969 - val_mse: 0.0288\n",
            "Epoch 66/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0894 - accuracy: 0.3438 - mae: 0.0894 - mse: 0.0266\n",
            "Epoch 00066: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0869 - accuracy: 0.4737 - mae: 0.0869 - mse: 0.0198 - val_loss: 0.0959 - val_accuracy: 0.4744 - val_mae: 0.0959 - val_mse: 0.0286\n",
            "Epoch 67/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0845 - accuracy: 0.5000 - mae: 0.0845 - mse: 0.0160\n",
            "Epoch 00067: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0854 - accuracy: 0.4737 - mae: 0.0854 - mse: 0.0195 - val_loss: 0.0951 - val_accuracy: 0.4744 - val_mae: 0.0951 - val_mse: 0.0285\n",
            "Epoch 68/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0778 - accuracy: 0.4062 - mae: 0.0778 - mse: 0.0187\n",
            "Epoch 00068: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0843 - accuracy: 0.4737 - mae: 0.0843 - mse: 0.0192 - val_loss: 0.0937 - val_accuracy: 0.4744 - val_mae: 0.0937 - val_mse: 0.0280\n",
            "Epoch 69/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0903 - accuracy: 0.4062 - mae: 0.0903 - mse: 0.0242\n",
            "Epoch 00069: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0830 - accuracy: 0.4737 - mae: 0.0830 - mse: 0.0188 - val_loss: 0.0932 - val_accuracy: 0.4744 - val_mae: 0.0932 - val_mse: 0.0280\n",
            "Epoch 70/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1047 - accuracy: 0.3750 - mae: 0.1047 - mse: 0.0239\n",
            "Epoch 00070: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0819 - accuracy: 0.4765 - mae: 0.0819 - mse: 0.0186 - val_loss: 0.0917 - val_accuracy: 0.4744 - val_mae: 0.0917 - val_mse: 0.0274\n",
            "Epoch 71/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0763 - accuracy: 0.5000 - mae: 0.0763 - mse: 0.0159\n",
            "Epoch 00071: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0808 - accuracy: 0.4765 - mae: 0.0808 - mse: 0.0183 - val_loss: 0.0910 - val_accuracy: 0.4744 - val_mae: 0.0910 - val_mse: 0.0273\n",
            "Epoch 72/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0510 - accuracy: 0.5000 - mae: 0.0510 - mse: 0.0083\n",
            "Epoch 00072: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0798 - accuracy: 0.4765 - mae: 0.0798 - mse: 0.0180 - val_loss: 0.0906 - val_accuracy: 0.4744 - val_mae: 0.0906 - val_mse: 0.0273\n",
            "Epoch 73/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0754 - accuracy: 0.4375 - mae: 0.0754 - mse: 0.0167\n",
            "Epoch 00073: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0788 - accuracy: 0.4765 - mae: 0.0788 - mse: 0.0178 - val_loss: 0.0896 - val_accuracy: 0.4744 - val_mae: 0.0896 - val_mse: 0.0270\n",
            "Epoch 74/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1041 - accuracy: 0.5000 - mae: 0.1041 - mse: 0.0315\n",
            "Epoch 00074: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0777 - accuracy: 0.4765 - mae: 0.0777 - mse: 0.0175 - val_loss: 0.0891 - val_accuracy: 0.4744 - val_mae: 0.0891 - val_mse: 0.0269\n",
            "Epoch 75/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0928 - accuracy: 0.4375 - mae: 0.0928 - mse: 0.0203\n",
            "Epoch 00075: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0768 - accuracy: 0.4765 - mae: 0.0768 - mse: 0.0173 - val_loss: 0.0880 - val_accuracy: 0.4744 - val_mae: 0.0880 - val_mse: 0.0265\n",
            "Epoch 76/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0687 - accuracy: 0.5000 - mae: 0.0687 - mse: 0.0146\n",
            "Epoch 00076: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0758 - accuracy: 0.4765 - mae: 0.0758 - mse: 0.0171 - val_loss: 0.0870 - val_accuracy: 0.4744 - val_mae: 0.0870 - val_mse: 0.0261\n",
            "Epoch 77/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0788 - accuracy: 0.5312 - mae: 0.0788 - mse: 0.0178\n",
            "Epoch 00077: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0750 - accuracy: 0.4765 - mae: 0.0750 - mse: 0.0168 - val_loss: 0.0874 - val_accuracy: 0.4744 - val_mae: 0.0874 - val_mse: 0.0265\n",
            "Epoch 78/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0524 - accuracy: 0.4062 - mae: 0.0524 - mse: 0.0058\n",
            "Epoch 00078: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0741 - accuracy: 0.4765 - mae: 0.0741 - mse: 0.0167 - val_loss: 0.0861 - val_accuracy: 0.4744 - val_mae: 0.0861 - val_mse: 0.0260\n",
            "Epoch 79/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0896 - accuracy: 0.5312 - mae: 0.0896 - mse: 0.0205\n",
            "Epoch 00079: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0733 - accuracy: 0.4765 - mae: 0.0733 - mse: 0.0164 - val_loss: 0.0859 - val_accuracy: 0.4744 - val_mae: 0.0859 - val_mse: 0.0261\n",
            "Epoch 80/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0481 - accuracy: 0.5312 - mae: 0.0481 - mse: 0.0075\n",
            "Epoch 00080: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0724 - accuracy: 0.4765 - mae: 0.0724 - mse: 0.0162 - val_loss: 0.0851 - val_accuracy: 0.4744 - val_mae: 0.0851 - val_mse: 0.0258\n",
            "Epoch 81/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1019 - accuracy: 0.4375 - mae: 0.1019 - mse: 0.0366\n",
            "Epoch 00081: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0718 - accuracy: 0.4765 - mae: 0.0718 - mse: 0.0161 - val_loss: 0.0846 - val_accuracy: 0.4744 - val_mae: 0.0846 - val_mse: 0.0257\n",
            "Epoch 82/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0907 - accuracy: 0.6250 - mae: 0.0907 - mse: 0.0234\n",
            "Epoch 00082: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0711 - accuracy: 0.4765 - mae: 0.0711 - mse: 0.0159 - val_loss: 0.0840 - val_accuracy: 0.4744 - val_mae: 0.0840 - val_mse: 0.0255\n",
            "Epoch 83/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0633 - accuracy: 0.4688 - mae: 0.0633 - mse: 0.0119\n",
            "Epoch 00083: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0702 - accuracy: 0.4765 - mae: 0.0702 - mse: 0.0157 - val_loss: 0.0831 - val_accuracy: 0.4744 - val_mae: 0.0831 - val_mse: 0.0251\n",
            "Epoch 84/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0463 - accuracy: 0.5312 - mae: 0.0463 - mse: 0.0067\n",
            "Epoch 00084: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0697 - accuracy: 0.4765 - mae: 0.0697 - mse: 0.0155 - val_loss: 0.0830 - val_accuracy: 0.4744 - val_mae: 0.0830 - val_mse: 0.0253\n",
            "Epoch 85/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0424 - accuracy: 0.5625 - mae: 0.0424 - mse: 0.0054\n",
            "Epoch 00085: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0690 - accuracy: 0.4765 - mae: 0.0690 - mse: 0.0153 - val_loss: 0.0831 - val_accuracy: 0.4744 - val_mae: 0.0831 - val_mse: 0.0254\n",
            "Epoch 86/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0682 - accuracy: 0.3438 - mae: 0.0682 - mse: 0.0127\n",
            "Epoch 00086: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0686 - accuracy: 0.4765 - mae: 0.0686 - mse: 0.0153 - val_loss: 0.0819 - val_accuracy: 0.4744 - val_mae: 0.0819 - val_mse: 0.0250\n",
            "Epoch 87/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0512 - accuracy: 0.5312 - mae: 0.0512 - mse: 0.0082\n",
            "Epoch 00087: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0678 - accuracy: 0.4765 - mae: 0.0678 - mse: 0.0151 - val_loss: 0.0815 - val_accuracy: 0.4744 - val_mae: 0.0815 - val_mse: 0.0249\n",
            "Epoch 88/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0546 - accuracy: 0.5312 - mae: 0.0546 - mse: 0.0098\n",
            "Epoch 00088: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0673 - accuracy: 0.4765 - mae: 0.0673 - mse: 0.0150 - val_loss: 0.0810 - val_accuracy: 0.4744 - val_mae: 0.0810 - val_mse: 0.0247\n",
            "Epoch 89/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0587 - accuracy: 0.5000 - mae: 0.0587 - mse: 0.0102\n",
            "Epoch 00089: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0666 - accuracy: 0.4765 - mae: 0.0666 - mse: 0.0148 - val_loss: 0.0802 - val_accuracy: 0.4744 - val_mae: 0.0802 - val_mse: 0.0244\n",
            "Epoch 90/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0916 - accuracy: 0.4688 - mae: 0.0916 - mse: 0.0204\n",
            "Epoch 00090: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0660 - accuracy: 0.4765 - mae: 0.0660 - mse: 0.0146 - val_loss: 0.0799 - val_accuracy: 0.4744 - val_mae: 0.0799 - val_mse: 0.0244\n",
            "Epoch 91/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0612 - accuracy: 0.3438 - mae: 0.0612 - mse: 0.0096\n",
            "Epoch 00091: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0655 - accuracy: 0.4765 - mae: 0.0655 - mse: 0.0145 - val_loss: 0.0794 - val_accuracy: 0.4744 - val_mae: 0.0794 - val_mse: 0.0242\n",
            "Epoch 92/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0511 - accuracy: 0.5000 - mae: 0.0511 - mse: 0.0051\n",
            "Epoch 00092: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0651 - accuracy: 0.4765 - mae: 0.0651 - mse: 0.0144 - val_loss: 0.0791 - val_accuracy: 0.4808 - val_mae: 0.0791 - val_mse: 0.0242\n",
            "Epoch 93/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0679 - accuracy: 0.4688 - mae: 0.0679 - mse: 0.0139\n",
            "Epoch 00093: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0644 - accuracy: 0.4765 - mae: 0.0644 - mse: 0.0143 - val_loss: 0.0787 - val_accuracy: 0.4808 - val_mae: 0.0787 - val_mse: 0.0241\n",
            "Epoch 94/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0534 - accuracy: 0.4688 - mae: 0.0534 - mse: 0.0111\n",
            "Epoch 00094: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0639 - accuracy: 0.4765 - mae: 0.0639 - mse: 0.0141 - val_loss: 0.0788 - val_accuracy: 0.4808 - val_mae: 0.0788 - val_mse: 0.0242\n",
            "Epoch 95/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0842 - accuracy: 0.4688 - mae: 0.0842 - mse: 0.0197\n",
            "Epoch 00095: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0634 - accuracy: 0.4765 - mae: 0.0634 - mse: 0.0141 - val_loss: 0.0774 - val_accuracy: 0.4808 - val_mae: 0.0774 - val_mse: 0.0236\n",
            "Epoch 96/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0623 - accuracy: 0.6562 - mae: 0.0623 - mse: 0.0082\n",
            "Epoch 00096: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0628 - accuracy: 0.4765 - mae: 0.0628 - mse: 0.0138 - val_loss: 0.0780 - val_accuracy: 0.4808 - val_mae: 0.0780 - val_mse: 0.0239\n",
            "Epoch 97/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0708 - accuracy: 0.5312 - mae: 0.0708 - mse: 0.0145\n",
            "Epoch 00097: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0626 - accuracy: 0.4765 - mae: 0.0626 - mse: 0.0138 - val_loss: 0.0768 - val_accuracy: 0.4808 - val_mae: 0.0768 - val_mse: 0.0235\n",
            "Epoch 98/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0857 - accuracy: 0.5312 - mae: 0.0857 - mse: 0.0218\n",
            "Epoch 00098: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0618 - accuracy: 0.4765 - mae: 0.0618 - mse: 0.0136 - val_loss: 0.0784 - val_accuracy: 0.4808 - val_mae: 0.0784 - val_mse: 0.0240\n",
            "Epoch 99/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0600 - accuracy: 0.4688 - mae: 0.0600 - mse: 0.0157\n",
            "Epoch 00099: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0616 - accuracy: 0.4765 - mae: 0.0616 - mse: 0.0136 - val_loss: 0.0762 - val_accuracy: 0.4808 - val_mae: 0.0762 - val_mse: 0.0234\n",
            "Epoch 100/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0797 - accuracy: 0.5312 - mae: 0.0797 - mse: 0.0240\n",
            "Epoch 00100: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0609 - accuracy: 0.4765 - mae: 0.0609 - mse: 0.0134 - val_loss: 0.0755 - val_accuracy: 0.4808 - val_mae: 0.0755 - val_mse: 0.0231\n",
            "Epoch 101/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0304 - accuracy: 0.5312 - mae: 0.0304 - mse: 0.0037\n",
            "Epoch 00101: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0604 - accuracy: 0.4765 - mae: 0.0604 - mse: 0.0133 - val_loss: 0.0750 - val_accuracy: 0.4808 - val_mae: 0.0750 - val_mse: 0.0229\n",
            "Epoch 102/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0938 - accuracy: 0.5625 - mae: 0.0938 - mse: 0.0228\n",
            "Epoch 00102: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0602 - accuracy: 0.4765 - mae: 0.0602 - mse: 0.0132 - val_loss: 0.0749 - val_accuracy: 0.4808 - val_mae: 0.0749 - val_mse: 0.0230\n",
            "Epoch 103/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0345 - accuracy: 0.4375 - mae: 0.0345 - mse: 0.0047\n",
            "Epoch 00103: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0596 - accuracy: 0.4765 - mae: 0.0596 - mse: 0.0130 - val_loss: 0.0743 - val_accuracy: 0.4808 - val_mae: 0.0743 - val_mse: 0.0228\n",
            "Epoch 104/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0647 - accuracy: 0.5938 - mae: 0.0647 - mse: 0.0158\n",
            "Epoch 00104: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0593 - accuracy: 0.4765 - mae: 0.0593 - mse: 0.0129 - val_loss: 0.0738 - val_accuracy: 0.4808 - val_mae: 0.0738 - val_mse: 0.0226\n",
            "Epoch 105/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0346 - accuracy: 0.4688 - mae: 0.0346 - mse: 0.0027\n",
            "Epoch 00105: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0589 - accuracy: 0.4765 - mae: 0.0589 - mse: 0.0128 - val_loss: 0.0741 - val_accuracy: 0.4808 - val_mae: 0.0741 - val_mse: 0.0228\n",
            "Epoch 106/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0566 - accuracy: 0.4062 - mae: 0.0566 - mse: 0.0090\n",
            "Epoch 00106: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0585 - accuracy: 0.4765 - mae: 0.0585 - mse: 0.0127 - val_loss: 0.0735 - val_accuracy: 0.4808 - val_mae: 0.0735 - val_mse: 0.0226\n",
            "Epoch 107/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0457 - accuracy: 0.5312 - mae: 0.0457 - mse: 0.0086\n",
            "Epoch 00107: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0581 - accuracy: 0.4765 - mae: 0.0581 - mse: 0.0126 - val_loss: 0.0735 - val_accuracy: 0.4808 - val_mae: 0.0735 - val_mse: 0.0226\n",
            "Epoch 108/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0477 - accuracy: 0.4375 - mae: 0.0477 - mse: 0.0068\n",
            "Epoch 00108: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0576 - accuracy: 0.4765 - mae: 0.0576 - mse: 0.0125 - val_loss: 0.0725 - val_accuracy: 0.4808 - val_mae: 0.0725 - val_mse: 0.0223\n",
            "Epoch 109/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0604 - accuracy: 0.6250 - mae: 0.0604 - mse: 0.0208\n",
            "Epoch 00109: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0573 - accuracy: 0.4765 - mae: 0.0573 - mse: 0.0124 - val_loss: 0.0725 - val_accuracy: 0.4808 - val_mae: 0.0725 - val_mse: 0.0223\n",
            "Epoch 110/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0532 - accuracy: 0.5312 - mae: 0.0532 - mse: 0.0102\n",
            "Epoch 00110: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0569 - accuracy: 0.4765 - mae: 0.0569 - mse: 0.0123 - val_loss: 0.0721 - val_accuracy: 0.4808 - val_mae: 0.0721 - val_mse: 0.0222\n",
            "Epoch 111/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0426 - accuracy: 0.5312 - mae: 0.0426 - mse: 0.0082\n",
            "Epoch 00111: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0566 - accuracy: 0.4765 - mae: 0.0566 - mse: 0.0122 - val_loss: 0.0713 - val_accuracy: 0.4808 - val_mae: 0.0713 - val_mse: 0.0219\n",
            "Epoch 112/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0658 - accuracy: 0.4688 - mae: 0.0658 - mse: 0.0137\n",
            "Epoch 00112: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0560 - accuracy: 0.4765 - mae: 0.0560 - mse: 0.0121 - val_loss: 0.0726 - val_accuracy: 0.4808 - val_mae: 0.0726 - val_mse: 0.0223\n",
            "Epoch 113/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0464 - accuracy: 0.4375 - mae: 0.0464 - mse: 0.0098\n",
            "Epoch 00113: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0559 - accuracy: 0.4765 - mae: 0.0559 - mse: 0.0120 - val_loss: 0.0708 - val_accuracy: 0.4808 - val_mae: 0.0708 - val_mse: 0.0218\n",
            "Epoch 114/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0638 - accuracy: 0.4375 - mae: 0.0638 - mse: 0.0210\n",
            "Epoch 00114: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0555 - accuracy: 0.4765 - mae: 0.0555 - mse: 0.0119 - val_loss: 0.0703 - val_accuracy: 0.4808 - val_mae: 0.0703 - val_mse: 0.0215\n",
            "Epoch 115/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0587 - accuracy: 0.5625 - mae: 0.0587 - mse: 0.0138\n",
            "Epoch 00115: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0552 - accuracy: 0.4765 - mae: 0.0552 - mse: 0.0118 - val_loss: 0.0699 - val_accuracy: 0.4808 - val_mae: 0.0699 - val_mse: 0.0214\n",
            "Epoch 116/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0578 - accuracy: 0.4062 - mae: 0.0578 - mse: 0.0106\n",
            "Epoch 00116: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0549 - accuracy: 0.4765 - mae: 0.0549 - mse: 0.0117 - val_loss: 0.0696 - val_accuracy: 0.4808 - val_mae: 0.0696 - val_mse: 0.0213\n",
            "Epoch 117/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0539 - accuracy: 0.4688 - mae: 0.0539 - mse: 0.0135\n",
            "Epoch 00117: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0547 - accuracy: 0.4765 - mae: 0.0547 - mse: 0.0116 - val_loss: 0.0694 - val_accuracy: 0.4808 - val_mae: 0.0694 - val_mse: 0.0214\n",
            "Epoch 118/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0547 - accuracy: 0.5625 - mae: 0.0547 - mse: 0.0127\n",
            "Epoch 00118: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0540 - accuracy: 0.4765 - mae: 0.0540 - mse: 0.0115 - val_loss: 0.0689 - val_accuracy: 0.4808 - val_mae: 0.0689 - val_mse: 0.0213\n",
            "Epoch 119/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0533 - accuracy: 0.3438 - mae: 0.0533 - mse: 0.0099\n",
            "Epoch 00119: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0537 - accuracy: 0.4765 - mae: 0.0537 - mse: 0.0114 - val_loss: 0.0688 - val_accuracy: 0.4808 - val_mae: 0.0688 - val_mse: 0.0213\n",
            "Epoch 120/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0435 - accuracy: 0.3438 - mae: 0.0435 - mse: 0.0056\n",
            "Epoch 00120: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0533 - accuracy: 0.4765 - mae: 0.0533 - mse: 0.0113 - val_loss: 0.0684 - val_accuracy: 0.4808 - val_mae: 0.0684 - val_mse: 0.0212\n",
            "Epoch 121/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0315 - accuracy: 0.4375 - mae: 0.0315 - mse: 0.0043\n",
            "Epoch 00121: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0530 - accuracy: 0.4765 - mae: 0.0530 - mse: 0.0112 - val_loss: 0.0680 - val_accuracy: 0.4808 - val_mae: 0.0680 - val_mse: 0.0211\n",
            "Epoch 122/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0385 - accuracy: 0.5625 - mae: 0.0385 - mse: 0.0022\n",
            "Epoch 00122: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0527 - accuracy: 0.4765 - mae: 0.0527 - mse: 0.0112 - val_loss: 0.0680 - val_accuracy: 0.4808 - val_mae: 0.0680 - val_mse: 0.0211\n",
            "Epoch 123/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0495 - accuracy: 0.4688 - mae: 0.0495 - mse: 0.0091\n",
            "Epoch 00123: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0524 - accuracy: 0.4765 - mae: 0.0524 - mse: 0.0111 - val_loss: 0.0677 - val_accuracy: 0.4808 - val_mae: 0.0677 - val_mse: 0.0210\n",
            "Epoch 124/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0289 - accuracy: 0.6250 - mae: 0.0289 - mse: 0.0015\n",
            "Epoch 00124: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0522 - accuracy: 0.4765 - mae: 0.0522 - mse: 0.0110 - val_loss: 0.0669 - val_accuracy: 0.4808 - val_mae: 0.0669 - val_mse: 0.0206\n",
            "Epoch 125/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0433 - accuracy: 0.4375 - mae: 0.0433 - mse: 0.0077\n",
            "Epoch 00125: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0517 - accuracy: 0.4765 - mae: 0.0517 - mse: 0.0109 - val_loss: 0.0667 - val_accuracy: 0.4808 - val_mae: 0.0667 - val_mse: 0.0207\n",
            "Epoch 126/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0378 - accuracy: 0.5625 - mae: 0.0378 - mse: 0.0053\n",
            "Epoch 00126: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0513 - accuracy: 0.4765 - mae: 0.0513 - mse: 0.0108 - val_loss: 0.0663 - val_accuracy: 0.4808 - val_mae: 0.0663 - val_mse: 0.0203\n",
            "Epoch 127/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0928 - accuracy: 0.4062 - mae: 0.0928 - mse: 0.0308\n",
            "Epoch 00127: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0512 - accuracy: 0.4765 - mae: 0.0512 - mse: 0.0107 - val_loss: 0.0660 - val_accuracy: 0.4808 - val_mae: 0.0660 - val_mse: 0.0202\n",
            "Epoch 128/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0599 - accuracy: 0.5000 - mae: 0.0599 - mse: 0.0099\n",
            "Epoch 00128: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0508 - accuracy: 0.4765 - mae: 0.0508 - mse: 0.0106 - val_loss: 0.0655 - val_accuracy: 0.4808 - val_mae: 0.0655 - val_mse: 0.0203\n",
            "Epoch 129/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0636 - accuracy: 0.5312 - mae: 0.0636 - mse: 0.0195\n",
            "Epoch 00129: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0502 - accuracy: 0.4765 - mae: 0.0502 - mse: 0.0105 - val_loss: 0.0659 - val_accuracy: 0.4808 - val_mae: 0.0659 - val_mse: 0.0205\n",
            "Epoch 130/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0654 - accuracy: 0.5312 - mae: 0.0654 - mse: 0.0125\n",
            "Epoch 00130: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0500 - accuracy: 0.4765 - mae: 0.0500 - mse: 0.0105 - val_loss: 0.0657 - val_accuracy: 0.4808 - val_mae: 0.0657 - val_mse: 0.0204\n",
            "Epoch 131/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0451 - accuracy: 0.4375 - mae: 0.0451 - mse: 0.0072\n",
            "Epoch 00131: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0495 - accuracy: 0.4765 - mae: 0.0495 - mse: 0.0104 - val_loss: 0.0646 - val_accuracy: 0.4808 - val_mae: 0.0646 - val_mse: 0.0200\n",
            "Epoch 132/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0542 - accuracy: 0.5312 - mae: 0.0542 - mse: 0.0181\n",
            "Epoch 00132: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0493 - accuracy: 0.4765 - mae: 0.0493 - mse: 0.0102 - val_loss: 0.0649 - val_accuracy: 0.4808 - val_mae: 0.0649 - val_mse: 0.0202\n",
            "Epoch 133/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0567 - accuracy: 0.4375 - mae: 0.0567 - mse: 0.0098\n",
            "Epoch 00133: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0490 - accuracy: 0.4765 - mae: 0.0490 - mse: 0.0102 - val_loss: 0.0647 - val_accuracy: 0.4808 - val_mae: 0.0647 - val_mse: 0.0202\n",
            "Epoch 134/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0481 - accuracy: 0.3750 - mae: 0.0481 - mse: 0.0103\n",
            "Epoch 00134: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0487 - accuracy: 0.4765 - mae: 0.0487 - mse: 0.0101 - val_loss: 0.0637 - val_accuracy: 0.4808 - val_mae: 0.0637 - val_mse: 0.0199\n",
            "Epoch 135/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.1117 - accuracy: 0.4375 - mae: 0.1117 - mse: 0.0289\n",
            "Epoch 00135: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0485 - accuracy: 0.4765 - mae: 0.0485 - mse: 0.0101 - val_loss: 0.0633 - val_accuracy: 0.4808 - val_mae: 0.0633 - val_mse: 0.0198\n",
            "Epoch 136/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0638 - accuracy: 0.4688 - mae: 0.0638 - mse: 0.0234\n",
            "Epoch 00136: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0480 - accuracy: 0.4765 - mae: 0.0480 - mse: 0.0100 - val_loss: 0.0635 - val_accuracy: 0.4808 - val_mae: 0.0635 - val_mse: 0.0199\n",
            "Epoch 137/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0377 - accuracy: 0.4375 - mae: 0.0377 - mse: 0.0042\n",
            "Epoch 00137: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0477 - accuracy: 0.4765 - mae: 0.0477 - mse: 0.0099 - val_loss: 0.0629 - val_accuracy: 0.4808 - val_mae: 0.0629 - val_mse: 0.0197\n",
            "Epoch 138/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0637 - accuracy: 0.4688 - mae: 0.0637 - mse: 0.0163\n",
            "Epoch 00138: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0473 - accuracy: 0.4765 - mae: 0.0473 - mse: 0.0098 - val_loss: 0.0633 - val_accuracy: 0.4808 - val_mae: 0.0633 - val_mse: 0.0198\n",
            "Epoch 139/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0418 - accuracy: 0.4062 - mae: 0.0418 - mse: 0.0075\n",
            "Epoch 00139: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0472 - accuracy: 0.4765 - mae: 0.0472 - mse: 0.0098 - val_loss: 0.0622 - val_accuracy: 0.4808 - val_mae: 0.0622 - val_mse: 0.0195\n",
            "Epoch 140/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0167 - accuracy: 0.4375 - mae: 0.0167 - mse: 4.2946e-04\n",
            "Epoch 00140: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0467 - accuracy: 0.4765 - mae: 0.0467 - mse: 0.0097 - val_loss: 0.0627 - val_accuracy: 0.4808 - val_mae: 0.0627 - val_mse: 0.0197\n",
            "Epoch 141/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0376 - accuracy: 0.4375 - mae: 0.0376 - mse: 0.0069\n",
            "Epoch 00141: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0465 - accuracy: 0.4765 - mae: 0.0465 - mse: 0.0096 - val_loss: 0.0621 - val_accuracy: 0.4808 - val_mae: 0.0621 - val_mse: 0.0195\n",
            "Epoch 142/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0357 - accuracy: 0.4688 - mae: 0.0357 - mse: 0.0062\n",
            "Epoch 00142: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0462 - accuracy: 0.4765 - mae: 0.0462 - mse: 0.0095 - val_loss: 0.0612 - val_accuracy: 0.4808 - val_mae: 0.0612 - val_mse: 0.0193\n",
            "Epoch 143/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0519 - accuracy: 0.5625 - mae: 0.0519 - mse: 0.0108\n",
            "Epoch 00143: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0458 - accuracy: 0.4765 - mae: 0.0458 - mse: 0.0095 - val_loss: 0.0609 - val_accuracy: 0.4808 - val_mae: 0.0609 - val_mse: 0.0192\n",
            "Epoch 144/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0334 - accuracy: 0.3750 - mae: 0.0334 - mse: 0.0035\n",
            "Epoch 00144: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0454 - accuracy: 0.4765 - mae: 0.0454 - mse: 0.0094 - val_loss: 0.0606 - val_accuracy: 0.4808 - val_mae: 0.0606 - val_mse: 0.0190\n",
            "Epoch 145/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0401 - accuracy: 0.5312 - mae: 0.0401 - mse: 0.0151\n",
            "Epoch 00145: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0453 - accuracy: 0.4765 - mae: 0.0453 - mse: 0.0093 - val_loss: 0.0610 - val_accuracy: 0.4808 - val_mae: 0.0610 - val_mse: 0.0192\n",
            "Epoch 146/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0355 - accuracy: 0.3438 - mae: 0.0355 - mse: 0.0064\n",
            "Epoch 00146: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0449 - accuracy: 0.4765 - mae: 0.0449 - mse: 0.0092 - val_loss: 0.0611 - val_accuracy: 0.4808 - val_mae: 0.0611 - val_mse: 0.0192\n",
            "Epoch 147/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0535 - accuracy: 0.5000 - mae: 0.0535 - mse: 0.0125\n",
            "Epoch 00147: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0447 - accuracy: 0.4765 - mae: 0.0447 - mse: 0.0092 - val_loss: 0.0600 - val_accuracy: 0.4808 - val_mae: 0.0600 - val_mse: 0.0190\n",
            "Epoch 148/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0417 - accuracy: 0.4062 - mae: 0.0417 - mse: 0.0082\n",
            "Epoch 00148: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0443 - accuracy: 0.4765 - mae: 0.0443 - mse: 0.0091 - val_loss: 0.0594 - val_accuracy: 0.4808 - val_mae: 0.0594 - val_mse: 0.0187\n",
            "Epoch 149/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0406 - accuracy: 0.3750 - mae: 0.0406 - mse: 0.0077\n",
            "Epoch 00149: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0439 - accuracy: 0.4765 - mae: 0.0439 - mse: 0.0090 - val_loss: 0.0597 - val_accuracy: 0.4808 - val_mae: 0.0597 - val_mse: 0.0189\n",
            "Epoch 150/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0508 - accuracy: 0.5000 - mae: 0.0508 - mse: 0.0091\n",
            "Epoch 00150: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0438 - accuracy: 0.4765 - mae: 0.0438 - mse: 0.0090 - val_loss: 0.0591 - val_accuracy: 0.4808 - val_mae: 0.0591 - val_mse: 0.0188\n",
            "Epoch 151/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0313 - accuracy: 0.5625 - mae: 0.0313 - mse: 0.0053\n",
            "Epoch 00151: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0434 - accuracy: 0.4765 - mae: 0.0434 - mse: 0.0090 - val_loss: 0.0587 - val_accuracy: 0.4808 - val_mae: 0.0587 - val_mse: 0.0186\n",
            "Epoch 152/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0452 - accuracy: 0.5000 - mae: 0.0452 - mse: 0.0072\n",
            "Epoch 00152: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0431 - accuracy: 0.4765 - mae: 0.0431 - mse: 0.0088 - val_loss: 0.0586 - val_accuracy: 0.4808 - val_mae: 0.0586 - val_mse: 0.0186\n",
            "Epoch 153/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0440 - accuracy: 0.4688 - mae: 0.0440 - mse: 0.0088\n",
            "Epoch 00153: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0430 - accuracy: 0.4765 - mae: 0.0430 - mse: 0.0088 - val_loss: 0.0587 - val_accuracy: 0.4808 - val_mae: 0.0587 - val_mse: 0.0186\n",
            "Epoch 154/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0416 - accuracy: 0.4375 - mae: 0.0416 - mse: 0.0093\n",
            "Epoch 00154: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0428 - accuracy: 0.4765 - mae: 0.0428 - mse: 0.0087 - val_loss: 0.0581 - val_accuracy: 0.4808 - val_mae: 0.0581 - val_mse: 0.0182\n",
            "Epoch 155/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0506 - accuracy: 0.4375 - mae: 0.0506 - mse: 0.0119\n",
            "Epoch 00155: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0424 - accuracy: 0.4765 - mae: 0.0424 - mse: 0.0086 - val_loss: 0.0579 - val_accuracy: 0.4808 - val_mae: 0.0579 - val_mse: 0.0181\n",
            "Epoch 156/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0197 - accuracy: 0.5312 - mae: 0.0197 - mse: 6.3451e-04\n",
            "Epoch 00156: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0423 - accuracy: 0.4765 - mae: 0.0423 - mse: 0.0086 - val_loss: 0.0575 - val_accuracy: 0.4808 - val_mae: 0.0575 - val_mse: 0.0183\n",
            "Epoch 157/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0524 - accuracy: 0.5312 - mae: 0.0524 - mse: 0.0119\n",
            "Epoch 00157: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0419 - accuracy: 0.4765 - mae: 0.0419 - mse: 0.0085 - val_loss: 0.0572 - val_accuracy: 0.4808 - val_mae: 0.0572 - val_mse: 0.0181\n",
            "Epoch 158/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0428 - accuracy: 0.5938 - mae: 0.0428 - mse: 0.0078\n",
            "Epoch 00158: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0417 - accuracy: 0.4765 - mae: 0.0417 - mse: 0.0085 - val_loss: 0.0569 - val_accuracy: 0.4808 - val_mae: 0.0569 - val_mse: 0.0181\n",
            "Epoch 159/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0290 - accuracy: 0.5312 - mae: 0.0290 - mse: 0.0036\n",
            "Epoch 00159: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0413 - accuracy: 0.4765 - mae: 0.0413 - mse: 0.0085 - val_loss: 0.0567 - val_accuracy: 0.4808 - val_mae: 0.0567 - val_mse: 0.0179\n",
            "Epoch 160/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0375 - accuracy: 0.4688 - mae: 0.0375 - mse: 0.0065\n",
            "Epoch 00160: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0412 - accuracy: 0.4765 - mae: 0.0412 - mse: 0.0084 - val_loss: 0.0567 - val_accuracy: 0.4808 - val_mae: 0.0567 - val_mse: 0.0178\n",
            "Epoch 161/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0470 - accuracy: 0.4062 - mae: 0.0470 - mse: 0.0095\n",
            "Epoch 00161: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0408 - accuracy: 0.4765 - mae: 0.0408 - mse: 0.0083 - val_loss: 0.0562 - val_accuracy: 0.4808 - val_mae: 0.0562 - val_mse: 0.0180\n",
            "Epoch 162/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0360 - accuracy: 0.5625 - mae: 0.0360 - mse: 0.0067\n",
            "Epoch 00162: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0409 - accuracy: 0.4765 - mae: 0.0409 - mse: 0.0083 - val_loss: 0.0561 - val_accuracy: 0.4808 - val_mae: 0.0561 - val_mse: 0.0179\n",
            "Epoch 163/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0612 - accuracy: 0.4062 - mae: 0.0612 - mse: 0.0113\n",
            "Epoch 00163: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0404 - accuracy: 0.4765 - mae: 0.0404 - mse: 0.0082 - val_loss: 0.0559 - val_accuracy: 0.4808 - val_mae: 0.0559 - val_mse: 0.0179\n",
            "Epoch 164/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0274 - accuracy: 0.3125 - mae: 0.0274 - mse: 0.0044\n",
            "Epoch 00164: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0400 - accuracy: 0.4765 - mae: 0.0400 - mse: 0.0081 - val_loss: 0.0558 - val_accuracy: 0.4808 - val_mae: 0.0558 - val_mse: 0.0175\n",
            "Epoch 165/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0533 - accuracy: 0.2812 - mae: 0.0533 - mse: 0.0130\n",
            "Epoch 00165: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0401 - accuracy: 0.4765 - mae: 0.0401 - mse: 0.0081 - val_loss: 0.0553 - val_accuracy: 0.4808 - val_mae: 0.0553 - val_mse: 0.0176\n",
            "Epoch 166/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0449 - accuracy: 0.5625 - mae: 0.0449 - mse: 0.0083\n",
            "Epoch 00166: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0397 - accuracy: 0.4765 - mae: 0.0397 - mse: 0.0080 - val_loss: 0.0553 - val_accuracy: 0.4808 - val_mae: 0.0553 - val_mse: 0.0175\n",
            "Epoch 167/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0610 - accuracy: 0.3438 - mae: 0.0610 - mse: 0.0131\n",
            "Epoch 00167: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0393 - accuracy: 0.4765 - mae: 0.0393 - mse: 0.0080 - val_loss: 0.0555 - val_accuracy: 0.4808 - val_mae: 0.0555 - val_mse: 0.0178\n",
            "Epoch 168/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0274 - accuracy: 0.3750 - mae: 0.0274 - mse: 0.0025\n",
            "Epoch 00168: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0392 - accuracy: 0.4765 - mae: 0.0392 - mse: 0.0080 - val_loss: 0.0552 - val_accuracy: 0.4808 - val_mae: 0.0552 - val_mse: 0.0176\n",
            "Epoch 169/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0188 - accuracy: 0.4375 - mae: 0.0188 - mse: 9.1510e-04\n",
            "Epoch 00169: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0390 - accuracy: 0.4765 - mae: 0.0390 - mse: 0.0079 - val_loss: 0.0546 - val_accuracy: 0.4808 - val_mae: 0.0546 - val_mse: 0.0174\n",
            "Epoch 170/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0367 - accuracy: 0.5625 - mae: 0.0367 - mse: 0.0063\n",
            "Epoch 00170: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0388 - accuracy: 0.4765 - mae: 0.0388 - mse: 0.0078 - val_loss: 0.0563 - val_accuracy: 0.4808 - val_mae: 0.0563 - val_mse: 0.0178\n",
            "Epoch 171/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0278 - accuracy: 0.5000 - mae: 0.0278 - mse: 0.0037\n",
            "Epoch 00171: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0388 - accuracy: 0.4765 - mae: 0.0388 - mse: 0.0078 - val_loss: 0.0543 - val_accuracy: 0.4808 - val_mae: 0.0543 - val_mse: 0.0173\n",
            "Epoch 172/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0439 - accuracy: 0.3750 - mae: 0.0439 - mse: 0.0080\n",
            "Epoch 00172: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0382 - accuracy: 0.4765 - mae: 0.0382 - mse: 0.0077 - val_loss: 0.0540 - val_accuracy: 0.4808 - val_mae: 0.0540 - val_mse: 0.0172\n",
            "Epoch 173/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0196 - accuracy: 0.4062 - mae: 0.0196 - mse: 0.0019\n",
            "Epoch 00173: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0381 - accuracy: 0.4765 - mae: 0.0381 - mse: 0.0077 - val_loss: 0.0540 - val_accuracy: 0.4808 - val_mae: 0.0540 - val_mse: 0.0173\n",
            "Epoch 174/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0536 - accuracy: 0.3750 - mae: 0.0536 - mse: 0.0110\n",
            "Epoch 00174: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0380 - accuracy: 0.4765 - mae: 0.0380 - mse: 0.0077 - val_loss: 0.0555 - val_accuracy: 0.4808 - val_mae: 0.0555 - val_mse: 0.0175\n",
            "Epoch 175/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0491 - accuracy: 0.3750 - mae: 0.0491 - mse: 0.0102\n",
            "Epoch 00175: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0380 - accuracy: 0.4765 - mae: 0.0380 - mse: 0.0076 - val_loss: 0.0536 - val_accuracy: 0.4808 - val_mae: 0.0536 - val_mse: 0.0171\n",
            "Epoch 176/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0281 - accuracy: 0.5312 - mae: 0.0281 - mse: 0.0037\n",
            "Epoch 00176: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0375 - accuracy: 0.4765 - mae: 0.0375 - mse: 0.0076 - val_loss: 0.0534 - val_accuracy: 0.4808 - val_mae: 0.0534 - val_mse: 0.0170\n",
            "Epoch 177/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0178 - accuracy: 0.4375 - mae: 0.0178 - mse: 0.0010\n",
            "Epoch 00177: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0374 - accuracy: 0.4765 - mae: 0.0374 - mse: 0.0075 - val_loss: 0.0535 - val_accuracy: 0.4808 - val_mae: 0.0535 - val_mse: 0.0171\n",
            "Epoch 178/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0374 - accuracy: 0.3750 - mae: 0.0374 - mse: 0.0071\n",
            "Epoch 00178: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0372 - accuracy: 0.4765 - mae: 0.0372 - mse: 0.0075 - val_loss: 0.0530 - val_accuracy: 0.4808 - val_mae: 0.0530 - val_mse: 0.0169\n",
            "Epoch 179/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0426 - accuracy: 0.4688 - mae: 0.0426 - mse: 0.0085\n",
            "Epoch 00179: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0370 - accuracy: 0.4765 - mae: 0.0370 - mse: 0.0074 - val_loss: 0.0538 - val_accuracy: 0.4808 - val_mae: 0.0538 - val_mse: 0.0171\n",
            "Epoch 180/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0582 - accuracy: 0.5312 - mae: 0.0582 - mse: 0.0117\n",
            "Epoch 00180: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0368 - accuracy: 0.4765 - mae: 0.0368 - mse: 0.0074 - val_loss: 0.0527 - val_accuracy: 0.4808 - val_mae: 0.0527 - val_mse: 0.0168\n",
            "Epoch 181/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0477 - accuracy: 0.5938 - mae: 0.0477 - mse: 0.0102\n",
            "Epoch 00181: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0367 - accuracy: 0.4765 - mae: 0.0367 - mse: 0.0073 - val_loss: 0.0534 - val_accuracy: 0.4808 - val_mae: 0.0534 - val_mse: 0.0170\n",
            "Epoch 182/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0426 - accuracy: 0.5625 - mae: 0.0426 - mse: 0.0082\n",
            "Epoch 00182: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0365 - accuracy: 0.4765 - mae: 0.0365 - mse: 0.0073 - val_loss: 0.0523 - val_accuracy: 0.4808 - val_mae: 0.0523 - val_mse: 0.0167\n",
            "Epoch 183/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0540 - accuracy: 0.5000 - mae: 0.0540 - mse: 0.0115\n",
            "Epoch 00183: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0363 - accuracy: 0.4765 - mae: 0.0363 - mse: 0.0072 - val_loss: 0.0522 - val_accuracy: 0.4808 - val_mae: 0.0522 - val_mse: 0.0167\n",
            "Epoch 184/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0466 - accuracy: 0.4688 - mae: 0.0466 - mse: 0.0102\n",
            "Epoch 00184: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0362 - accuracy: 0.4765 - mae: 0.0362 - mse: 0.0072 - val_loss: 0.0521 - val_accuracy: 0.4808 - val_mae: 0.0521 - val_mse: 0.0165\n",
            "Epoch 185/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0367 - accuracy: 0.5000 - mae: 0.0367 - mse: 0.0047\n",
            "Epoch 00185: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0359 - accuracy: 0.4765 - mae: 0.0359 - mse: 0.0071 - val_loss: 0.0521 - val_accuracy: 0.4808 - val_mae: 0.0521 - val_mse: 0.0164\n",
            "Epoch 186/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0272 - accuracy: 0.5625 - mae: 0.0272 - mse: 0.0039\n",
            "Epoch 00186: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0360 - accuracy: 0.4765 - mae: 0.0360 - mse: 0.0071 - val_loss: 0.0521 - val_accuracy: 0.4808 - val_mae: 0.0521 - val_mse: 0.0163\n",
            "Epoch 187/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0288 - accuracy: 0.5938 - mae: 0.0288 - mse: 0.0044\n",
            "Epoch 00187: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0357 - accuracy: 0.4765 - mae: 0.0357 - mse: 0.0071 - val_loss: 0.0519 - val_accuracy: 0.4808 - val_mae: 0.0519 - val_mse: 0.0166\n",
            "Epoch 188/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0360 - accuracy: 0.4688 - mae: 0.0360 - mse: 0.0062\n",
            "Epoch 00188: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0355 - accuracy: 0.4765 - mae: 0.0355 - mse: 0.0070 - val_loss: 0.0527 - val_accuracy: 0.4808 - val_mae: 0.0527 - val_mse: 0.0167\n",
            "Epoch 189/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0434 - accuracy: 0.5312 - mae: 0.0434 - mse: 0.0080\n",
            "Epoch 00189: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0355 - accuracy: 0.4765 - mae: 0.0355 - mse: 0.0070 - val_loss: 0.0514 - val_accuracy: 0.4808 - val_mae: 0.0514 - val_mse: 0.0164\n",
            "Epoch 190/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0717 - accuracy: 0.4688 - mae: 0.0717 - mse: 0.0259\n",
            "Epoch 00190: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0354 - accuracy: 0.4765 - mae: 0.0354 - mse: 0.0069 - val_loss: 0.0514 - val_accuracy: 0.4808 - val_mae: 0.0514 - val_mse: 0.0164\n",
            "Epoch 191/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0389 - accuracy: 0.5000 - mae: 0.0389 - mse: 0.0074\n",
            "Epoch 00191: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0349 - accuracy: 0.4765 - mae: 0.0349 - mse: 0.0069 - val_loss: 0.0525 - val_accuracy: 0.4808 - val_mae: 0.0525 - val_mse: 0.0165\n",
            "Epoch 192/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0376 - accuracy: 0.5312 - mae: 0.0376 - mse: 0.0067\n",
            "Epoch 00192: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0351 - accuracy: 0.4765 - mae: 0.0351 - mse: 0.0068 - val_loss: 0.0518 - val_accuracy: 0.4808 - val_mae: 0.0518 - val_mse: 0.0164\n",
            "Epoch 193/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0337 - accuracy: 0.5312 - mae: 0.0337 - mse: 0.0055\n",
            "Epoch 00193: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0347 - accuracy: 0.4765 - mae: 0.0347 - mse: 0.0068 - val_loss: 0.0506 - val_accuracy: 0.4808 - val_mae: 0.0506 - val_mse: 0.0161\n",
            "Epoch 194/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0594 - accuracy: 0.4375 - mae: 0.0594 - mse: 0.0212\n",
            "Epoch 00194: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0346 - accuracy: 0.4765 - mae: 0.0346 - mse: 0.0067 - val_loss: 0.0509 - val_accuracy: 0.4808 - val_mae: 0.0509 - val_mse: 0.0162\n",
            "Epoch 195/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0362 - accuracy: 0.3438 - mae: 0.0362 - mse: 0.0066\n",
            "Epoch 00195: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0345 - accuracy: 0.4765 - mae: 0.0345 - mse: 0.0067 - val_loss: 0.0504 - val_accuracy: 0.4808 - val_mae: 0.0504 - val_mse: 0.0160\n",
            "Epoch 196/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0249 - accuracy: 0.6562 - mae: 0.0249 - mse: 0.0031\n",
            "Epoch 00196: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0342 - accuracy: 0.4765 - mae: 0.0342 - mse: 0.0067 - val_loss: 0.0502 - val_accuracy: 0.4808 - val_mae: 0.0502 - val_mse: 0.0160\n",
            "Epoch 197/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0320 - accuracy: 0.4688 - mae: 0.0320 - mse: 0.0057\n",
            "Epoch 00197: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0343 - accuracy: 0.4765 - mae: 0.0343 - mse: 0.0066 - val_loss: 0.0501 - val_accuracy: 0.4808 - val_mae: 0.0501 - val_mse: 0.0159\n",
            "Epoch 198/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0308 - accuracy: 0.4375 - mae: 0.0308 - mse: 0.0147\n",
            "Epoch 00198: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0340 - accuracy: 0.4765 - mae: 0.0340 - mse: 0.0066 - val_loss: 0.0501 - val_accuracy: 0.4808 - val_mae: 0.0501 - val_mse: 0.0160\n",
            "Epoch 199/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0343 - accuracy: 0.5000 - mae: 0.0343 - mse: 0.0043\n",
            "Epoch 00199: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0339 - accuracy: 0.4765 - mae: 0.0339 - mse: 0.0066 - val_loss: 0.0500 - val_accuracy: 0.4808 - val_mae: 0.0500 - val_mse: 0.0159\n",
            "Epoch 200/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0396 - accuracy: 0.3750 - mae: 0.0396 - mse: 0.0066\n",
            "Epoch 00200: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0338 - accuracy: 0.4765 - mae: 0.0338 - mse: 0.0065 - val_loss: 0.0501 - val_accuracy: 0.4808 - val_mae: 0.0501 - val_mse: 0.0159\n",
            "Epoch 201/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0372 - accuracy: 0.4688 - mae: 0.0372 - mse: 0.0169\n",
            "Epoch 00201: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0335 - accuracy: 0.4765 - mae: 0.0335 - mse: 0.0065 - val_loss: 0.0511 - val_accuracy: 0.4808 - val_mae: 0.0511 - val_mse: 0.0161\n",
            "Epoch 202/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0480 - accuracy: 0.4375 - mae: 0.0480 - mse: 0.0089\n",
            "Epoch 00202: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0337 - accuracy: 0.4765 - mae: 0.0337 - mse: 0.0065 - val_loss: 0.0493 - val_accuracy: 0.4808 - val_mae: 0.0493 - val_mse: 0.0156\n",
            "Epoch 203/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0339 - accuracy: 0.5312 - mae: 0.0339 - mse: 0.0067\n",
            "Epoch 00203: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0333 - accuracy: 0.4765 - mae: 0.0333 - mse: 0.0064 - val_loss: 0.0497 - val_accuracy: 0.4808 - val_mae: 0.0497 - val_mse: 0.0158\n",
            "Epoch 204/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0190 - accuracy: 0.3750 - mae: 0.0190 - mse: 0.0011\n",
            "Epoch 00204: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0334 - accuracy: 0.4765 - mae: 0.0334 - mse: 0.0064 - val_loss: 0.0490 - val_accuracy: 0.4808 - val_mae: 0.0490 - val_mse: 0.0156\n",
            "Epoch 205/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0470 - accuracy: 0.5000 - mae: 0.0470 - mse: 0.0085\n",
            "Epoch 00205: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0330 - accuracy: 0.4765 - mae: 0.0330 - mse: 0.0063 - val_loss: 0.0496 - val_accuracy: 0.4808 - val_mae: 0.0496 - val_mse: 0.0154\n",
            "Epoch 206/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0315 - accuracy: 0.4375 - mae: 0.0315 - mse: 0.0045\n",
            "Epoch 00206: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0329 - accuracy: 0.4765 - mae: 0.0329 - mse: 0.0063 - val_loss: 0.0488 - val_accuracy: 0.4808 - val_mae: 0.0488 - val_mse: 0.0155\n",
            "Epoch 207/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0189 - accuracy: 0.5625 - mae: 0.0189 - mse: 0.0021\n",
            "Epoch 00207: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0327 - accuracy: 0.4765 - mae: 0.0327 - mse: 0.0063 - val_loss: 0.0488 - val_accuracy: 0.4808 - val_mae: 0.0488 - val_mse: 0.0155\n",
            "Epoch 208/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0374 - accuracy: 0.4062 - mae: 0.0374 - mse: 0.0066\n",
            "Epoch 00208: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0326 - accuracy: 0.4765 - mae: 0.0326 - mse: 0.0062 - val_loss: 0.0487 - val_accuracy: 0.4808 - val_mae: 0.0487 - val_mse: 0.0155\n",
            "Epoch 209/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0285 - accuracy: 0.5000 - mae: 0.0285 - mse: 0.0045\n",
            "Epoch 00209: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0326 - accuracy: 0.4765 - mae: 0.0326 - mse: 0.0062 - val_loss: 0.0483 - val_accuracy: 0.4808 - val_mae: 0.0483 - val_mse: 0.0154\n",
            "Epoch 210/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0264 - accuracy: 0.5000 - mae: 0.0264 - mse: 0.0039\n",
            "Epoch 00210: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0324 - accuracy: 0.4765 - mae: 0.0324 - mse: 0.0061 - val_loss: 0.0483 - val_accuracy: 0.4808 - val_mae: 0.0483 - val_mse: 0.0154\n",
            "Epoch 211/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0258 - accuracy: 0.3750 - mae: 0.0258 - mse: 0.0045\n",
            "Epoch 00211: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0323 - accuracy: 0.4765 - mae: 0.0323 - mse: 0.0061 - val_loss: 0.0488 - val_accuracy: 0.4808 - val_mae: 0.0488 - val_mse: 0.0155\n",
            "Epoch 212/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0451 - accuracy: 0.5000 - mae: 0.0451 - mse: 0.0082\n",
            "Epoch 00212: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0322 - accuracy: 0.4765 - mae: 0.0322 - mse: 0.0061 - val_loss: 0.0479 - val_accuracy: 0.4808 - val_mae: 0.0479 - val_mse: 0.0152\n",
            "Epoch 213/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0500 - accuracy: 0.5000 - mae: 0.0500 - mse: 0.0189\n",
            "Epoch 00213: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0321 - accuracy: 0.4765 - mae: 0.0321 - mse: 0.0060 - val_loss: 0.0479 - val_accuracy: 0.4808 - val_mae: 0.0479 - val_mse: 0.0152\n",
            "Epoch 214/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0392 - accuracy: 0.5312 - mae: 0.0392 - mse: 0.0065\n",
            "Epoch 00214: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0319 - accuracy: 0.4765 - mae: 0.0319 - mse: 0.0060 - val_loss: 0.0482 - val_accuracy: 0.4808 - val_mae: 0.0482 - val_mse: 0.0153\n",
            "Epoch 215/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0206 - accuracy: 0.4375 - mae: 0.0206 - mse: 0.0028\n",
            "Epoch 00215: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0318 - accuracy: 0.4765 - mae: 0.0318 - mse: 0.0060 - val_loss: 0.0495 - val_accuracy: 0.4808 - val_mae: 0.0495 - val_mse: 0.0154\n",
            "Epoch 216/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0398 - accuracy: 0.4062 - mae: 0.0398 - mse: 0.0075\n",
            "Epoch 00216: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0319 - accuracy: 0.4765 - mae: 0.0319 - mse: 0.0060 - val_loss: 0.0474 - val_accuracy: 0.4808 - val_mae: 0.0474 - val_mse: 0.0150\n",
            "Epoch 217/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0218 - accuracy: 0.5938 - mae: 0.0218 - mse: 0.0035\n",
            "Epoch 00217: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0315 - accuracy: 0.4765 - mae: 0.0315 - mse: 0.0059 - val_loss: 0.0477 - val_accuracy: 0.4808 - val_mae: 0.0477 - val_mse: 0.0151\n",
            "Epoch 218/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0390 - accuracy: 0.4375 - mae: 0.0390 - mse: 0.0058\n",
            "Epoch 00218: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0316 - accuracy: 0.4765 - mae: 0.0316 - mse: 0.0059 - val_loss: 0.0475 - val_accuracy: 0.4808 - val_mae: 0.0475 - val_mse: 0.0150\n",
            "Epoch 219/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0152 - accuracy: 0.4062 - mae: 0.0152 - mse: 0.0012\n",
            "Epoch 00219: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0315 - accuracy: 0.4765 - mae: 0.0315 - mse: 0.0058 - val_loss: 0.0471 - val_accuracy: 0.4808 - val_mae: 0.0471 - val_mse: 0.0149\n",
            "Epoch 220/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0410 - accuracy: 0.4688 - mae: 0.0410 - mse: 0.0074\n",
            "Epoch 00220: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0312 - accuracy: 0.4765 - mae: 0.0312 - mse: 0.0058 - val_loss: 0.0469 - val_accuracy: 0.4808 - val_mae: 0.0469 - val_mse: 0.0148\n",
            "Epoch 221/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0480 - accuracy: 0.3750 - mae: 0.0480 - mse: 0.0087\n",
            "Epoch 00221: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0312 - accuracy: 0.4765 - mae: 0.0312 - mse: 0.0058 - val_loss: 0.0472 - val_accuracy: 0.4808 - val_mae: 0.0472 - val_mse: 0.0146\n",
            "Epoch 222/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0559 - accuracy: 0.5000 - mae: 0.0559 - mse: 0.0095\n",
            "Epoch 00222: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0311 - accuracy: 0.4765 - mae: 0.0311 - mse: 0.0057 - val_loss: 0.0467 - val_accuracy: 0.4808 - val_mae: 0.0467 - val_mse: 0.0147\n",
            "Epoch 223/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0367 - accuracy: 0.4688 - mae: 0.0367 - mse: 0.0054\n",
            "Epoch 00223: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0309 - accuracy: 0.4765 - mae: 0.0309 - mse: 0.0057 - val_loss: 0.0471 - val_accuracy: 0.4808 - val_mae: 0.0471 - val_mse: 0.0149\n",
            "Epoch 224/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0200 - accuracy: 0.5938 - mae: 0.0200 - mse: 0.0028\n",
            "Epoch 00224: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0309 - accuracy: 0.4765 - mae: 0.0309 - mse: 0.0057 - val_loss: 0.0464 - val_accuracy: 0.4808 - val_mae: 0.0464 - val_mse: 0.0147\n",
            "Epoch 225/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0219 - accuracy: 0.5625 - mae: 0.0219 - mse: 0.0023\n",
            "Epoch 00225: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0308 - accuracy: 0.4765 - mae: 0.0308 - mse: 0.0056 - val_loss: 0.0469 - val_accuracy: 0.4808 - val_mae: 0.0469 - val_mse: 0.0148\n",
            "Epoch 226/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0163 - accuracy: 0.5625 - mae: 0.0163 - mse: 0.0017\n",
            "Epoch 00226: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0307 - accuracy: 0.4765 - mae: 0.0307 - mse: 0.0056 - val_loss: 0.0475 - val_accuracy: 0.4808 - val_mae: 0.0475 - val_mse: 0.0148\n",
            "Epoch 227/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0268 - accuracy: 0.6562 - mae: 0.0268 - mse: 0.0038\n",
            "Epoch 00227: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0307 - accuracy: 0.4765 - mae: 0.0307 - mse: 0.0056 - val_loss: 0.0469 - val_accuracy: 0.4808 - val_mae: 0.0469 - val_mse: 0.0148\n",
            "Epoch 228/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0323 - accuracy: 0.4375 - mae: 0.0323 - mse: 0.0049\n",
            "Epoch 00228: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0305 - accuracy: 0.4765 - mae: 0.0305 - mse: 0.0056 - val_loss: 0.0461 - val_accuracy: 0.4808 - val_mae: 0.0461 - val_mse: 0.0146\n",
            "Epoch 229/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0473 - accuracy: 0.5312 - mae: 0.0473 - mse: 0.0091\n",
            "Epoch 00229: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0305 - accuracy: 0.4765 - mae: 0.0305 - mse: 0.0055 - val_loss: 0.0458 - val_accuracy: 0.4808 - val_mae: 0.0458 - val_mse: 0.0145\n",
            "Epoch 230/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0166 - accuracy: 0.4375 - mae: 0.0166 - mse: 0.0018\n",
            "Epoch 00230: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0302 - accuracy: 0.4765 - mae: 0.0302 - mse: 0.0055 - val_loss: 0.0459 - val_accuracy: 0.4808 - val_mae: 0.0459 - val_mse: 0.0145\n",
            "Epoch 231/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0278 - accuracy: 0.3750 - mae: 0.0278 - mse: 0.0035\n",
            "Epoch 00231: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0301 - accuracy: 0.4765 - mae: 0.0301 - mse: 0.0055 - val_loss: 0.0456 - val_accuracy: 0.4808 - val_mae: 0.0456 - val_mse: 0.0144\n",
            "Epoch 232/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0182 - accuracy: 0.6562 - mae: 0.0182 - mse: 0.0027\n",
            "Epoch 00232: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0301 - accuracy: 0.4765 - mae: 0.0301 - mse: 0.0054 - val_loss: 0.0457 - val_accuracy: 0.4808 - val_mae: 0.0457 - val_mse: 0.0145\n",
            "Epoch 233/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0276 - accuracy: 0.4375 - mae: 0.0276 - mse: 0.0043\n",
            "Epoch 00233: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0301 - accuracy: 0.4765 - mae: 0.0301 - mse: 0.0054 - val_loss: 0.0459 - val_accuracy: 0.4808 - val_mae: 0.0459 - val_mse: 0.0142\n",
            "Epoch 234/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0381 - accuracy: 0.3438 - mae: 0.0381 - mse: 0.0067\n",
            "Epoch 00234: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0300 - accuracy: 0.4765 - mae: 0.0300 - mse: 0.0054 - val_loss: 0.0453 - val_accuracy: 0.4808 - val_mae: 0.0453 - val_mse: 0.0142\n",
            "Epoch 235/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0387 - accuracy: 0.5000 - mae: 0.0387 - mse: 0.0064\n",
            "Epoch 00235: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0298 - accuracy: 0.4765 - mae: 0.0298 - mse: 0.0053 - val_loss: 0.0453 - val_accuracy: 0.4808 - val_mae: 0.0453 - val_mse: 0.0142\n",
            "Epoch 236/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0222 - accuracy: 0.4375 - mae: 0.0222 - mse: 0.0024\n",
            "Epoch 00236: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0298 - accuracy: 0.4765 - mae: 0.0298 - mse: 0.0053 - val_loss: 0.0454 - val_accuracy: 0.4808 - val_mae: 0.0454 - val_mse: 0.0143\n",
            "Epoch 237/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0363 - accuracy: 0.2812 - mae: 0.0363 - mse: 0.0051\n",
            "Epoch 00237: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0295 - accuracy: 0.4765 - mae: 0.0295 - mse: 0.0053 - val_loss: 0.0450 - val_accuracy: 0.4808 - val_mae: 0.0450 - val_mse: 0.0141\n",
            "Epoch 238/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0331 - accuracy: 0.4375 - mae: 0.0331 - mse: 0.0055\n",
            "Epoch 00238: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0295 - accuracy: 0.4765 - mae: 0.0295 - mse: 0.0052 - val_loss: 0.0452 - val_accuracy: 0.4808 - val_mae: 0.0452 - val_mse: 0.0143\n",
            "Epoch 239/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0330 - accuracy: 0.4062 - mae: 0.0330 - mse: 0.0052\n",
            "Epoch 00239: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0294 - accuracy: 0.4765 - mae: 0.0294 - mse: 0.0052 - val_loss: 0.0451 - val_accuracy: 0.4808 - val_mae: 0.0451 - val_mse: 0.0142\n",
            "Epoch 240/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0335 - accuracy: 0.4062 - mae: 0.0335 - mse: 0.0042\n",
            "Epoch 00240: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0294 - accuracy: 0.4765 - mae: 0.0294 - mse: 0.0052 - val_loss: 0.0460 - val_accuracy: 0.4808 - val_mae: 0.0460 - val_mse: 0.0143\n",
            "Epoch 241/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0098 - accuracy: 0.5000 - mae: 0.0098 - mse: 2.2320e-04\n",
            "Epoch 00241: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0294 - accuracy: 0.4765 - mae: 0.0294 - mse: 0.0052 - val_loss: 0.0452 - val_accuracy: 0.4808 - val_mae: 0.0452 - val_mse: 0.0142\n",
            "Epoch 242/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0438 - accuracy: 0.5625 - mae: 0.0438 - mse: 0.0071\n",
            "Epoch 00242: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0291 - accuracy: 0.4765 - mae: 0.0291 - mse: 0.0051 - val_loss: 0.0444 - val_accuracy: 0.4808 - val_mae: 0.0444 - val_mse: 0.0140\n",
            "Epoch 243/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0378 - accuracy: 0.5938 - mae: 0.0378 - mse: 0.0168\n",
            "Epoch 00243: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0289 - accuracy: 0.4765 - mae: 0.0289 - mse: 0.0051 - val_loss: 0.0443 - val_accuracy: 0.4808 - val_mae: 0.0443 - val_mse: 0.0140\n",
            "Epoch 244/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0172 - accuracy: 0.5312 - mae: 0.0172 - mse: 0.0018\n",
            "Epoch 00244: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0290 - accuracy: 0.4765 - mae: 0.0290 - mse: 0.0051 - val_loss: 0.0450 - val_accuracy: 0.4808 - val_mae: 0.0450 - val_mse: 0.0141\n",
            "Epoch 245/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0389 - accuracy: 0.4375 - mae: 0.0389 - mse: 0.0061\n",
            "Epoch 00245: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0289 - accuracy: 0.4765 - mae: 0.0289 - mse: 0.0050 - val_loss: 0.0441 - val_accuracy: 0.4808 - val_mae: 0.0441 - val_mse: 0.0137\n",
            "Epoch 246/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0302 - accuracy: 0.6562 - mae: 0.0302 - mse: 0.0037\n",
            "Epoch 00246: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0289 - accuracy: 0.4765 - mae: 0.0289 - mse: 0.0050 - val_loss: 0.0439 - val_accuracy: 0.4808 - val_mae: 0.0439 - val_mse: 0.0139\n",
            "Epoch 247/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0514 - accuracy: 0.5625 - mae: 0.0514 - mse: 0.0198\n",
            "Epoch 00247: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0286 - accuracy: 0.4765 - mae: 0.0286 - mse: 0.0050 - val_loss: 0.0446 - val_accuracy: 0.4808 - val_mae: 0.0446 - val_mse: 0.0140\n",
            "Epoch 248/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0222 - accuracy: 0.4375 - mae: 0.0222 - mse: 0.0032\n",
            "Epoch 00248: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0285 - accuracy: 0.4765 - mae: 0.0285 - mse: 0.0050 - val_loss: 0.0437 - val_accuracy: 0.4808 - val_mae: 0.0437 - val_mse: 0.0137\n",
            "Epoch 249/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0477 - accuracy: 0.3750 - mae: 0.0477 - mse: 0.0177\n",
            "Epoch 00249: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0284 - accuracy: 0.4765 - mae: 0.0284 - mse: 0.0049 - val_loss: 0.0436 - val_accuracy: 0.4808 - val_mae: 0.0436 - val_mse: 0.0138\n",
            "Epoch 250/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0328 - accuracy: 0.5625 - mae: 0.0328 - mse: 0.0042\n",
            "Epoch 00250: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0284 - accuracy: 0.4765 - mae: 0.0284 - mse: 0.0049 - val_loss: 0.0434 - val_accuracy: 0.4808 - val_mae: 0.0434 - val_mse: 0.0136\n",
            "Epoch 251/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0325 - accuracy: 0.4688 - mae: 0.0325 - mse: 0.0052\n",
            "Epoch 00251: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0283 - accuracy: 0.4765 - mae: 0.0283 - mse: 0.0049 - val_loss: 0.0433 - val_accuracy: 0.4808 - val_mae: 0.0433 - val_mse: 0.0137\n",
            "Epoch 252/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0339 - accuracy: 0.3125 - mae: 0.0339 - mse: 0.0043\n",
            "Epoch 00252: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0281 - accuracy: 0.4765 - mae: 0.0281 - mse: 0.0048 - val_loss: 0.0464 - val_accuracy: 0.4808 - val_mae: 0.0464 - val_mse: 0.0140\n",
            "Epoch 253/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0286 - accuracy: 0.5625 - mae: 0.0286 - mse: 0.0029\n",
            "Epoch 00253: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0283 - accuracy: 0.4765 - mae: 0.0283 - mse: 0.0048 - val_loss: 0.0433 - val_accuracy: 0.4808 - val_mae: 0.0433 - val_mse: 0.0135\n",
            "Epoch 254/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0289 - accuracy: 0.3750 - mae: 0.0289 - mse: 0.0046\n",
            "Epoch 00254: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0280 - accuracy: 0.4765 - mae: 0.0280 - mse: 0.0048 - val_loss: 0.0431 - val_accuracy: 0.4808 - val_mae: 0.0431 - val_mse: 0.0136\n",
            "Epoch 255/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0299 - accuracy: 0.4062 - mae: 0.0299 - mse: 0.0040\n",
            "Epoch 00255: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0278 - accuracy: 0.4765 - mae: 0.0278 - mse: 0.0048 - val_loss: 0.0430 - val_accuracy: 0.4808 - val_mae: 0.0430 - val_mse: 0.0136\n",
            "Epoch 256/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0240 - accuracy: 0.2812 - mae: 0.0240 - mse: 0.0024\n",
            "Epoch 00256: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0279 - accuracy: 0.4765 - mae: 0.0279 - mse: 0.0047 - val_loss: 0.0439 - val_accuracy: 0.4808 - val_mae: 0.0439 - val_mse: 0.0132\n",
            "Epoch 257/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0215 - accuracy: 0.4688 - mae: 0.0215 - mse: 0.0021\n",
            "Epoch 00257: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0277 - accuracy: 0.4765 - mae: 0.0277 - mse: 0.0047 - val_loss: 0.0436 - val_accuracy: 0.4808 - val_mae: 0.0436 - val_mse: 0.0136\n",
            "Epoch 258/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0431 - accuracy: 0.6562 - mae: 0.0431 - mse: 0.0172\n",
            "Epoch 00258: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0276 - accuracy: 0.4765 - mae: 0.0276 - mse: 0.0047 - val_loss: 0.0429 - val_accuracy: 0.4808 - val_mae: 0.0429 - val_mse: 0.0135\n",
            "Epoch 259/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0512 - accuracy: 0.4062 - mae: 0.0512 - mse: 0.0190\n",
            "Epoch 00259: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0276 - accuracy: 0.4765 - mae: 0.0276 - mse: 0.0047 - val_loss: 0.0433 - val_accuracy: 0.4808 - val_mae: 0.0433 - val_mse: 0.0132\n",
            "Epoch 260/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0257 - accuracy: 0.5938 - mae: 0.0257 - mse: 0.0021\n",
            "Epoch 00260: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0274 - accuracy: 0.4765 - mae: 0.0274 - mse: 0.0046 - val_loss: 0.0424 - val_accuracy: 0.4808 - val_mae: 0.0424 - val_mse: 0.0134\n",
            "Epoch 261/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0392 - accuracy: 0.4375 - mae: 0.0392 - mse: 0.0053\n",
            "Epoch 00261: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0273 - accuracy: 0.4765 - mae: 0.0273 - mse: 0.0046 - val_loss: 0.0436 - val_accuracy: 0.4808 - val_mae: 0.0436 - val_mse: 0.0135\n",
            "Epoch 262/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0307 - accuracy: 0.4062 - mae: 0.0307 - mse: 0.0032\n",
            "Epoch 00262: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0276 - accuracy: 0.4765 - mae: 0.0276 - mse: 0.0046 - val_loss: 0.0421 - val_accuracy: 0.4808 - val_mae: 0.0421 - val_mse: 0.0132\n",
            "Epoch 263/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0407 - accuracy: 0.4375 - mae: 0.0407 - mse: 0.0073\n",
            "Epoch 00263: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0271 - accuracy: 0.4765 - mae: 0.0271 - mse: 0.0046 - val_loss: 0.0421 - val_accuracy: 0.4808 - val_mae: 0.0421 - val_mse: 0.0133\n",
            "Epoch 264/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0363 - accuracy: 0.3750 - mae: 0.0363 - mse: 0.0050\n",
            "Epoch 00264: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0270 - accuracy: 0.4765 - mae: 0.0270 - mse: 0.0045 - val_loss: 0.0420 - val_accuracy: 0.4808 - val_mae: 0.0420 - val_mse: 0.0131\n",
            "Epoch 265/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0456 - accuracy: 0.3750 - mae: 0.0456 - mse: 0.0071\n",
            "Epoch 00265: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0271 - accuracy: 0.4765 - mae: 0.0271 - mse: 0.0045 - val_loss: 0.0423 - val_accuracy: 0.4808 - val_mae: 0.0423 - val_mse: 0.0133\n",
            "Epoch 266/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0246 - accuracy: 0.5312 - mae: 0.0246 - mse: 0.0028\n",
            "Epoch 00266: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0269 - accuracy: 0.4765 - mae: 0.0269 - mse: 0.0045 - val_loss: 0.0417 - val_accuracy: 0.4808 - val_mae: 0.0417 - val_mse: 0.0131\n",
            "Epoch 267/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0232 - accuracy: 0.4375 - mae: 0.0232 - mse: 0.0027\n",
            "Epoch 00267: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0268 - accuracy: 0.4765 - mae: 0.0268 - mse: 0.0045 - val_loss: 0.0423 - val_accuracy: 0.4808 - val_mae: 0.0423 - val_mse: 0.0133\n",
            "Epoch 268/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0281 - accuracy: 0.4688 - mae: 0.0281 - mse: 0.0044\n",
            "Epoch 00268: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0269 - accuracy: 0.4765 - mae: 0.0269 - mse: 0.0045 - val_loss: 0.0428 - val_accuracy: 0.4808 - val_mae: 0.0428 - val_mse: 0.0133\n",
            "Epoch 269/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0509 - accuracy: 0.5938 - mae: 0.0509 - mse: 0.0198\n",
            "Epoch 00269: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0266 - accuracy: 0.4765 - mae: 0.0266 - mse: 0.0044 - val_loss: 0.0416 - val_accuracy: 0.4808 - val_mae: 0.0416 - val_mse: 0.0131\n",
            "Epoch 270/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0258 - accuracy: 0.3438 - mae: 0.0258 - mse: 0.0026\n",
            "Epoch 00270: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0269 - accuracy: 0.4765 - mae: 0.0269 - mse: 0.0044 - val_loss: 0.0415 - val_accuracy: 0.4808 - val_mae: 0.0415 - val_mse: 0.0129\n",
            "Epoch 271/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0264 - accuracy: 0.5938 - mae: 0.0264 - mse: 0.0029\n",
            "Epoch 00271: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0264 - accuracy: 0.4765 - mae: 0.0264 - mse: 0.0044 - val_loss: 0.0417 - val_accuracy: 0.4808 - val_mae: 0.0417 - val_mse: 0.0128\n",
            "Epoch 272/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0602 - accuracy: 0.6875 - mae: 0.0602 - mse: 0.0202\n",
            "Epoch 00272: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0264 - accuracy: 0.4765 - mae: 0.0264 - mse: 0.0043 - val_loss: 0.0411 - val_accuracy: 0.4808 - val_mae: 0.0411 - val_mse: 0.0129\n",
            "Epoch 273/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0161 - accuracy: 0.4375 - mae: 0.0161 - mse: 0.0011\n",
            "Epoch 00273: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0262 - accuracy: 0.4765 - mae: 0.0262 - mse: 0.0043 - val_loss: 0.0416 - val_accuracy: 0.4808 - val_mae: 0.0416 - val_mse: 0.0128\n",
            "Epoch 274/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0101 - accuracy: 0.5938 - mae: 0.0101 - mse: 1.4860e-04\n",
            "Epoch 00274: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0263 - accuracy: 0.4765 - mae: 0.0263 - mse: 0.0043 - val_loss: 0.0409 - val_accuracy: 0.4808 - val_mae: 0.0409 - val_mse: 0.0129\n",
            "Epoch 275/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0192 - accuracy: 0.5312 - mae: 0.0192 - mse: 0.0022\n",
            "Epoch 00275: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0260 - accuracy: 0.4765 - mae: 0.0260 - mse: 0.0043 - val_loss: 0.0408 - val_accuracy: 0.4808 - val_mae: 0.0408 - val_mse: 0.0128\n",
            "Epoch 276/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0107 - accuracy: 0.4688 - mae: 0.0107 - mse: 9.6170e-04\n",
            "Epoch 00276: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0260 - accuracy: 0.4765 - mae: 0.0260 - mse: 0.0043 - val_loss: 0.0409 - val_accuracy: 0.4808 - val_mae: 0.0409 - val_mse: 0.0127\n",
            "Epoch 277/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0210 - accuracy: 0.3438 - mae: 0.0210 - mse: 0.0021\n",
            "Epoch 00277: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0259 - accuracy: 0.4765 - mae: 0.0259 - mse: 0.0042 - val_loss: 0.0408 - val_accuracy: 0.4808 - val_mae: 0.0408 - val_mse: 0.0127\n",
            "Epoch 278/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0461 - accuracy: 0.4688 - mae: 0.0461 - mse: 0.0072\n",
            "Epoch 00278: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0259 - accuracy: 0.4765 - mae: 0.0259 - mse: 0.0042 - val_loss: 0.0404 - val_accuracy: 0.4808 - val_mae: 0.0404 - val_mse: 0.0127\n",
            "Epoch 279/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0163 - accuracy: 0.4688 - mae: 0.0163 - mse: 0.0016\n",
            "Epoch 00279: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0257 - accuracy: 0.4765 - mae: 0.0257 - mse: 0.0042 - val_loss: 0.0404 - val_accuracy: 0.4808 - val_mae: 0.0404 - val_mse: 0.0127\n",
            "Epoch 280/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0275 - accuracy: 0.3125 - mae: 0.0275 - mse: 0.0035\n",
            "Epoch 00280: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0256 - accuracy: 0.4765 - mae: 0.0256 - mse: 0.0041 - val_loss: 0.0403 - val_accuracy: 0.4808 - val_mae: 0.0403 - val_mse: 0.0127\n",
            "Epoch 281/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0162 - accuracy: 0.4375 - mae: 0.0162 - mse: 0.0013\n",
            "Epoch 00281: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0255 - accuracy: 0.4765 - mae: 0.0255 - mse: 0.0041 - val_loss: 0.0404 - val_accuracy: 0.4808 - val_mae: 0.0404 - val_mse: 0.0126\n",
            "Epoch 282/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0240 - accuracy: 0.5000 - mae: 0.0240 - mse: 0.0038\n",
            "Epoch 00282: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0255 - accuracy: 0.4765 - mae: 0.0255 - mse: 0.0041 - val_loss: 0.0402 - val_accuracy: 0.4808 - val_mae: 0.0402 - val_mse: 0.0127\n",
            "Epoch 283/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0213 - accuracy: 0.5000 - mae: 0.0213 - mse: 0.0017\n",
            "Epoch 00283: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0253 - accuracy: 0.4765 - mae: 0.0253 - mse: 0.0041 - val_loss: 0.0400 - val_accuracy: 0.4808 - val_mae: 0.0400 - val_mse: 0.0126\n",
            "Epoch 284/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0487 - accuracy: 0.5625 - mae: 0.0487 - mse: 0.0077\n",
            "Epoch 00284: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0253 - accuracy: 0.4765 - mae: 0.0253 - mse: 0.0041 - val_loss: 0.0399 - val_accuracy: 0.4808 - val_mae: 0.0399 - val_mse: 0.0126\n",
            "Epoch 285/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0432 - accuracy: 0.4688 - mae: 0.0432 - mse: 0.0162\n",
            "Epoch 00285: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0253 - accuracy: 0.4765 - mae: 0.0253 - mse: 0.0041 - val_loss: 0.0399 - val_accuracy: 0.4808 - val_mae: 0.0399 - val_mse: 0.0125\n",
            "Epoch 286/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0283 - accuracy: 0.5000 - mae: 0.0283 - mse: 0.0038\n",
            "Epoch 00286: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0251 - accuracy: 0.4765 - mae: 0.0251 - mse: 0.0040 - val_loss: 0.0397 - val_accuracy: 0.4808 - val_mae: 0.0397 - val_mse: 0.0125\n",
            "Epoch 287/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0143 - accuracy: 0.3438 - mae: 0.0143 - mse: 0.0012\n",
            "Epoch 00287: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0250 - accuracy: 0.4765 - mae: 0.0250 - mse: 0.0040 - val_loss: 0.0397 - val_accuracy: 0.4808 - val_mae: 0.0397 - val_mse: 0.0125\n",
            "Epoch 288/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0350 - accuracy: 0.4375 - mae: 0.0350 - mse: 0.0158\n",
            "Epoch 00288: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0250 - accuracy: 0.4765 - mae: 0.0250 - mse: 0.0040 - val_loss: 0.0398 - val_accuracy: 0.4808 - val_mae: 0.0398 - val_mse: 0.0126\n",
            "Epoch 289/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0149 - accuracy: 0.4062 - mae: 0.0149 - mse: 0.0010\n",
            "Epoch 00289: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0250 - accuracy: 0.4765 - mae: 0.0250 - mse: 0.0040 - val_loss: 0.0394 - val_accuracy: 0.4808 - val_mae: 0.0394 - val_mse: 0.0125\n",
            "Epoch 290/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0130 - accuracy: 0.4062 - mae: 0.0130 - mse: 0.0011\n",
            "Epoch 00290: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0248 - accuracy: 0.4765 - mae: 0.0248 - mse: 0.0039 - val_loss: 0.0401 - val_accuracy: 0.4808 - val_mae: 0.0401 - val_mse: 0.0122\n",
            "Epoch 291/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0210 - accuracy: 0.4375 - mae: 0.0210 - mse: 0.0019\n",
            "Epoch 00291: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0248 - accuracy: 0.4765 - mae: 0.0248 - mse: 0.0039 - val_loss: 0.0397 - val_accuracy: 0.4808 - val_mae: 0.0397 - val_mse: 0.0122\n",
            "Epoch 292/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0676 - accuracy: 0.4688 - mae: 0.0676 - mse: 0.0212\n",
            "Epoch 00292: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0247 - accuracy: 0.4765 - mae: 0.0247 - mse: 0.0039 - val_loss: 0.0390 - val_accuracy: 0.4808 - val_mae: 0.0390 - val_mse: 0.0123\n",
            "Epoch 293/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0202 - accuracy: 0.5312 - mae: 0.0202 - mse: 0.0029\n",
            "Epoch 00293: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0246 - accuracy: 0.4765 - mae: 0.0246 - mse: 0.0039 - val_loss: 0.0390 - val_accuracy: 0.4808 - val_mae: 0.0390 - val_mse: 0.0123\n",
            "Epoch 294/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0172 - accuracy: 0.5000 - mae: 0.0172 - mse: 0.0020\n",
            "Epoch 00294: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0245 - accuracy: 0.4765 - mae: 0.0245 - mse: 0.0038 - val_loss: 0.0390 - val_accuracy: 0.4808 - val_mae: 0.0390 - val_mse: 0.0123\n",
            "Epoch 295/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0323 - accuracy: 0.3750 - mae: 0.0323 - mse: 0.0151\n",
            "Epoch 00295: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0244 - accuracy: 0.4765 - mae: 0.0244 - mse: 0.0038 - val_loss: 0.0389 - val_accuracy: 0.4808 - val_mae: 0.0389 - val_mse: 0.0123\n",
            "Epoch 296/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0441 - accuracy: 0.2812 - mae: 0.0441 - mse: 0.0061\n",
            "Epoch 00296: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0243 - accuracy: 0.4765 - mae: 0.0243 - mse: 0.0038 - val_loss: 0.0389 - val_accuracy: 0.4808 - val_mae: 0.0389 - val_mse: 0.0121\n",
            "Epoch 297/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0196 - accuracy: 0.5938 - mae: 0.0196 - mse: 0.0017\n",
            "Epoch 00297: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0243 - accuracy: 0.4765 - mae: 0.0243 - mse: 0.0038 - val_loss: 0.0394 - val_accuracy: 0.4808 - val_mae: 0.0394 - val_mse: 0.0124\n",
            "Epoch 298/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0398 - accuracy: 0.5625 - mae: 0.0398 - mse: 0.0062\n",
            "Epoch 00298: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0241 - accuracy: 0.4765 - mae: 0.0241 - mse: 0.0038 - val_loss: 0.0387 - val_accuracy: 0.4808 - val_mae: 0.0387 - val_mse: 0.0121\n",
            "Epoch 299/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0173 - accuracy: 0.3750 - mae: 0.0173 - mse: 0.0015\n",
            "Epoch 00299: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0239 - accuracy: 0.4765 - mae: 0.0239 - mse: 0.0037 - val_loss: 0.0384 - val_accuracy: 0.4808 - val_mae: 0.0384 - val_mse: 0.0122\n",
            "Epoch 300/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0412 - accuracy: 0.4375 - mae: 0.0412 - mse: 0.0057\n",
            "Epoch 00300: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0239 - accuracy: 0.4765 - mae: 0.0239 - mse: 0.0037 - val_loss: 0.0384 - val_accuracy: 0.4808 - val_mae: 0.0384 - val_mse: 0.0121\n",
            "Epoch 301/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0293 - accuracy: 0.5000 - mae: 0.0293 - mse: 0.0034\n",
            "Epoch 00301: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0238 - accuracy: 0.4765 - mae: 0.0238 - mse: 0.0037 - val_loss: 0.0382 - val_accuracy: 0.4808 - val_mae: 0.0382 - val_mse: 0.0121\n",
            "Epoch 302/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0412 - accuracy: 0.4688 - mae: 0.0412 - mse: 0.0164\n",
            "Epoch 00302: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0237 - accuracy: 0.4765 - mae: 0.0237 - mse: 0.0037 - val_loss: 0.0381 - val_accuracy: 0.4808 - val_mae: 0.0381 - val_mse: 0.0121\n",
            "Epoch 303/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0164 - accuracy: 0.5625 - mae: 0.0164 - mse: 0.0015\n",
            "Epoch 00303: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0236 - accuracy: 0.4765 - mae: 0.0236 - mse: 0.0037 - val_loss: 0.0380 - val_accuracy: 0.4808 - val_mae: 0.0380 - val_mse: 0.0121\n",
            "Epoch 304/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0250 - accuracy: 0.3750 - mae: 0.0250 - mse: 0.0023\n",
            "Epoch 00304: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0237 - accuracy: 0.4765 - mae: 0.0237 - mse: 0.0036 - val_loss: 0.0391 - val_accuracy: 0.4808 - val_mae: 0.0391 - val_mse: 0.0118\n",
            "Epoch 305/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0175 - accuracy: 0.4375 - mae: 0.0175 - mse: 0.0014\n",
            "Epoch 00305: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0238 - accuracy: 0.4765 - mae: 0.0238 - mse: 0.0036 - val_loss: 0.0380 - val_accuracy: 0.4808 - val_mae: 0.0380 - val_mse: 0.0119\n",
            "Epoch 306/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0249 - accuracy: 0.5000 - mae: 0.0249 - mse: 0.0036\n",
            "Epoch 00306: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0233 - accuracy: 0.4765 - mae: 0.0233 - mse: 0.0036 - val_loss: 0.0379 - val_accuracy: 0.4808 - val_mae: 0.0379 - val_mse: 0.0119\n",
            "Epoch 307/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0283 - accuracy: 0.4375 - mae: 0.0283 - mse: 0.0036\n",
            "Epoch 00307: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0236 - accuracy: 0.4765 - mae: 0.0236 - mse: 0.0036 - val_loss: 0.0377 - val_accuracy: 0.4808 - val_mae: 0.0377 - val_mse: 0.0120\n",
            "Epoch 308/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0229 - accuracy: 0.4688 - mae: 0.0229 - mse: 0.0030\n",
            "Epoch 00308: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0233 - accuracy: 0.4765 - mae: 0.0233 - mse: 0.0036 - val_loss: 0.0378 - val_accuracy: 0.4808 - val_mae: 0.0378 - val_mse: 0.0120\n",
            "Epoch 309/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0268 - accuracy: 0.4688 - mae: 0.0268 - mse: 0.0038\n",
            "Epoch 00309: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0232 - accuracy: 0.4765 - mae: 0.0232 - mse: 0.0035 - val_loss: 0.0377 - val_accuracy: 0.4808 - val_mae: 0.0377 - val_mse: 0.0118\n",
            "Epoch 310/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0274 - accuracy: 0.4062 - mae: 0.0274 - mse: 0.0029\n",
            "Epoch 00310: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0231 - accuracy: 0.4765 - mae: 0.0231 - mse: 0.0035 - val_loss: 0.0375 - val_accuracy: 0.4808 - val_mae: 0.0375 - val_mse: 0.0119\n",
            "Epoch 311/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0211 - accuracy: 0.4688 - mae: 0.0211 - mse: 0.0024\n",
            "Epoch 00311: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0231 - accuracy: 0.4765 - mae: 0.0231 - mse: 0.0035 - val_loss: 0.0372 - val_accuracy: 0.4808 - val_mae: 0.0372 - val_mse: 0.0119\n",
            "Epoch 312/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0184 - accuracy: 0.4688 - mae: 0.0184 - mse: 0.0015\n",
            "Epoch 00312: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0229 - accuracy: 0.4765 - mae: 0.0229 - mse: 0.0035 - val_loss: 0.0375 - val_accuracy: 0.4808 - val_mae: 0.0375 - val_mse: 0.0117\n",
            "Epoch 313/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0166 - accuracy: 0.5000 - mae: 0.0166 - mse: 0.0014\n",
            "Epoch 00313: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0228 - accuracy: 0.4765 - mae: 0.0228 - mse: 0.0035 - val_loss: 0.0373 - val_accuracy: 0.4808 - val_mae: 0.0373 - val_mse: 0.0117\n",
            "Epoch 314/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0198 - accuracy: 0.5938 - mae: 0.0198 - mse: 0.0020\n",
            "Epoch 00314: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0230 - accuracy: 0.4765 - mae: 0.0230 - mse: 0.0035 - val_loss: 0.0387 - val_accuracy: 0.4808 - val_mae: 0.0387 - val_mse: 0.0115\n",
            "Epoch 315/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0279 - accuracy: 0.4688 - mae: 0.0279 - mse: 0.0024\n",
            "Epoch 00315: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0230 - accuracy: 0.4765 - mae: 0.0230 - mse: 0.0034 - val_loss: 0.0370 - val_accuracy: 0.4808 - val_mae: 0.0370 - val_mse: 0.0118\n",
            "Epoch 316/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0153 - accuracy: 0.5000 - mae: 0.0153 - mse: 0.0011\n",
            "Epoch 00316: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0227 - accuracy: 0.4765 - mae: 0.0227 - mse: 0.0034 - val_loss: 0.0370 - val_accuracy: 0.4808 - val_mae: 0.0370 - val_mse: 0.0116\n",
            "Epoch 317/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0139 - accuracy: 0.5000 - mae: 0.0139 - mse: 0.0011\n",
            "Epoch 00317: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0226 - accuracy: 0.4765 - mae: 0.0226 - mse: 0.0034 - val_loss: 0.0367 - val_accuracy: 0.4808 - val_mae: 0.0367 - val_mse: 0.0117\n",
            "Epoch 318/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0331 - accuracy: 0.4688 - mae: 0.0331 - mse: 0.0152\n",
            "Epoch 00318: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0225 - accuracy: 0.4765 - mae: 0.0225 - mse: 0.0034 - val_loss: 0.0369 - val_accuracy: 0.4808 - val_mae: 0.0369 - val_mse: 0.0117\n",
            "Epoch 319/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0128 - accuracy: 0.5312 - mae: 0.0128 - mse: 7.8024e-04\n",
            "Epoch 00319: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0225 - accuracy: 0.4765 - mae: 0.0225 - mse: 0.0034 - val_loss: 0.0371 - val_accuracy: 0.4808 - val_mae: 0.0371 - val_mse: 0.0115\n",
            "Epoch 320/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0518 - accuracy: 0.3750 - mae: 0.0518 - mse: 0.0176\n",
            "Epoch 00320: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0224 - accuracy: 0.4765 - mae: 0.0224 - mse: 0.0033 - val_loss: 0.0379 - val_accuracy: 0.4808 - val_mae: 0.0379 - val_mse: 0.0118\n",
            "Epoch 321/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0168 - accuracy: 0.4062 - mae: 0.0168 - mse: 0.0016\n",
            "Epoch 00321: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0223 - accuracy: 0.4765 - mae: 0.0223 - mse: 0.0033 - val_loss: 0.0363 - val_accuracy: 0.4808 - val_mae: 0.0363 - val_mse: 0.0116\n",
            "Epoch 322/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0127 - accuracy: 0.3438 - mae: 0.0127 - mse: 8.6432e-04\n",
            "Epoch 00322: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0223 - accuracy: 0.4765 - mae: 0.0223 - mse: 0.0033 - val_loss: 0.0376 - val_accuracy: 0.4808 - val_mae: 0.0376 - val_mse: 0.0114\n",
            "Epoch 323/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0164 - accuracy: 0.4062 - mae: 0.0164 - mse: 7.5796e-04\n",
            "Epoch 00323: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0223 - accuracy: 0.4765 - mae: 0.0223 - mse: 0.0033 - val_loss: 0.0374 - val_accuracy: 0.4808 - val_mae: 0.0374 - val_mse: 0.0114\n",
            "Epoch 324/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0119 - accuracy: 0.4688 - mae: 0.0119 - mse: 3.9138e-04\n",
            "Epoch 00324: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0221 - accuracy: 0.4765 - mae: 0.0221 - mse: 0.0033 - val_loss: 0.0371 - val_accuracy: 0.4808 - val_mae: 0.0371 - val_mse: 0.0117\n",
            "Epoch 325/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0181 - accuracy: 0.6250 - mae: 0.0181 - mse: 0.0015\n",
            "Epoch 00325: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0221 - accuracy: 0.4765 - mae: 0.0221 - mse: 0.0033 - val_loss: 0.0361 - val_accuracy: 0.4808 - val_mae: 0.0361 - val_mse: 0.0114\n",
            "Epoch 326/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0096 - accuracy: 0.4688 - mae: 0.0096 - mse: 4.0072e-04\n",
            "Epoch 00326: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0219 - accuracy: 0.4765 - mae: 0.0219 - mse: 0.0032 - val_loss: 0.0365 - val_accuracy: 0.4808 - val_mae: 0.0365 - val_mse: 0.0116\n",
            "Epoch 327/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0202 - accuracy: 0.5938 - mae: 0.0202 - mse: 0.0021\n",
            "Epoch 00327: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0219 - accuracy: 0.4765 - mae: 0.0219 - mse: 0.0032 - val_loss: 0.0361 - val_accuracy: 0.4808 - val_mae: 0.0361 - val_mse: 0.0114\n",
            "Epoch 328/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0178 - accuracy: 0.4688 - mae: 0.0178 - mse: 0.0014\n",
            "Epoch 00328: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0219 - accuracy: 0.4765 - mae: 0.0219 - mse: 0.0032 - val_loss: 0.0358 - val_accuracy: 0.4808 - val_mae: 0.0358 - val_mse: 0.0115\n",
            "Epoch 329/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0239 - accuracy: 0.3438 - mae: 0.0239 - mse: 0.0021\n",
            "Epoch 00329: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0216 - accuracy: 0.4765 - mae: 0.0216 - mse: 0.0032 - val_loss: 0.0360 - val_accuracy: 0.4808 - val_mae: 0.0360 - val_mse: 0.0115\n",
            "Epoch 330/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0136 - accuracy: 0.4375 - mae: 0.0136 - mse: 8.3935e-04\n",
            "Epoch 00330: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0216 - accuracy: 0.4765 - mae: 0.0216 - mse: 0.0032 - val_loss: 0.0357 - val_accuracy: 0.4808 - val_mae: 0.0357 - val_mse: 0.0115\n",
            "Epoch 331/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0075 - accuracy: 0.5625 - mae: 0.0075 - mse: 1.4458e-04\n",
            "Epoch 00331: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0215 - accuracy: 0.4765 - mae: 0.0215 - mse: 0.0032 - val_loss: 0.0360 - val_accuracy: 0.4808 - val_mae: 0.0360 - val_mse: 0.0113\n",
            "Epoch 332/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0242 - accuracy: 0.4688 - mae: 0.0242 - mse: 0.0019\n",
            "Epoch 00332: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0218 - accuracy: 0.4765 - mae: 0.0218 - mse: 0.0031 - val_loss: 0.0355 - val_accuracy: 0.4808 - val_mae: 0.0355 - val_mse: 0.0113\n",
            "Epoch 333/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0375 - accuracy: 0.4062 - mae: 0.0375 - mse: 0.0049\n",
            "Epoch 00333: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0213 - accuracy: 0.4765 - mae: 0.0213 - mse: 0.0031 - val_loss: 0.0354 - val_accuracy: 0.4808 - val_mae: 0.0354 - val_mse: 0.0114\n",
            "Epoch 334/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0171 - accuracy: 0.5312 - mae: 0.0171 - mse: 0.0010\n",
            "Epoch 00334: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0213 - accuracy: 0.4765 - mae: 0.0213 - mse: 0.0031 - val_loss: 0.0358 - val_accuracy: 0.4808 - val_mae: 0.0358 - val_mse: 0.0114\n",
            "Epoch 335/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0139 - accuracy: 0.4062 - mae: 0.0139 - mse: 8.2440e-04\n",
            "Epoch 00335: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0212 - accuracy: 0.4765 - mae: 0.0212 - mse: 0.0031 - val_loss: 0.0352 - val_accuracy: 0.4808 - val_mae: 0.0352 - val_mse: 0.0112\n",
            "Epoch 336/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0156 - accuracy: 0.5938 - mae: 0.0156 - mse: 0.0013\n",
            "Epoch 00336: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0212 - accuracy: 0.4765 - mae: 0.0212 - mse: 0.0031 - val_loss: 0.0351 - val_accuracy: 0.4808 - val_mae: 0.0351 - val_mse: 0.0113\n",
            "Epoch 337/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0236 - accuracy: 0.5312 - mae: 0.0236 - mse: 0.0024\n",
            "Epoch 00337: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0212 - accuracy: 0.4765 - mae: 0.0212 - mse: 0.0031 - val_loss: 0.0357 - val_accuracy: 0.4808 - val_mae: 0.0357 - val_mse: 0.0111\n",
            "Epoch 338/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0253 - accuracy: 0.4688 - mae: 0.0253 - mse: 0.0031\n",
            "Epoch 00338: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0210 - accuracy: 0.4765 - mae: 0.0210 - mse: 0.0030 - val_loss: 0.0348 - val_accuracy: 0.4808 - val_mae: 0.0348 - val_mse: 0.0112\n",
            "Epoch 339/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0175 - accuracy: 0.4688 - mae: 0.0175 - mse: 0.0013\n",
            "Epoch 00339: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0208 - accuracy: 0.4765 - mae: 0.0208 - mse: 0.0030 - val_loss: 0.0350 - val_accuracy: 0.4808 - val_mae: 0.0350 - val_mse: 0.0113\n",
            "Epoch 340/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0193 - accuracy: 0.4375 - mae: 0.0193 - mse: 0.0017\n",
            "Epoch 00340: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0211 - accuracy: 0.4765 - mae: 0.0211 - mse: 0.0030 - val_loss: 0.0349 - val_accuracy: 0.4808 - val_mae: 0.0349 - val_mse: 0.0113\n",
            "Epoch 341/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0269 - accuracy: 0.3750 - mae: 0.0269 - mse: 0.0032\n",
            "Epoch 00341: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0208 - accuracy: 0.4765 - mae: 0.0208 - mse: 0.0030 - val_loss: 0.0355 - val_accuracy: 0.4808 - val_mae: 0.0355 - val_mse: 0.0110\n",
            "Epoch 342/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0203 - accuracy: 0.3750 - mae: 0.0203 - mse: 0.0016\n",
            "Epoch 00342: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0208 - accuracy: 0.4765 - mae: 0.0208 - mse: 0.0030 - val_loss: 0.0353 - val_accuracy: 0.4808 - val_mae: 0.0353 - val_mse: 0.0113\n",
            "Epoch 343/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0202 - accuracy: 0.4375 - mae: 0.0202 - mse: 0.0022\n",
            "Epoch 00343: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0206 - accuracy: 0.4765 - mae: 0.0206 - mse: 0.0030 - val_loss: 0.0356 - val_accuracy: 0.4808 - val_mae: 0.0356 - val_mse: 0.0110\n",
            "Epoch 344/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0147 - accuracy: 0.5625 - mae: 0.0147 - mse: 9.3566e-04\n",
            "Epoch 00344: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0208 - accuracy: 0.4765 - mae: 0.0208 - mse: 0.0030 - val_loss: 0.0357 - val_accuracy: 0.4808 - val_mae: 0.0357 - val_mse: 0.0113\n",
            "Epoch 345/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0249 - accuracy: 0.4688 - mae: 0.0249 - mse: 0.0030\n",
            "Epoch 00345: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0205 - accuracy: 0.4765 - mae: 0.0205 - mse: 0.0030 - val_loss: 0.0342 - val_accuracy: 0.4808 - val_mae: 0.0342 - val_mse: 0.0111\n",
            "Epoch 346/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0091 - accuracy: 0.5000 - mae: 0.0091 - mse: 3.9169e-04\n",
            "Epoch 00346: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0205 - accuracy: 0.4765 - mae: 0.0205 - mse: 0.0029 - val_loss: 0.0345 - val_accuracy: 0.4808 - val_mae: 0.0345 - val_mse: 0.0110\n",
            "Epoch 347/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0149 - accuracy: 0.4688 - mae: 0.0149 - mse: 0.0010\n",
            "Epoch 00347: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0203 - accuracy: 0.4765 - mae: 0.0203 - mse: 0.0029 - val_loss: 0.0344 - val_accuracy: 0.4808 - val_mae: 0.0344 - val_mse: 0.0110\n",
            "Epoch 348/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0230 - accuracy: 0.4688 - mae: 0.0230 - mse: 0.0016\n",
            "Epoch 00348: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0204 - accuracy: 0.4765 - mae: 0.0204 - mse: 0.0029 - val_loss: 0.0339 - val_accuracy: 0.4808 - val_mae: 0.0339 - val_mse: 0.0110\n",
            "Epoch 349/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0332 - accuracy: 0.3750 - mae: 0.0332 - mse: 0.0151\n",
            "Epoch 00349: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0201 - accuracy: 0.4765 - mae: 0.0201 - mse: 0.0029 - val_loss: 0.0347 - val_accuracy: 0.4808 - val_mae: 0.0347 - val_mse: 0.0109\n",
            "Epoch 350/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0139 - accuracy: 0.5000 - mae: 0.0139 - mse: 8.7151e-04\n",
            "Epoch 00350: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0205 - accuracy: 0.4765 - mae: 0.0205 - mse: 0.0029 - val_loss: 0.0345 - val_accuracy: 0.4808 - val_mae: 0.0345 - val_mse: 0.0109\n",
            "Epoch 351/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0223 - accuracy: 0.4375 - mae: 0.0223 - mse: 0.0018\n",
            "Epoch 00351: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0201 - accuracy: 0.4765 - mae: 0.0201 - mse: 0.0028 - val_loss: 0.0337 - val_accuracy: 0.4808 - val_mae: 0.0337 - val_mse: 0.0109\n",
            "Epoch 352/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0468 - accuracy: 0.3438 - mae: 0.0468 - mse: 0.0170\n",
            "Epoch 00352: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0200 - accuracy: 0.4765 - mae: 0.0200 - mse: 0.0028 - val_loss: 0.0350 - val_accuracy: 0.4808 - val_mae: 0.0350 - val_mse: 0.0112\n",
            "Epoch 353/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0168 - accuracy: 0.4375 - mae: 0.0168 - mse: 0.0013\n",
            "Epoch 00353: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0202 - accuracy: 0.4765 - mae: 0.0202 - mse: 0.0028 - val_loss: 0.0336 - val_accuracy: 0.4808 - val_mae: 0.0336 - val_mse: 0.0109\n",
            "Epoch 354/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0151 - accuracy: 0.6250 - mae: 0.0151 - mse: 0.0013\n",
            "Epoch 00354: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0199 - accuracy: 0.4765 - mae: 0.0199 - mse: 0.0028 - val_loss: 0.0336 - val_accuracy: 0.4808 - val_mae: 0.0336 - val_mse: 0.0108\n",
            "Epoch 355/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0315 - accuracy: 0.5312 - mae: 0.0315 - mse: 0.0032\n",
            "Epoch 00355: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0198 - accuracy: 0.4765 - mae: 0.0198 - mse: 0.0028 - val_loss: 0.0334 - val_accuracy: 0.4808 - val_mae: 0.0334 - val_mse: 0.0109\n",
            "Epoch 356/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0146 - accuracy: 0.4375 - mae: 0.0146 - mse: 9.1533e-04\n",
            "Epoch 00356: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0197 - accuracy: 0.4765 - mae: 0.0197 - mse: 0.0028 - val_loss: 0.0343 - val_accuracy: 0.4808 - val_mae: 0.0343 - val_mse: 0.0111\n",
            "Epoch 357/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0114 - accuracy: 0.5000 - mae: 0.0114 - mse: 5.3391e-04\n",
            "Epoch 00357: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0197 - accuracy: 0.4765 - mae: 0.0197 - mse: 0.0028 - val_loss: 0.0349 - val_accuracy: 0.4808 - val_mae: 0.0349 - val_mse: 0.0111\n",
            "Epoch 358/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0204 - accuracy: 0.4375 - mae: 0.0204 - mse: 0.0014\n",
            "Epoch 00358: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0197 - accuracy: 0.4765 - mae: 0.0197 - mse: 0.0027 - val_loss: 0.0331 - val_accuracy: 0.4808 - val_mae: 0.0331 - val_mse: 0.0108\n",
            "Epoch 359/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0115 - accuracy: 0.5938 - mae: 0.0115 - mse: 0.0011\n",
            "Epoch 00359: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0197 - accuracy: 0.4765 - mae: 0.0197 - mse: 0.0027 - val_loss: 0.0336 - val_accuracy: 0.4808 - val_mae: 0.0336 - val_mse: 0.0107\n",
            "Epoch 360/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0283 - accuracy: 0.5312 - mae: 0.0283 - mse: 0.0022\n",
            "Epoch 00360: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0195 - accuracy: 0.4765 - mae: 0.0195 - mse: 0.0027 - val_loss: 0.0329 - val_accuracy: 0.4808 - val_mae: 0.0329 - val_mse: 0.0108\n",
            "Epoch 361/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0130 - accuracy: 0.5000 - mae: 0.0130 - mse: 0.0013\n",
            "Epoch 00361: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0194 - accuracy: 0.4765 - mae: 0.0194 - mse: 0.0027 - val_loss: 0.0335 - val_accuracy: 0.4808 - val_mae: 0.0335 - val_mse: 0.0107\n",
            "Epoch 362/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0096 - accuracy: 0.5000 - mae: 0.0096 - mse: 2.5666e-04\n",
            "Epoch 00362: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0193 - accuracy: 0.4765 - mae: 0.0193 - mse: 0.0027 - val_loss: 0.0327 - val_accuracy: 0.4808 - val_mae: 0.0327 - val_mse: 0.0108\n",
            "Epoch 363/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0219 - accuracy: 0.5312 - mae: 0.0219 - mse: 0.0026\n",
            "Epoch 00363: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0192 - accuracy: 0.4765 - mae: 0.0192 - mse: 0.0027 - val_loss: 0.0354 - val_accuracy: 0.4808 - val_mae: 0.0354 - val_mse: 0.0111\n",
            "Epoch 364/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0165 - accuracy: 0.5000 - mae: 0.0165 - mse: 8.3779e-04\n",
            "Epoch 00364: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0194 - accuracy: 0.4765 - mae: 0.0194 - mse: 0.0027 - val_loss: 0.0326 - val_accuracy: 0.4808 - val_mae: 0.0326 - val_mse: 0.0107\n",
            "Epoch 365/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0254 - accuracy: 0.5625 - mae: 0.0254 - mse: 0.0144\n",
            "Epoch 00365: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0190 - accuracy: 0.4765 - mae: 0.0190 - mse: 0.0027 - val_loss: 0.0326 - val_accuracy: 0.4808 - val_mae: 0.0326 - val_mse: 0.0107\n",
            "Epoch 366/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0094 - accuracy: 0.5000 - mae: 0.0094 - mse: 3.7841e-04\n",
            "Epoch 00366: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0190 - accuracy: 0.4765 - mae: 0.0190 - mse: 0.0026 - val_loss: 0.0323 - val_accuracy: 0.4808 - val_mae: 0.0323 - val_mse: 0.0107\n",
            "Epoch 367/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0079 - accuracy: 0.5000 - mae: 0.0079 - mse: 3.9743e-04\n",
            "Epoch 00367: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0189 - accuracy: 0.4765 - mae: 0.0189 - mse: 0.0026 - val_loss: 0.0331 - val_accuracy: 0.4808 - val_mae: 0.0331 - val_mse: 0.0106\n",
            "Epoch 368/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0195 - accuracy: 0.5000 - mae: 0.0195 - mse: 0.0017\n",
            "Epoch 00368: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0190 - accuracy: 0.4765 - mae: 0.0190 - mse: 0.0026 - val_loss: 0.0321 - val_accuracy: 0.4808 - val_mae: 0.0321 - val_mse: 0.0107\n",
            "Epoch 369/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0224 - accuracy: 0.4062 - mae: 0.0224 - mse: 0.0021\n",
            "Epoch 00369: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0188 - accuracy: 0.4765 - mae: 0.0188 - mse: 0.0026 - val_loss: 0.0326 - val_accuracy: 0.4808 - val_mae: 0.0326 - val_mse: 0.0106\n",
            "Epoch 370/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0105 - accuracy: 0.3750 - mae: 0.0105 - mse: 3.4763e-04\n",
            "Epoch 00370: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0190 - accuracy: 0.4765 - mae: 0.0190 - mse: 0.0026 - val_loss: 0.0320 - val_accuracy: 0.4808 - val_mae: 0.0320 - val_mse: 0.0106\n",
            "Epoch 371/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0101 - accuracy: 0.5000 - mae: 0.0101 - mse: 6.4580e-04\n",
            "Epoch 00371: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0186 - accuracy: 0.4765 - mae: 0.0186 - mse: 0.0026 - val_loss: 0.0326 - val_accuracy: 0.4808 - val_mae: 0.0326 - val_mse: 0.0105\n",
            "Epoch 372/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0214 - accuracy: 0.5312 - mae: 0.0214 - mse: 0.0013\n",
            "Epoch 00372: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0187 - accuracy: 0.4765 - mae: 0.0187 - mse: 0.0026 - val_loss: 0.0319 - val_accuracy: 0.4808 - val_mae: 0.0319 - val_mse: 0.0107\n",
            "Epoch 373/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0138 - accuracy: 0.3438 - mae: 0.0138 - mse: 7.6954e-04\n",
            "Epoch 00373: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0186 - accuracy: 0.4765 - mae: 0.0186 - mse: 0.0026 - val_loss: 0.0328 - val_accuracy: 0.4808 - val_mae: 0.0328 - val_mse: 0.0108\n",
            "Epoch 374/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0212 - accuracy: 0.5000 - mae: 0.0212 - mse: 0.0023\n",
            "Epoch 00374: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0185 - accuracy: 0.4765 - mae: 0.0185 - mse: 0.0025 - val_loss: 0.0320 - val_accuracy: 0.4808 - val_mae: 0.0320 - val_mse: 0.0107\n",
            "Epoch 375/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0410 - accuracy: 0.3750 - mae: 0.0410 - mse: 0.0159\n",
            "Epoch 00375: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0183 - accuracy: 0.4765 - mae: 0.0183 - mse: 0.0025 - val_loss: 0.0319 - val_accuracy: 0.4808 - val_mae: 0.0319 - val_mse: 0.0105\n",
            "Epoch 376/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0125 - accuracy: 0.3438 - mae: 0.0125 - mse: 4.7967e-04\n",
            "Epoch 00376: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0184 - accuracy: 0.4765 - mae: 0.0184 - mse: 0.0025 - val_loss: 0.0319 - val_accuracy: 0.4808 - val_mae: 0.0319 - val_mse: 0.0107\n",
            "Epoch 377/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0410 - accuracy: 0.5625 - mae: 0.0410 - mse: 0.0159\n",
            "Epoch 00377: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0181 - accuracy: 0.4765 - mae: 0.0181 - mse: 0.0025 - val_loss: 0.0320 - val_accuracy: 0.4808 - val_mae: 0.0320 - val_mse: 0.0104\n",
            "Epoch 378/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0155 - accuracy: 0.5938 - mae: 0.0155 - mse: 0.0013\n",
            "Epoch 00378: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0182 - accuracy: 0.4765 - mae: 0.0182 - mse: 0.0025 - val_loss: 0.0336 - val_accuracy: 0.4808 - val_mae: 0.0336 - val_mse: 0.0108\n",
            "Epoch 379/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0168 - accuracy: 0.5000 - mae: 0.0168 - mse: 9.6127e-04\n",
            "Epoch 00379: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0185 - accuracy: 0.4765 - mae: 0.0185 - mse: 0.0025 - val_loss: 0.0313 - val_accuracy: 0.4808 - val_mae: 0.0313 - val_mse: 0.0104\n",
            "Epoch 380/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0347 - accuracy: 0.4688 - mae: 0.0347 - mse: 0.0152\n",
            "Epoch 00380: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0180 - accuracy: 0.4765 - mae: 0.0180 - mse: 0.0025 - val_loss: 0.0317 - val_accuracy: 0.4808 - val_mae: 0.0317 - val_mse: 0.0104\n",
            "Epoch 381/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0111 - accuracy: 0.4688 - mae: 0.0111 - mse: 4.1939e-04\n",
            "Epoch 00381: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0180 - accuracy: 0.4765 - mae: 0.0180 - mse: 0.0025 - val_loss: 0.0310 - val_accuracy: 0.4808 - val_mae: 0.0310 - val_mse: 0.0105\n",
            "Epoch 382/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0206 - accuracy: 0.3750 - mae: 0.0206 - mse: 0.0016\n",
            "Epoch 00382: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0178 - accuracy: 0.4765 - mae: 0.0178 - mse: 0.0025 - val_loss: 0.0317 - val_accuracy: 0.4808 - val_mae: 0.0317 - val_mse: 0.0106\n",
            "Epoch 383/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0138 - accuracy: 0.5625 - mae: 0.0138 - mse: 0.0014\n",
            "Epoch 00383: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0180 - accuracy: 0.4765 - mae: 0.0180 - mse: 0.0025 - val_loss: 0.0317 - val_accuracy: 0.4808 - val_mae: 0.0317 - val_mse: 0.0103\n",
            "Epoch 384/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0179 - accuracy: 0.5312 - mae: 0.0179 - mse: 0.0011\n",
            "Epoch 00384: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0179 - accuracy: 0.4765 - mae: 0.0179 - mse: 0.0024 - val_loss: 0.0308 - val_accuracy: 0.4808 - val_mae: 0.0308 - val_mse: 0.0104\n",
            "Epoch 385/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0177 - accuracy: 0.6562 - mae: 0.0177 - mse: 0.0016\n",
            "Epoch 00385: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0178 - accuracy: 0.4765 - mae: 0.0178 - mse: 0.0025 - val_loss: 0.0323 - val_accuracy: 0.4808 - val_mae: 0.0323 - val_mse: 0.0106\n",
            "Epoch 386/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0232 - accuracy: 0.3750 - mae: 0.0232 - mse: 0.0019\n",
            "Epoch 00386: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0177 - accuracy: 0.4765 - mae: 0.0177 - mse: 0.0024 - val_loss: 0.0307 - val_accuracy: 0.4808 - val_mae: 0.0307 - val_mse: 0.0104\n",
            "Epoch 387/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0155 - accuracy: 0.5312 - mae: 0.0155 - mse: 7.3268e-04\n",
            "Epoch 00387: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0175 - accuracy: 0.4765 - mae: 0.0175 - mse: 0.0024 - val_loss: 0.0307 - val_accuracy: 0.4808 - val_mae: 0.0307 - val_mse: 0.0105\n",
            "Epoch 388/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0151 - accuracy: 0.5938 - mae: 0.0151 - mse: 8.1910e-04\n",
            "Epoch 00388: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0175 - accuracy: 0.4765 - mae: 0.0175 - mse: 0.0024 - val_loss: 0.0307 - val_accuracy: 0.4808 - val_mae: 0.0307 - val_mse: 0.0103\n",
            "Epoch 389/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0068 - accuracy: 0.5938 - mae: 0.0068 - mse: 9.7765e-05\n",
            "Epoch 00389: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0175 - accuracy: 0.4765 - mae: 0.0175 - mse: 0.0024 - val_loss: 0.0324 - val_accuracy: 0.4808 - val_mae: 0.0324 - val_mse: 0.0106\n",
            "Epoch 390/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0206 - accuracy: 0.4062 - mae: 0.0206 - mse: 0.0016\n",
            "Epoch 00390: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0176 - accuracy: 0.4765 - mae: 0.0176 - mse: 0.0024 - val_loss: 0.0303 - val_accuracy: 0.4808 - val_mae: 0.0303 - val_mse: 0.0104\n",
            "Epoch 391/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0124 - accuracy: 0.5000 - mae: 0.0124 - mse: 6.3981e-04\n",
            "Epoch 00391: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0172 - accuracy: 0.4765 - mae: 0.0172 - mse: 0.0024 - val_loss: 0.0304 - val_accuracy: 0.4808 - val_mae: 0.0304 - val_mse: 0.0104\n",
            "Epoch 392/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0354 - accuracy: 0.4688 - mae: 0.0354 - mse: 0.0155\n",
            "Epoch 00392: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0173 - accuracy: 0.4765 - mae: 0.0173 - mse: 0.0024 - val_loss: 0.0303 - val_accuracy: 0.4808 - val_mae: 0.0303 - val_mse: 0.0104\n",
            "Epoch 393/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0223 - accuracy: 0.4375 - mae: 0.0223 - mse: 0.0016\n",
            "Epoch 00393: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0173 - accuracy: 0.4765 - mae: 0.0173 - mse: 0.0023 - val_loss: 0.0314 - val_accuracy: 0.4808 - val_mae: 0.0314 - val_mse: 0.0102\n",
            "Epoch 394/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0173 - accuracy: 0.5312 - mae: 0.0173 - mse: 7.1158e-04\n",
            "Epoch 00394: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0172 - accuracy: 0.4765 - mae: 0.0172 - mse: 0.0023 - val_loss: 0.0305 - val_accuracy: 0.4808 - val_mae: 0.0305 - val_mse: 0.0102\n",
            "Epoch 395/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0310 - accuracy: 0.3750 - mae: 0.0310 - mse: 0.0032\n",
            "Epoch 00395: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0171 - accuracy: 0.4765 - mae: 0.0171 - mse: 0.0023 - val_loss: 0.0308 - val_accuracy: 0.4808 - val_mae: 0.0308 - val_mse: 0.0104\n",
            "Epoch 396/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0152 - accuracy: 0.5625 - mae: 0.0152 - mse: 7.0467e-04\n",
            "Epoch 00396: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0171 - accuracy: 0.4765 - mae: 0.0171 - mse: 0.0023 - val_loss: 0.0300 - val_accuracy: 0.4808 - val_mae: 0.0300 - val_mse: 0.0103\n",
            "Epoch 397/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0254 - accuracy: 0.6562 - mae: 0.0254 - mse: 0.0028\n",
            "Epoch 00397: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0168 - accuracy: 0.4765 - mae: 0.0168 - mse: 0.0023 - val_loss: 0.0311 - val_accuracy: 0.4808 - val_mae: 0.0311 - val_mse: 0.0104\n",
            "Epoch 398/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0163 - accuracy: 0.5000 - mae: 0.0163 - mse: 7.3698e-04\n",
            "Epoch 00398: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0171 - accuracy: 0.4765 - mae: 0.0171 - mse: 0.0023 - val_loss: 0.0300 - val_accuracy: 0.4808 - val_mae: 0.0300 - val_mse: 0.0102\n",
            "Epoch 399/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0336 - accuracy: 0.5000 - mae: 0.0336 - mse: 0.0150\n",
            "Epoch 00399: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0168 - accuracy: 0.4765 - mae: 0.0168 - mse: 0.0023 - val_loss: 0.0298 - val_accuracy: 0.4808 - val_mae: 0.0298 - val_mse: 0.0102\n",
            "Epoch 400/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0069 - accuracy: 0.5312 - mae: 0.0069 - mse: 1.7874e-04\n",
            "Epoch 00400: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0167 - accuracy: 0.4765 - mae: 0.0167 - mse: 0.0023 - val_loss: 0.0297 - val_accuracy: 0.4808 - val_mae: 0.0297 - val_mse: 0.0102\n",
            "Epoch 401/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0130 - accuracy: 0.4062 - mae: 0.0130 - mse: 6.6797e-04\n",
            "Epoch 00401: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0165 - accuracy: 0.4765 - mae: 0.0165 - mse: 0.0023 - val_loss: 0.0298 - val_accuracy: 0.4808 - val_mae: 0.0298 - val_mse: 0.0103\n",
            "Epoch 402/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0162 - accuracy: 0.4688 - mae: 0.0162 - mse: 0.0012\n",
            "Epoch 00402: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0166 - accuracy: 0.4765 - mae: 0.0166 - mse: 0.0023 - val_loss: 0.0306 - val_accuracy: 0.4808 - val_mae: 0.0306 - val_mse: 0.0101\n",
            "Epoch 403/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0135 - accuracy: 0.5625 - mae: 0.0135 - mse: 5.1109e-04\n",
            "Epoch 00403: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0167 - accuracy: 0.4765 - mae: 0.0167 - mse: 0.0023 - val_loss: 0.0293 - val_accuracy: 0.4808 - val_mae: 0.0293 - val_mse: 0.0102\n",
            "Epoch 404/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0125 - accuracy: 0.5312 - mae: 0.0125 - mse: 5.3393e-04\n",
            "Epoch 00404: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0164 - accuracy: 0.4765 - mae: 0.0164 - mse: 0.0022 - val_loss: 0.0292 - val_accuracy: 0.4808 - val_mae: 0.0292 - val_mse: 0.0102\n",
            "Epoch 405/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0061 - accuracy: 0.6250 - mae: 0.0061 - mse: 2.1563e-04\n",
            "Epoch 00405: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0163 - accuracy: 0.4765 - mae: 0.0163 - mse: 0.0022 - val_loss: 0.0294 - val_accuracy: 0.4808 - val_mae: 0.0294 - val_mse: 0.0103\n",
            "Epoch 406/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0145 - accuracy: 0.4375 - mae: 0.0145 - mse: 0.0010\n",
            "Epoch 00406: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0164 - accuracy: 0.4765 - mae: 0.0164 - mse: 0.0022 - val_loss: 0.0305 - val_accuracy: 0.4808 - val_mae: 0.0305 - val_mse: 0.0104\n",
            "Epoch 407/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0176 - accuracy: 0.4375 - mae: 0.0176 - mse: 0.0016\n",
            "Epoch 00407: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0164 - accuracy: 0.4765 - mae: 0.0164 - mse: 0.0022 - val_loss: 0.0293 - val_accuracy: 0.4808 - val_mae: 0.0293 - val_mse: 0.0101\n",
            "Epoch 408/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0154 - accuracy: 0.4375 - mae: 0.0154 - mse: 9.2338e-04\n",
            "Epoch 00408: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0163 - accuracy: 0.4765 - mae: 0.0163 - mse: 0.0022 - val_loss: 0.0303 - val_accuracy: 0.4808 - val_mae: 0.0303 - val_mse: 0.0100\n",
            "Epoch 409/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0360 - accuracy: 0.5000 - mae: 0.0360 - mse: 0.0148\n",
            "Epoch 00409: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0164 - accuracy: 0.4765 - mae: 0.0164 - mse: 0.0022 - val_loss: 0.0289 - val_accuracy: 0.4808 - val_mae: 0.0289 - val_mse: 0.0101\n",
            "Epoch 410/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0147 - accuracy: 0.4062 - mae: 0.0147 - mse: 0.0010\n",
            "Epoch 00410: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0161 - accuracy: 0.4765 - mae: 0.0161 - mse: 0.0022 - val_loss: 0.0307 - val_accuracy: 0.4808 - val_mae: 0.0307 - val_mse: 0.0104\n",
            "Epoch 411/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0149 - accuracy: 0.5312 - mae: 0.0149 - mse: 6.2594e-04\n",
            "Epoch 00411: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0161 - accuracy: 0.4765 - mae: 0.0161 - mse: 0.0022 - val_loss: 0.0300 - val_accuracy: 0.4808 - val_mae: 0.0300 - val_mse: 0.0103\n",
            "Epoch 412/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0195 - accuracy: 0.4375 - mae: 0.0195 - mse: 0.0020\n",
            "Epoch 00412: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0160 - accuracy: 0.4765 - mae: 0.0160 - mse: 0.0022 - val_loss: 0.0288 - val_accuracy: 0.4808 - val_mae: 0.0288 - val_mse: 0.0102\n",
            "Epoch 413/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0108 - accuracy: 0.4688 - mae: 0.0108 - mse: 7.4630e-04\n",
            "Epoch 00413: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0158 - accuracy: 0.4765 - mae: 0.0158 - mse: 0.0022 - val_loss: 0.0287 - val_accuracy: 0.4808 - val_mae: 0.0287 - val_mse: 0.0101\n",
            "Epoch 414/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0088 - accuracy: 0.5000 - mae: 0.0088 - mse: 2.9132e-04\n",
            "Epoch 00414: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0157 - accuracy: 0.4765 - mae: 0.0157 - mse: 0.0022 - val_loss: 0.0286 - val_accuracy: 0.4808 - val_mae: 0.0286 - val_mse: 0.0101\n",
            "Epoch 415/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0121 - accuracy: 0.5938 - mae: 0.0121 - mse: 0.0012\n",
            "Epoch 00415: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0157 - accuracy: 0.4765 - mae: 0.0157 - mse: 0.0022 - val_loss: 0.0289 - val_accuracy: 0.4808 - val_mae: 0.0289 - val_mse: 0.0100\n",
            "Epoch 416/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0163 - accuracy: 0.4062 - mae: 0.0163 - mse: 0.0011\n",
            "Epoch 00416: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0156 - accuracy: 0.4765 - mae: 0.0156 - mse: 0.0022 - val_loss: 0.0285 - val_accuracy: 0.4808 - val_mae: 0.0285 - val_mse: 0.0101\n",
            "Epoch 417/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0198 - accuracy: 0.5000 - mae: 0.0198 - mse: 0.0017\n",
            "Epoch 00417: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0157 - accuracy: 0.4765 - mae: 0.0157 - mse: 0.0021 - val_loss: 0.0295 - val_accuracy: 0.4808 - val_mae: 0.0295 - val_mse: 0.0103\n",
            "Epoch 418/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0169 - accuracy: 0.3438 - mae: 0.0169 - mse: 0.0015\n",
            "Epoch 00418: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0156 - accuracy: 0.4765 - mae: 0.0156 - mse: 0.0022 - val_loss: 0.0284 - val_accuracy: 0.4808 - val_mae: 0.0284 - val_mse: 0.0101\n",
            "Epoch 419/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0051 - accuracy: 0.6562 - mae: 0.0051 - mse: 4.9347e-05\n",
            "Epoch 00419: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0155 - accuracy: 0.4765 - mae: 0.0155 - mse: 0.0022 - val_loss: 0.0282 - val_accuracy: 0.4808 - val_mae: 0.0282 - val_mse: 0.0100\n",
            "Epoch 420/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0191 - accuracy: 0.4062 - mae: 0.0191 - mse: 0.0020\n",
            "Epoch 00420: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0155 - accuracy: 0.4765 - mae: 0.0155 - mse: 0.0021 - val_loss: 0.0281 - val_accuracy: 0.4808 - val_mae: 0.0281 - val_mse: 0.0100\n",
            "Epoch 421/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0164 - accuracy: 0.5000 - mae: 0.0164 - mse: 0.0012\n",
            "Epoch 00421: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0152 - accuracy: 0.4765 - mae: 0.0152 - mse: 0.0021 - val_loss: 0.0279 - val_accuracy: 0.4808 - val_mae: 0.0279 - val_mse: 0.0101\n",
            "Epoch 422/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0202 - accuracy: 0.4375 - mae: 0.0202 - mse: 0.0018\n",
            "Epoch 00422: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0155 - accuracy: 0.4765 - mae: 0.0155 - mse: 0.0021 - val_loss: 0.0284 - val_accuracy: 0.4808 - val_mae: 0.0284 - val_mse: 0.0101\n",
            "Epoch 423/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0066 - accuracy: 0.4375 - mae: 0.0066 - mse: 2.0177e-04\n",
            "Epoch 00423: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0152 - accuracy: 0.4765 - mae: 0.0152 - mse: 0.0021 - val_loss: 0.0278 - val_accuracy: 0.4808 - val_mae: 0.0278 - val_mse: 0.0100\n",
            "Epoch 424/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0090 - accuracy: 0.4062 - mae: 0.0090 - mse: 2.8042e-04\n",
            "Epoch 00424: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0153 - accuracy: 0.4765 - mae: 0.0153 - mse: 0.0021 - val_loss: 0.0277 - val_accuracy: 0.4808 - val_mae: 0.0277 - val_mse: 0.0100\n",
            "Epoch 425/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0146 - accuracy: 0.4062 - mae: 0.0146 - mse: 0.0014\n",
            "Epoch 00425: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0151 - accuracy: 0.4765 - mae: 0.0151 - mse: 0.0021 - val_loss: 0.0276 - val_accuracy: 0.4808 - val_mae: 0.0276 - val_mse: 0.0100\n",
            "Epoch 426/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0098 - accuracy: 0.5000 - mae: 0.0098 - mse: 2.8671e-04\n",
            "Epoch 00426: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0149 - accuracy: 0.4765 - mae: 0.0149 - mse: 0.0021 - val_loss: 0.0285 - val_accuracy: 0.4808 - val_mae: 0.0285 - val_mse: 0.0099\n",
            "Epoch 427/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0212 - accuracy: 0.5625 - mae: 0.0212 - mse: 0.0012\n",
            "Epoch 00427: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0151 - accuracy: 0.4765 - mae: 0.0151 - mse: 0.0021 - val_loss: 0.0295 - val_accuracy: 0.4808 - val_mae: 0.0295 - val_mse: 0.0098\n",
            "Epoch 428/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0131 - accuracy: 0.3750 - mae: 0.0131 - mse: 4.2955e-04\n",
            "Epoch 00428: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0150 - accuracy: 0.4765 - mae: 0.0150 - mse: 0.0021 - val_loss: 0.0276 - val_accuracy: 0.4808 - val_mae: 0.0276 - val_mse: 0.0101\n",
            "Epoch 429/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0298 - accuracy: 0.5312 - mae: 0.0298 - mse: 0.0148\n",
            "Epoch 00429: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0148 - accuracy: 0.4765 - mae: 0.0148 - mse: 0.0021 - val_loss: 0.0279 - val_accuracy: 0.4808 - val_mae: 0.0279 - val_mse: 0.0099\n",
            "Epoch 430/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0271 - accuracy: 0.4062 - mae: 0.0271 - mse: 0.0027\n",
            "Epoch 00430: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0151 - accuracy: 0.4765 - mae: 0.0151 - mse: 0.0021 - val_loss: 0.0290 - val_accuracy: 0.4808 - val_mae: 0.0290 - val_mse: 0.0102\n",
            "Epoch 431/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0230 - accuracy: 0.4062 - mae: 0.0230 - mse: 0.0017\n",
            "Epoch 00431: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0149 - accuracy: 0.4765 - mae: 0.0149 - mse: 0.0021 - val_loss: 0.0278 - val_accuracy: 0.4808 - val_mae: 0.0278 - val_mse: 0.0099\n",
            "Epoch 432/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0185 - accuracy: 0.5000 - mae: 0.0185 - mse: 0.0017\n",
            "Epoch 00432: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0146 - accuracy: 0.4765 - mae: 0.0146 - mse: 0.0021 - val_loss: 0.0277 - val_accuracy: 0.4808 - val_mae: 0.0277 - val_mse: 0.0099\n",
            "Epoch 433/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0155 - accuracy: 0.5938 - mae: 0.0155 - mse: 0.0015\n",
            "Epoch 00433: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0149 - accuracy: 0.4765 - mae: 0.0149 - mse: 0.0021 - val_loss: 0.0277 - val_accuracy: 0.4808 - val_mae: 0.0277 - val_mse: 0.0101\n",
            "Epoch 434/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0376 - accuracy: 0.2812 - mae: 0.0376 - mse: 0.0160\n",
            "Epoch 00434: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0147 - accuracy: 0.4765 - mae: 0.0147 - mse: 0.0021 - val_loss: 0.0284 - val_accuracy: 0.4808 - val_mae: 0.0284 - val_mse: 0.0098\n",
            "Epoch 435/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0163 - accuracy: 0.5938 - mae: 0.0163 - mse: 0.0012\n",
            "Epoch 00435: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0147 - accuracy: 0.4765 - mae: 0.0147 - mse: 0.0020 - val_loss: 0.0272 - val_accuracy: 0.4808 - val_mae: 0.0272 - val_mse: 0.0099\n",
            "Epoch 436/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0191 - accuracy: 0.4062 - mae: 0.0191 - mse: 0.0015\n",
            "Epoch 00436: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0145 - accuracy: 0.4765 - mae: 0.0145 - mse: 0.0020 - val_loss: 0.0282 - val_accuracy: 0.4808 - val_mae: 0.0282 - val_mse: 0.0098\n",
            "Epoch 437/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0157 - accuracy: 0.5000 - mae: 0.0157 - mse: 6.4508e-04\n",
            "Epoch 00437: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0146 - accuracy: 0.4765 - mae: 0.0146 - mse: 0.0020 - val_loss: 0.0271 - val_accuracy: 0.4808 - val_mae: 0.0271 - val_mse: 0.0099\n",
            "Epoch 438/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0134 - accuracy: 0.5625 - mae: 0.0134 - mse: 4.3746e-04\n",
            "Epoch 00438: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0145 - accuracy: 0.4765 - mae: 0.0145 - mse: 0.0020 - val_loss: 0.0291 - val_accuracy: 0.4808 - val_mae: 0.0291 - val_mse: 0.0102\n",
            "Epoch 439/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0121 - accuracy: 0.4062 - mae: 0.0121 - mse: 5.1441e-04\n",
            "Epoch 00439: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0148 - accuracy: 0.4765 - mae: 0.0148 - mse: 0.0020 - val_loss: 0.0272 - val_accuracy: 0.4808 - val_mae: 0.0272 - val_mse: 0.0100\n",
            "Epoch 440/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0088 - accuracy: 0.4688 - mae: 0.0088 - mse: 2.4035e-04\n",
            "Epoch 00440: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0142 - accuracy: 0.4765 - mae: 0.0142 - mse: 0.0020 - val_loss: 0.0270 - val_accuracy: 0.4808 - val_mae: 0.0270 - val_mse: 0.0100\n",
            "Epoch 441/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0310 - accuracy: 0.4062 - mae: 0.0310 - mse: 0.0150\n",
            "Epoch 00441: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0143 - accuracy: 0.4765 - mae: 0.0143 - mse: 0.0020 - val_loss: 0.0268 - val_accuracy: 0.4808 - val_mae: 0.0268 - val_mse: 0.0099\n",
            "Epoch 442/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0096 - accuracy: 0.4688 - mae: 0.0096 - mse: 2.4193e-04\n",
            "Epoch 00442: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0144 - accuracy: 0.4765 - mae: 0.0144 - mse: 0.0020 - val_loss: 0.0267 - val_accuracy: 0.4808 - val_mae: 0.0267 - val_mse: 0.0100\n",
            "Epoch 443/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0185 - accuracy: 0.3750 - mae: 0.0185 - mse: 9.5500e-04\n",
            "Epoch 00443: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0143 - accuracy: 0.4765 - mae: 0.0143 - mse: 0.0020 - val_loss: 0.0267 - val_accuracy: 0.4808 - val_mae: 0.0267 - val_mse: 0.0098\n",
            "Epoch 444/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0153 - accuracy: 0.4688 - mae: 0.0153 - mse: 5.9023e-04\n",
            "Epoch 00444: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0141 - accuracy: 0.4765 - mae: 0.0141 - mse: 0.0020 - val_loss: 0.0280 - val_accuracy: 0.4808 - val_mae: 0.0280 - val_mse: 0.0097\n",
            "Epoch 445/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0199 - accuracy: 0.4688 - mae: 0.0199 - mse: 0.0014\n",
            "Epoch 00445: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0141 - accuracy: 0.4765 - mae: 0.0141 - mse: 0.0020 - val_loss: 0.0263 - val_accuracy: 0.4808 - val_mae: 0.0263 - val_mse: 0.0099\n",
            "Epoch 446/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0079 - accuracy: 0.5312 - mae: 0.0079 - mse: 3.5763e-04\n",
            "Epoch 00446: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0139 - accuracy: 0.4765 - mae: 0.0139 - mse: 0.0020 - val_loss: 0.0276 - val_accuracy: 0.4808 - val_mae: 0.0276 - val_mse: 0.0101\n",
            "Epoch 447/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0161 - accuracy: 0.4375 - mae: 0.0161 - mse: 8.7412e-04\n",
            "Epoch 00447: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0140 - accuracy: 0.4765 - mae: 0.0140 - mse: 0.0020 - val_loss: 0.0262 - val_accuracy: 0.4808 - val_mae: 0.0262 - val_mse: 0.0099\n",
            "Epoch 448/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0197 - accuracy: 0.4375 - mae: 0.0197 - mse: 0.0018\n",
            "Epoch 00448: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0137 - accuracy: 0.4765 - mae: 0.0137 - mse: 0.0020 - val_loss: 0.0279 - val_accuracy: 0.4808 - val_mae: 0.0279 - val_mse: 0.0101\n",
            "Epoch 449/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0089 - accuracy: 0.5312 - mae: 0.0089 - mse: 2.9073e-04\n",
            "Epoch 00449: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0140 - accuracy: 0.4765 - mae: 0.0140 - mse: 0.0020 - val_loss: 0.0262 - val_accuracy: 0.4808 - val_mae: 0.0262 - val_mse: 0.0098\n",
            "Epoch 450/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0181 - accuracy: 0.5000 - mae: 0.0181 - mse: 0.0016\n",
            "Epoch 00450: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0136 - accuracy: 0.4765 - mae: 0.0136 - mse: 0.0020 - val_loss: 0.0267 - val_accuracy: 0.4808 - val_mae: 0.0267 - val_mse: 0.0100\n",
            "Epoch 451/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0294 - accuracy: 0.5938 - mae: 0.0294 - mse: 0.0151\n",
            "Epoch 00451: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0136 - accuracy: 0.4765 - mae: 0.0136 - mse: 0.0020 - val_loss: 0.0260 - val_accuracy: 0.4808 - val_mae: 0.0260 - val_mse: 0.0099\n",
            "Epoch 452/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0117 - accuracy: 0.5000 - mae: 0.0117 - mse: 3.6867e-04\n",
            "Epoch 00452: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0135 - accuracy: 0.4765 - mae: 0.0135 - mse: 0.0020 - val_loss: 0.0288 - val_accuracy: 0.4808 - val_mae: 0.0288 - val_mse: 0.0101\n",
            "Epoch 453/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0338 - accuracy: 0.2500 - mae: 0.0338 - mse: 0.0151\n",
            "Epoch 00453: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0139 - accuracy: 0.4765 - mae: 0.0139 - mse: 0.0020 - val_loss: 0.0261 - val_accuracy: 0.4808 - val_mae: 0.0261 - val_mse: 0.0099\n",
            "Epoch 454/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0210 - accuracy: 0.3438 - mae: 0.0210 - mse: 0.0022\n",
            "Epoch 00454: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0138 - accuracy: 0.4765 - mae: 0.0138 - mse: 0.0020 - val_loss: 0.0260 - val_accuracy: 0.4808 - val_mae: 0.0260 - val_mse: 0.0098\n",
            "Epoch 455/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0114 - accuracy: 0.4375 - mae: 0.0114 - mse: 0.0011\n",
            "Epoch 00455: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0136 - accuracy: 0.4765 - mae: 0.0136 - mse: 0.0020 - val_loss: 0.0260 - val_accuracy: 0.4808 - val_mae: 0.0260 - val_mse: 0.0099\n",
            "Epoch 456/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0046 - accuracy: 0.6875 - mae: 0.0046 - mse: 4.9652e-05\n",
            "Epoch 00456: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0133 - accuracy: 0.4765 - mae: 0.0133 - mse: 0.0020 - val_loss: 0.0259 - val_accuracy: 0.4808 - val_mae: 0.0259 - val_mse: 0.0099\n",
            "Epoch 457/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0191 - accuracy: 0.5625 - mae: 0.0191 - mse: 0.0018\n",
            "Epoch 00457: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0133 - accuracy: 0.4765 - mae: 0.0133 - mse: 0.0019 - val_loss: 0.0268 - val_accuracy: 0.4808 - val_mae: 0.0268 - val_mse: 0.0100\n",
            "Epoch 458/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0120 - accuracy: 0.4688 - mae: 0.0120 - mse: 6.4520e-04\n",
            "Epoch 00458: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0133 - accuracy: 0.4765 - mae: 0.0133 - mse: 0.0019 - val_loss: 0.0258 - val_accuracy: 0.4808 - val_mae: 0.0258 - val_mse: 0.0098\n",
            "Epoch 459/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0040 - accuracy: 0.4062 - mae: 0.0040 - mse: 2.5433e-05\n",
            "Epoch 00459: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0133 - accuracy: 0.4765 - mae: 0.0133 - mse: 0.0019 - val_loss: 0.0266 - val_accuracy: 0.4808 - val_mae: 0.0266 - val_mse: 0.0100\n",
            "Epoch 460/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0116 - accuracy: 0.5312 - mae: 0.0116 - mse: 4.5305e-04\n",
            "Epoch 00460: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0133 - accuracy: 0.4765 - mae: 0.0133 - mse: 0.0020 - val_loss: 0.0258 - val_accuracy: 0.4808 - val_mae: 0.0258 - val_mse: 0.0099\n",
            "Epoch 461/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0085 - accuracy: 0.4062 - mae: 0.0085 - mse: 1.6452e-04\n",
            "Epoch 00461: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0131 - accuracy: 0.4765 - mae: 0.0131 - mse: 0.0019 - val_loss: 0.0256 - val_accuracy: 0.4808 - val_mae: 0.0256 - val_mse: 0.0099\n",
            "Epoch 462/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0118 - accuracy: 0.5938 - mae: 0.0118 - mse: 8.7621e-04\n",
            "Epoch 00462: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0132 - accuracy: 0.4765 - mae: 0.0132 - mse: 0.0019 - val_loss: 0.0269 - val_accuracy: 0.4808 - val_mae: 0.0269 - val_mse: 0.0100\n",
            "Epoch 463/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0128 - accuracy: 0.4062 - mae: 0.0128 - mse: 4.0720e-04\n",
            "Epoch 00463: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0134 - accuracy: 0.4765 - mae: 0.0134 - mse: 0.0020 - val_loss: 0.0255 - val_accuracy: 0.4808 - val_mae: 0.0255 - val_mse: 0.0098\n",
            "Epoch 464/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0196 - accuracy: 0.3125 - mae: 0.0196 - mse: 0.0023\n",
            "Epoch 00464: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0130 - accuracy: 0.4765 - mae: 0.0130 - mse: 0.0019 - val_loss: 0.0260 - val_accuracy: 0.4808 - val_mae: 0.0260 - val_mse: 0.0099\n",
            "Epoch 465/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0088 - accuracy: 0.3750 - mae: 0.0088 - mse: 2.1949e-04\n",
            "Epoch 00465: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0129 - accuracy: 0.4765 - mae: 0.0129 - mse: 0.0019 - val_loss: 0.0253 - val_accuracy: 0.4808 - val_mae: 0.0253 - val_mse: 0.0098\n",
            "Epoch 466/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0116 - accuracy: 0.6250 - mae: 0.0116 - mse: 5.8569e-04\n",
            "Epoch 00466: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0130 - accuracy: 0.4765 - mae: 0.0130 - mse: 0.0019 - val_loss: 0.0264 - val_accuracy: 0.4808 - val_mae: 0.0264 - val_mse: 0.0100\n",
            "Epoch 467/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0344 - accuracy: 0.3125 - mae: 0.0344 - mse: 0.0159\n",
            "Epoch 00467: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0129 - accuracy: 0.4765 - mae: 0.0129 - mse: 0.0019 - val_loss: 0.0254 - val_accuracy: 0.4808 - val_mae: 0.0254 - val_mse: 0.0098\n",
            "Epoch 468/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0096 - accuracy: 0.5000 - mae: 0.0096 - mse: 2.6997e-04\n",
            "Epoch 00468: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0130 - accuracy: 0.4765 - mae: 0.0130 - mse: 0.0019 - val_loss: 0.0252 - val_accuracy: 0.4808 - val_mae: 0.0252 - val_mse: 0.0098\n",
            "Epoch 469/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0280 - accuracy: 0.4375 - mae: 0.0280 - mse: 0.0146\n",
            "Epoch 00469: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0129 - accuracy: 0.4765 - mae: 0.0129 - mse: 0.0019 - val_loss: 0.0263 - val_accuracy: 0.4808 - val_mae: 0.0263 - val_mse: 0.0099\n",
            "Epoch 470/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0082 - accuracy: 0.4688 - mae: 0.0082 - mse: 1.7783e-04\n",
            "Epoch 00470: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0128 - accuracy: 0.4765 - mae: 0.0128 - mse: 0.0019 - val_loss: 0.0252 - val_accuracy: 0.4808 - val_mae: 0.0252 - val_mse: 0.0099\n",
            "Epoch 471/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0103 - accuracy: 0.4375 - mae: 0.0103 - mse: 4.4723e-04\n",
            "Epoch 00471: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0127 - accuracy: 0.4765 - mae: 0.0127 - mse: 0.0019 - val_loss: 0.0265 - val_accuracy: 0.4808 - val_mae: 0.0265 - val_mse: 0.0096\n",
            "Epoch 472/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0086 - accuracy: 0.5000 - mae: 0.0086 - mse: 1.1726e-04\n",
            "Epoch 00472: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0128 - accuracy: 0.4765 - mae: 0.0128 - mse: 0.0019 - val_loss: 0.0251 - val_accuracy: 0.4808 - val_mae: 0.0251 - val_mse: 0.0098\n",
            "Epoch 473/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0194 - accuracy: 0.3438 - mae: 0.0194 - mse: 0.0017\n",
            "Epoch 00473: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0130 - accuracy: 0.4765 - mae: 0.0130 - mse: 0.0019 - val_loss: 0.0250 - val_accuracy: 0.4808 - val_mae: 0.0250 - val_mse: 0.0098\n",
            "Epoch 474/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0151 - accuracy: 0.3750 - mae: 0.0151 - mse: 0.0014\n",
            "Epoch 00474: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0126 - accuracy: 0.4765 - mae: 0.0126 - mse: 0.0019 - val_loss: 0.0249 - val_accuracy: 0.4808 - val_mae: 0.0249 - val_mse: 0.0098\n",
            "Epoch 475/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0168 - accuracy: 0.3125 - mae: 0.0168 - mse: 0.0015\n",
            "Epoch 00475: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0124 - accuracy: 0.4765 - mae: 0.0124 - mse: 0.0019 - val_loss: 0.0275 - val_accuracy: 0.4808 - val_mae: 0.0275 - val_mse: 0.0100\n",
            "Epoch 476/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0110 - accuracy: 0.5000 - mae: 0.0110 - mse: 3.7979e-04\n",
            "Epoch 00476: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0128 - accuracy: 0.4765 - mae: 0.0128 - mse: 0.0019 - val_loss: 0.0248 - val_accuracy: 0.4808 - val_mae: 0.0248 - val_mse: 0.0098\n",
            "Epoch 477/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0098 - accuracy: 0.3438 - mae: 0.0098 - mse: 2.8005e-04\n",
            "Epoch 00477: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0124 - accuracy: 0.4765 - mae: 0.0124 - mse: 0.0019 - val_loss: 0.0258 - val_accuracy: 0.4808 - val_mae: 0.0258 - val_mse: 0.0099\n",
            "Epoch 478/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0063 - accuracy: 0.5000 - mae: 0.0063 - mse: 6.5353e-05\n",
            "Epoch 00478: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0124 - accuracy: 0.4765 - mae: 0.0124 - mse: 0.0019 - val_loss: 0.0283 - val_accuracy: 0.4808 - val_mae: 0.0283 - val_mse: 0.0101\n",
            "Epoch 479/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0092 - accuracy: 0.4688 - mae: 0.0092 - mse: 1.6548e-04\n",
            "Epoch 00479: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0129 - accuracy: 0.4765 - mae: 0.0129 - mse: 0.0019 - val_loss: 0.0255 - val_accuracy: 0.4808 - val_mae: 0.0255 - val_mse: 0.0097\n",
            "Epoch 480/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0144 - accuracy: 0.3750 - mae: 0.0144 - mse: 0.0012\n",
            "Epoch 00480: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0124 - accuracy: 0.4765 - mae: 0.0124 - mse: 0.0019 - val_loss: 0.0248 - val_accuracy: 0.4808 - val_mae: 0.0248 - val_mse: 0.0098\n",
            "Epoch 481/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0195 - accuracy: 0.4688 - mae: 0.0195 - mse: 0.0025\n",
            "Epoch 00481: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0123 - accuracy: 0.4765 - mae: 0.0123 - mse: 0.0019 - val_loss: 0.0249 - val_accuracy: 0.4808 - val_mae: 0.0249 - val_mse: 0.0098\n",
            "Epoch 482/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0074 - accuracy: 0.5625 - mae: 0.0074 - mse: 2.3394e-04\n",
            "Epoch 00482: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0125 - accuracy: 0.4765 - mae: 0.0125 - mse: 0.0019 - val_loss: 0.0251 - val_accuracy: 0.4808 - val_mae: 0.0251 - val_mse: 0.0099\n",
            "Epoch 483/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0077 - accuracy: 0.5000 - mae: 0.0077 - mse: 4.2052e-04\n",
            "Epoch 00483: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0124 - accuracy: 0.4765 - mae: 0.0124 - mse: 0.0019 - val_loss: 0.0248 - val_accuracy: 0.4808 - val_mae: 0.0248 - val_mse: 0.0098\n",
            "Epoch 484/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0134 - accuracy: 0.5312 - mae: 0.0134 - mse: 6.1935e-04\n",
            "Epoch 00484: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0123 - accuracy: 0.4765 - mae: 0.0123 - mse: 0.0019 - val_loss: 0.0252 - val_accuracy: 0.4808 - val_mae: 0.0252 - val_mse: 0.0097\n",
            "Epoch 485/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0068 - accuracy: 0.5625 - mae: 0.0068 - mse: 1.4412e-04\n",
            "Epoch 00485: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0122 - accuracy: 0.4765 - mae: 0.0122 - mse: 0.0019 - val_loss: 0.0248 - val_accuracy: 0.4808 - val_mae: 0.0248 - val_mse: 0.0098\n",
            "Epoch 486/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0104 - accuracy: 0.4688 - mae: 0.0104 - mse: 4.7963e-04\n",
            "Epoch 00486: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0122 - accuracy: 0.4765 - mae: 0.0122 - mse: 0.0019 - val_loss: 0.0257 - val_accuracy: 0.4808 - val_mae: 0.0257 - val_mse: 0.0097\n",
            "Epoch 487/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0129 - accuracy: 0.5000 - mae: 0.0129 - mse: 0.0011\n",
            "Epoch 00487: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0126 - accuracy: 0.4765 - mae: 0.0126 - mse: 0.0019 - val_loss: 0.0256 - val_accuracy: 0.4808 - val_mae: 0.0256 - val_mse: 0.0097\n",
            "Epoch 488/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0106 - accuracy: 0.6562 - mae: 0.0106 - mse: 3.8056e-04\n",
            "Epoch 00488: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0124 - accuracy: 0.4765 - mae: 0.0124 - mse: 0.0019 - val_loss: 0.0262 - val_accuracy: 0.4808 - val_mae: 0.0262 - val_mse: 0.0100\n",
            "Epoch 489/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0152 - accuracy: 0.3438 - mae: 0.0152 - mse: 6.7454e-04\n",
            "Epoch 00489: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0121 - accuracy: 0.4765 - mae: 0.0121 - mse: 0.0019 - val_loss: 0.0253 - val_accuracy: 0.4808 - val_mae: 0.0253 - val_mse: 0.0097\n",
            "Epoch 490/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0111 - accuracy: 0.3750 - mae: 0.0111 - mse: 6.1062e-04\n",
            "Epoch 00490: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0121 - accuracy: 0.4765 - mae: 0.0121 - mse: 0.0019 - val_loss: 0.0247 - val_accuracy: 0.4808 - val_mae: 0.0247 - val_mse: 0.0098\n",
            "Epoch 491/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0083 - accuracy: 0.5312 - mae: 0.0083 - mse: 3.9228e-04\n",
            "Epoch 00491: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0120 - accuracy: 0.4765 - mae: 0.0120 - mse: 0.0019 - val_loss: 0.0251 - val_accuracy: 0.4808 - val_mae: 0.0251 - val_mse: 0.0097\n",
            "Epoch 492/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0138 - accuracy: 0.4062 - mae: 0.0138 - mse: 0.0013\n",
            "Epoch 00492: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0119 - accuracy: 0.4765 - mae: 0.0119 - mse: 0.0019 - val_loss: 0.0253 - val_accuracy: 0.4808 - val_mae: 0.0253 - val_mse: 0.0099\n",
            "Epoch 493/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0081 - accuracy: 0.3750 - mae: 0.0081 - mse: 2.1917e-04\n",
            "Epoch 00493: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0122 - accuracy: 0.4765 - mae: 0.0122 - mse: 0.0019 - val_loss: 0.0248 - val_accuracy: 0.4808 - val_mae: 0.0248 - val_mse: 0.0097\n",
            "Epoch 494/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0116 - accuracy: 0.5625 - mae: 0.0116 - mse: 7.3437e-04\n",
            "Epoch 00494: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0121 - accuracy: 0.4765 - mae: 0.0121 - mse: 0.0019 - val_loss: 0.0256 - val_accuracy: 0.4808 - val_mae: 0.0256 - val_mse: 0.0097\n",
            "Epoch 495/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0089 - accuracy: 0.4062 - mae: 0.0089 - mse: 3.0716e-04\n",
            "Epoch 00495: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0126 - accuracy: 0.4765 - mae: 0.0126 - mse: 0.0019 - val_loss: 0.0247 - val_accuracy: 0.4808 - val_mae: 0.0247 - val_mse: 0.0098\n",
            "Epoch 496/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0272 - accuracy: 0.5312 - mae: 0.0272 - mse: 0.0147\n",
            "Epoch 00496: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0119 - accuracy: 0.4765 - mae: 0.0119 - mse: 0.0019 - val_loss: 0.0251 - val_accuracy: 0.4808 - val_mae: 0.0251 - val_mse: 0.0097\n",
            "Epoch 497/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0063 - accuracy: 0.5625 - mae: 0.0063 - mse: 7.7320e-05\n",
            "Epoch 00497: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0122 - accuracy: 0.4765 - mae: 0.0122 - mse: 0.0019 - val_loss: 0.0247 - val_accuracy: 0.4808 - val_mae: 0.0247 - val_mse: 0.0098\n",
            "Epoch 498/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0091 - accuracy: 0.5000 - mae: 0.0091 - mse: 3.3160e-04\n",
            "Epoch 00498: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0119 - accuracy: 0.4765 - mae: 0.0119 - mse: 0.0019 - val_loss: 0.0261 - val_accuracy: 0.4808 - val_mae: 0.0261 - val_mse: 0.0096\n",
            "Epoch 499/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0202 - accuracy: 0.5625 - mae: 0.0202 - mse: 0.0021\n",
            "Epoch 00499: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0122 - accuracy: 0.4765 - mae: 0.0122 - mse: 0.0019 - val_loss: 0.0268 - val_accuracy: 0.4808 - val_mae: 0.0268 - val_mse: 0.0100\n",
            "Epoch 500/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0086 - accuracy: 0.4062 - mae: 0.0086 - mse: 2.2933e-04\n",
            "Epoch 00500: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0122 - accuracy: 0.4765 - mae: 0.0122 - mse: 0.0019 - val_loss: 0.0247 - val_accuracy: 0.4808 - val_mae: 0.0247 - val_mse: 0.0098\n",
            "Epoch 501/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0086 - accuracy: 0.5625 - mae: 0.0086 - mse: 2.7572e-04\n",
            "Epoch 00501: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0119 - accuracy: 0.4765 - mae: 0.0119 - mse: 0.0019 - val_loss: 0.0247 - val_accuracy: 0.4808 - val_mae: 0.0247 - val_mse: 0.0098\n",
            "Epoch 502/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0140 - accuracy: 0.4688 - mae: 0.0140 - mse: 5.3409e-04\n",
            "Epoch 00502: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0121 - accuracy: 0.4765 - mae: 0.0121 - mse: 0.0019 - val_loss: 0.0253 - val_accuracy: 0.4808 - val_mae: 0.0253 - val_mse: 0.0097\n",
            "Epoch 503/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0083 - accuracy: 0.5625 - mae: 0.0083 - mse: 2.3069e-04\n",
            "Epoch 00503: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0122 - accuracy: 0.4765 - mae: 0.0122 - mse: 0.0019 - val_loss: 0.0254 - val_accuracy: 0.4808 - val_mae: 0.0254 - val_mse: 0.0097\n",
            "Epoch 504/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0161 - accuracy: 0.4375 - mae: 0.0161 - mse: 0.0010\n",
            "Epoch 00504: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0120 - accuracy: 0.4765 - mae: 0.0120 - mse: 0.0019 - val_loss: 0.0250 - val_accuracy: 0.4808 - val_mae: 0.0250 - val_mse: 0.0097\n",
            "Epoch 505/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0099 - accuracy: 0.4688 - mae: 0.0099 - mse: 4.2939e-04\n",
            "Epoch 00505: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0119 - accuracy: 0.4765 - mae: 0.0119 - mse: 0.0019 - val_loss: 0.0251 - val_accuracy: 0.4808 - val_mae: 0.0251 - val_mse: 0.0097\n",
            "Epoch 506/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0114 - accuracy: 0.3750 - mae: 0.0114 - mse: 5.8329e-04\n",
            "Epoch 00506: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0118 - accuracy: 0.4765 - mae: 0.0118 - mse: 0.0019 - val_loss: 0.0247 - val_accuracy: 0.4808 - val_mae: 0.0247 - val_mse: 0.0098\n",
            "Epoch 507/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0078 - accuracy: 0.5625 - mae: 0.0078 - mse: 2.2499e-04\n",
            "Epoch 00507: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0118 - accuracy: 0.4765 - mae: 0.0118 - mse: 0.0019 - val_loss: 0.0248 - val_accuracy: 0.4808 - val_mae: 0.0248 - val_mse: 0.0097\n",
            "Epoch 508/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0058 - accuracy: 0.6250 - mae: 0.0058 - mse: 8.7067e-05\n",
            "Epoch 00508: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0121 - accuracy: 0.4765 - mae: 0.0121 - mse: 0.0019 - val_loss: 0.0248 - val_accuracy: 0.4808 - val_mae: 0.0248 - val_mse: 0.0097\n",
            "Epoch 509/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0071 - accuracy: 0.5625 - mae: 0.0071 - mse: 1.5182e-04\n",
            "Epoch 00509: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0119 - accuracy: 0.4765 - mae: 0.0119 - mse: 0.0019 - val_loss: 0.0266 - val_accuracy: 0.4808 - val_mae: 0.0266 - val_mse: 0.0099\n",
            "Epoch 510/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0169 - accuracy: 0.3750 - mae: 0.0169 - mse: 0.0014\n",
            "Epoch 00510: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0122 - accuracy: 0.4765 - mae: 0.0122 - mse: 0.0019 - val_loss: 0.0251 - val_accuracy: 0.4808 - val_mae: 0.0251 - val_mse: 0.0097\n",
            "Epoch 511/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0274 - accuracy: 0.5312 - mae: 0.0274 - mse: 0.0145\n",
            "Epoch 00511: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0120 - accuracy: 0.4765 - mae: 0.0120 - mse: 0.0019 - val_loss: 0.0248 - val_accuracy: 0.4808 - val_mae: 0.0248 - val_mse: 0.0097\n",
            "Epoch 512/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0111 - accuracy: 0.5000 - mae: 0.0111 - mse: 7.2156e-04\n",
            "Epoch 00512: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0119 - accuracy: 0.4765 - mae: 0.0119 - mse: 0.0019 - val_loss: 0.0258 - val_accuracy: 0.4808 - val_mae: 0.0258 - val_mse: 0.0096\n",
            "Epoch 513/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0073 - accuracy: 0.5000 - mae: 0.0073 - mse: 9.4000e-05\n",
            "Epoch 00513: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0120 - accuracy: 0.4765 - mae: 0.0120 - mse: 0.0019 - val_loss: 0.0247 - val_accuracy: 0.4808 - val_mae: 0.0247 - val_mse: 0.0098\n",
            "Epoch 514/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0135 - accuracy: 0.3750 - mae: 0.0135 - mse: 0.0013\n",
            "Epoch 00514: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0119 - accuracy: 0.4765 - mae: 0.0119 - mse: 0.0019 - val_loss: 0.0249 - val_accuracy: 0.4808 - val_mae: 0.0249 - val_mse: 0.0097\n",
            "Epoch 515/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0061 - accuracy: 0.5000 - mae: 0.0061 - mse: 1.4112e-04\n",
            "Epoch 00515: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0119 - accuracy: 0.4765 - mae: 0.0119 - mse: 0.0019 - val_loss: 0.0253 - val_accuracy: 0.4808 - val_mae: 0.0253 - val_mse: 0.0098\n",
            "Epoch 516/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0066 - accuracy: 0.4688 - mae: 0.0066 - mse: 1.8303e-04\n",
            "Epoch 00516: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0120 - accuracy: 0.4765 - mae: 0.0120 - mse: 0.0019 - val_loss: 0.0249 - val_accuracy: 0.4808 - val_mae: 0.0249 - val_mse: 0.0097\n",
            "Epoch 517/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0078 - accuracy: 0.4688 - mae: 0.0078 - mse: 2.6373e-04\n",
            "Epoch 00517: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0119 - accuracy: 0.4765 - mae: 0.0119 - mse: 0.0019 - val_loss: 0.0252 - val_accuracy: 0.4808 - val_mae: 0.0252 - val_mse: 0.0097\n",
            "Epoch 518/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0106 - accuracy: 0.4375 - mae: 0.0106 - mse: 7.3711e-04\n",
            "Epoch 00518: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0117 - accuracy: 0.4765 - mae: 0.0117 - mse: 0.0019 - val_loss: 0.0247 - val_accuracy: 0.4808 - val_mae: 0.0247 - val_mse: 0.0098\n",
            "Epoch 519/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0282 - accuracy: 0.3125 - mae: 0.0282 - mse: 0.0147\n",
            "Epoch 00519: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0118 - accuracy: 0.4765 - mae: 0.0118 - mse: 0.0019 - val_loss: 0.0248 - val_accuracy: 0.4808 - val_mae: 0.0248 - val_mse: 0.0098\n",
            "Epoch 520/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0153 - accuracy: 0.5000 - mae: 0.0153 - mse: 0.0014\n",
            "Epoch 00520: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0121 - accuracy: 0.4765 - mae: 0.0121 - mse: 0.0019 - val_loss: 0.0246 - val_accuracy: 0.4808 - val_mae: 0.0246 - val_mse: 0.0097\n",
            "Epoch 521/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0327 - accuracy: 0.3438 - mae: 0.0327 - mse: 0.0149\n",
            "Epoch 00521: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0120 - accuracy: 0.4765 - mae: 0.0120 - mse: 0.0019 - val_loss: 0.0254 - val_accuracy: 0.4808 - val_mae: 0.0254 - val_mse: 0.0096\n",
            "Epoch 522/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0051 - accuracy: 0.5312 - mae: 0.0051 - mse: 4.8729e-05\n",
            "Epoch 00522: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0119 - accuracy: 0.4765 - mae: 0.0119 - mse: 0.0019 - val_loss: 0.0250 - val_accuracy: 0.4808 - val_mae: 0.0250 - val_mse: 0.0097\n",
            "Epoch 523/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0088 - accuracy: 0.5625 - mae: 0.0088 - mse: 6.1945e-04\n",
            "Epoch 00523: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0119 - accuracy: 0.4765 - mae: 0.0119 - mse: 0.0019 - val_loss: 0.0252 - val_accuracy: 0.4808 - val_mae: 0.0252 - val_mse: 0.0096\n",
            "Epoch 524/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0162 - accuracy: 0.4062 - mae: 0.0162 - mse: 0.0020\n",
            "Epoch 00524: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0121 - accuracy: 0.4765 - mae: 0.0121 - mse: 0.0019 - val_loss: 0.0249 - val_accuracy: 0.4808 - val_mae: 0.0249 - val_mse: 0.0097\n",
            "Epoch 525/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0331 - accuracy: 0.5312 - mae: 0.0331 - mse: 0.0152\n",
            "Epoch 00525: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0117 - accuracy: 0.4765 - mae: 0.0117 - mse: 0.0019 - val_loss: 0.0248 - val_accuracy: 0.4808 - val_mae: 0.0248 - val_mse: 0.0097\n",
            "Epoch 526/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0094 - accuracy: 0.5312 - mae: 0.0094 - mse: 2.9061e-04\n",
            "Epoch 00526: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0119 - accuracy: 0.4765 - mae: 0.0119 - mse: 0.0019 - val_loss: 0.0251 - val_accuracy: 0.4808 - val_mae: 0.0251 - val_mse: 0.0097\n",
            "Epoch 527/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0078 - accuracy: 0.4062 - mae: 0.0078 - mse: 2.5448e-04\n",
            "Epoch 00527: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0120 - accuracy: 0.4765 - mae: 0.0120 - mse: 0.0019 - val_loss: 0.0246 - val_accuracy: 0.4808 - val_mae: 0.0246 - val_mse: 0.0097\n",
            "Epoch 528/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0065 - accuracy: 0.4688 - mae: 0.0065 - mse: 1.1984e-04\n",
            "Epoch 00528: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0122 - accuracy: 0.4765 - mae: 0.0122 - mse: 0.0019 - val_loss: 0.0261 - val_accuracy: 0.4808 - val_mae: 0.0261 - val_mse: 0.0099\n",
            "Epoch 529/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0105 - accuracy: 0.6250 - mae: 0.0105 - mse: 3.8995e-04\n",
            "Epoch 00529: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0121 - accuracy: 0.4765 - mae: 0.0121 - mse: 0.0019 - val_loss: 0.0246 - val_accuracy: 0.4808 - val_mae: 0.0246 - val_mse: 0.0098\n",
            "Epoch 530/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0177 - accuracy: 0.5000 - mae: 0.0177 - mse: 0.0015\n",
            "Epoch 00530: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0119 - accuracy: 0.4765 - mae: 0.0119 - mse: 0.0019 - val_loss: 0.0248 - val_accuracy: 0.4808 - val_mae: 0.0248 - val_mse: 0.0098\n",
            "Epoch 531/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0107 - accuracy: 0.5000 - mae: 0.0107 - mse: 3.8282e-04\n",
            "Epoch 00531: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0117 - accuracy: 0.4765 - mae: 0.0117 - mse: 0.0019 - val_loss: 0.0246 - val_accuracy: 0.4808 - val_mae: 0.0246 - val_mse: 0.0097\n",
            "Epoch 532/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0066 - accuracy: 0.4375 - mae: 0.0066 - mse: 1.8307e-04\n",
            "Epoch 00532: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0118 - accuracy: 0.4765 - mae: 0.0118 - mse: 0.0019 - val_loss: 0.0247 - val_accuracy: 0.4808 - val_mae: 0.0247 - val_mse: 0.0097\n",
            "Epoch 533/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0146 - accuracy: 0.4375 - mae: 0.0146 - mse: 0.0014\n",
            "Epoch 00533: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0120 - accuracy: 0.4765 - mae: 0.0120 - mse: 0.0019 - val_loss: 0.0246 - val_accuracy: 0.4808 - val_mae: 0.0246 - val_mse: 0.0097\n",
            "Epoch 534/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0077 - accuracy: 0.5000 - mae: 0.0077 - mse: 2.4984e-04\n",
            "Epoch 00534: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0116 - accuracy: 0.4765 - mae: 0.0116 - mse: 0.0019 - val_loss: 0.0251 - val_accuracy: 0.4808 - val_mae: 0.0251 - val_mse: 0.0096\n",
            "Epoch 535/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0102 - accuracy: 0.4375 - mae: 0.0102 - mse: 3.7918e-04\n",
            "Epoch 00535: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0118 - accuracy: 0.4765 - mae: 0.0118 - mse: 0.0019 - val_loss: 0.0246 - val_accuracy: 0.4808 - val_mae: 0.0246 - val_mse: 0.0097\n",
            "Epoch 536/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0094 - accuracy: 0.4688 - mae: 0.0094 - mse: 4.4119e-04\n",
            "Epoch 00536: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0118 - accuracy: 0.4765 - mae: 0.0118 - mse: 0.0019 - val_loss: 0.0246 - val_accuracy: 0.4808 - val_mae: 0.0246 - val_mse: 0.0097\n",
            "Epoch 537/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0040 - accuracy: 0.4688 - mae: 0.0040 - mse: 5.0925e-05\n",
            "Epoch 00537: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0117 - accuracy: 0.4765 - mae: 0.0117 - mse: 0.0019 - val_loss: 0.0249 - val_accuracy: 0.4808 - val_mae: 0.0249 - val_mse: 0.0098\n",
            "Epoch 538/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0425 - accuracy: 0.4688 - mae: 0.0425 - mse: 0.0163\n",
            "Epoch 00538: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0121 - accuracy: 0.4765 - mae: 0.0121 - mse: 0.0019 - val_loss: 0.0254 - val_accuracy: 0.4808 - val_mae: 0.0254 - val_mse: 0.0098\n",
            "Epoch 539/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0050 - accuracy: 0.5625 - mae: 0.0050 - mse: 5.9704e-05\n",
            "Epoch 00539: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0119 - accuracy: 0.4765 - mae: 0.0119 - mse: 0.0019 - val_loss: 0.0253 - val_accuracy: 0.4808 - val_mae: 0.0253 - val_mse: 0.0096\n",
            "Epoch 540/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0081 - accuracy: 0.3750 - mae: 0.0081 - mse: 3.0087e-04\n",
            "Epoch 00540: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0118 - accuracy: 0.4765 - mae: 0.0118 - mse: 0.0019 - val_loss: 0.0249 - val_accuracy: 0.4808 - val_mae: 0.0249 - val_mse: 0.0096\n",
            "Epoch 541/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0079 - accuracy: 0.4688 - mae: 0.0079 - mse: 2.8641e-04\n",
            "Epoch 00541: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0117 - accuracy: 0.4765 - mae: 0.0117 - mse: 0.0019 - val_loss: 0.0252 - val_accuracy: 0.4808 - val_mae: 0.0252 - val_mse: 0.0098\n",
            "Epoch 542/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0118 - accuracy: 0.5312 - mae: 0.0118 - mse: 4.0399e-04\n",
            "Epoch 00542: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0119 - accuracy: 0.4765 - mae: 0.0119 - mse: 0.0019 - val_loss: 0.0251 - val_accuracy: 0.4808 - val_mae: 0.0251 - val_mse: 0.0096\n",
            "Epoch 543/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0164 - accuracy: 0.3750 - mae: 0.0164 - mse: 0.0015\n",
            "Epoch 00543: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0118 - accuracy: 0.4765 - mae: 0.0118 - mse: 0.0019 - val_loss: 0.0251 - val_accuracy: 0.4808 - val_mae: 0.0251 - val_mse: 0.0098\n",
            "Epoch 544/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0106 - accuracy: 0.5312 - mae: 0.0106 - mse: 4.9701e-04\n",
            "Epoch 00544: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0118 - accuracy: 0.4765 - mae: 0.0118 - mse: 0.0019 - val_loss: 0.0246 - val_accuracy: 0.4808 - val_mae: 0.0246 - val_mse: 0.0097\n",
            "Epoch 545/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0094 - accuracy: 0.5312 - mae: 0.0094 - mse: 3.0817e-04\n",
            "Epoch 00545: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0118 - accuracy: 0.4765 - mae: 0.0118 - mse: 0.0019 - val_loss: 0.0252 - val_accuracy: 0.4808 - val_mae: 0.0252 - val_mse: 0.0096\n",
            "Epoch 546/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0077 - accuracy: 0.4688 - mae: 0.0077 - mse: 2.0932e-04\n",
            "Epoch 00546: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0117 - accuracy: 0.4765 - mae: 0.0117 - mse: 0.0019 - val_loss: 0.0253 - val_accuracy: 0.4808 - val_mae: 0.0253 - val_mse: 0.0098\n",
            "Epoch 547/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0087 - accuracy: 0.5312 - mae: 0.0087 - mse: 2.2591e-04\n",
            "Epoch 00547: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0119 - accuracy: 0.4765 - mae: 0.0119 - mse: 0.0019 - val_loss: 0.0248 - val_accuracy: 0.4808 - val_mae: 0.0248 - val_mse: 0.0096\n",
            "Epoch 548/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0198 - accuracy: 0.3438 - mae: 0.0198 - mse: 0.0017\n",
            "Epoch 00548: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0120 - accuracy: 0.4765 - mae: 0.0120 - mse: 0.0019 - val_loss: 0.0248 - val_accuracy: 0.4808 - val_mae: 0.0248 - val_mse: 0.0096\n",
            "Epoch 549/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0123 - accuracy: 0.4062 - mae: 0.0123 - mse: 8.5309e-04\n",
            "Epoch 00549: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0116 - accuracy: 0.4765 - mae: 0.0116 - mse: 0.0019 - val_loss: 0.0261 - val_accuracy: 0.4808 - val_mae: 0.0261 - val_mse: 0.0095\n",
            "Epoch 550/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0135 - accuracy: 0.4688 - mae: 0.0135 - mse: 5.7354e-04\n",
            "Epoch 00550: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0118 - accuracy: 0.4765 - mae: 0.0118 - mse: 0.0019 - val_loss: 0.0251 - val_accuracy: 0.4808 - val_mae: 0.0251 - val_mse: 0.0096\n",
            "Epoch 551/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0117 - accuracy: 0.4688 - mae: 0.0117 - mse: 0.0011\n",
            "Epoch 00551: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0118 - accuracy: 0.4765 - mae: 0.0118 - mse: 0.0019 - val_loss: 0.0260 - val_accuracy: 0.4808 - val_mae: 0.0260 - val_mse: 0.0099\n",
            "Epoch 552/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0149 - accuracy: 0.5312 - mae: 0.0149 - mse: 0.0013\n",
            "Epoch 00552: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0120 - accuracy: 0.4765 - mae: 0.0120 - mse: 0.0019 - val_loss: 0.0251 - val_accuracy: 0.4808 - val_mae: 0.0251 - val_mse: 0.0098\n",
            "Epoch 553/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0285 - accuracy: 0.5000 - mae: 0.0285 - mse: 0.0149\n",
            "Epoch 00553: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0120 - accuracy: 0.4765 - mae: 0.0120 - mse: 0.0019 - val_loss: 0.0253 - val_accuracy: 0.4808 - val_mae: 0.0253 - val_mse: 0.0096\n",
            "Epoch 554/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0075 - accuracy: 0.4688 - mae: 0.0075 - mse: 1.1391e-04\n",
            "Epoch 00554: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0119 - accuracy: 0.4765 - mae: 0.0119 - mse: 0.0018 - val_loss: 0.0253 - val_accuracy: 0.4808 - val_mae: 0.0253 - val_mse: 0.0096\n",
            "Epoch 555/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0089 - accuracy: 0.5625 - mae: 0.0089 - mse: 1.8224e-04\n",
            "Epoch 00555: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0117 - accuracy: 0.4765 - mae: 0.0117 - mse: 0.0019 - val_loss: 0.0247 - val_accuracy: 0.4808 - val_mae: 0.0247 - val_mse: 0.0096\n",
            "Epoch 556/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0054 - accuracy: 0.4062 - mae: 0.0054 - mse: 7.8185e-05\n",
            "Epoch 00556: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0117 - accuracy: 0.4765 - mae: 0.0117 - mse: 0.0019 - val_loss: 0.0246 - val_accuracy: 0.4808 - val_mae: 0.0246 - val_mse: 0.0097\n",
            "Epoch 557/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0117 - accuracy: 0.4062 - mae: 0.0117 - mse: 4.9704e-04\n",
            "Epoch 00557: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0120 - accuracy: 0.4765 - mae: 0.0120 - mse: 0.0019 - val_loss: 0.0249 - val_accuracy: 0.4808 - val_mae: 0.0249 - val_mse: 0.0096\n",
            "Epoch 558/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0279 - accuracy: 0.5000 - mae: 0.0279 - mse: 0.0145\n",
            "Epoch 00558: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0117 - accuracy: 0.4765 - mae: 0.0117 - mse: 0.0019 - val_loss: 0.0251 - val_accuracy: 0.4808 - val_mae: 0.0251 - val_mse: 0.0096\n",
            "Epoch 559/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0070 - accuracy: 0.4375 - mae: 0.0070 - mse: 2.4031e-04\n",
            "Epoch 00559: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0118 - accuracy: 0.4765 - mae: 0.0118 - mse: 0.0018 - val_loss: 0.0246 - val_accuracy: 0.4808 - val_mae: 0.0246 - val_mse: 0.0097\n",
            "Epoch 560/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0089 - accuracy: 0.5000 - mae: 0.0089 - mse: 8.0280e-04\n",
            "Epoch 00560: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0117 - accuracy: 0.4765 - mae: 0.0117 - mse: 0.0019 - val_loss: 0.0250 - val_accuracy: 0.4808 - val_mae: 0.0250 - val_mse: 0.0096\n",
            "Epoch 561/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0080 - accuracy: 0.5000 - mae: 0.0080 - mse: 2.5478e-04\n",
            "Epoch 00561: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0118 - accuracy: 0.4765 - mae: 0.0118 - mse: 0.0019 - val_loss: 0.0258 - val_accuracy: 0.4808 - val_mae: 0.0258 - val_mse: 0.0095\n",
            "Epoch 562/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0174 - accuracy: 0.4688 - mae: 0.0174 - mse: 0.0018\n",
            "Epoch 00562: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0120 - accuracy: 0.4765 - mae: 0.0120 - mse: 0.0019 - val_loss: 0.0247 - val_accuracy: 0.4808 - val_mae: 0.0247 - val_mse: 0.0096\n",
            "Epoch 563/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0052 - accuracy: 0.4688 - mae: 0.0052 - mse: 8.0064e-05\n",
            "Epoch 00563: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0118 - accuracy: 0.4765 - mae: 0.0118 - mse: 0.0019 - val_loss: 0.0252 - val_accuracy: 0.4808 - val_mae: 0.0252 - val_mse: 0.0096\n",
            "Epoch 564/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0273 - accuracy: 0.4062 - mae: 0.0273 - mse: 0.0145\n",
            "Epoch 00564: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0118 - accuracy: 0.4765 - mae: 0.0118 - mse: 0.0019 - val_loss: 0.0249 - val_accuracy: 0.4808 - val_mae: 0.0249 - val_mse: 0.0096\n",
            "Epoch 565/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0041 - accuracy: 0.4375 - mae: 0.0041 - mse: 3.1641e-05\n",
            "Epoch 00565: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0117 - accuracy: 0.4765 - mae: 0.0117 - mse: 0.0019 - val_loss: 0.0248 - val_accuracy: 0.4808 - val_mae: 0.0248 - val_mse: 0.0097\n",
            "Epoch 566/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0117 - accuracy: 0.5625 - mae: 0.0117 - mse: 0.0013\n",
            "Epoch 00566: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0117 - accuracy: 0.4765 - mae: 0.0117 - mse: 0.0019 - val_loss: 0.0246 - val_accuracy: 0.4808 - val_mae: 0.0246 - val_mse: 0.0096\n",
            "Epoch 567/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0124 - accuracy: 0.5000 - mae: 0.0124 - mse: 6.9889e-04\n",
            "Epoch 00567: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0116 - accuracy: 0.4765 - mae: 0.0116 - mse: 0.0019 - val_loss: 0.0248 - val_accuracy: 0.4808 - val_mae: 0.0248 - val_mse: 0.0096\n",
            "Epoch 568/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0401 - accuracy: 0.3750 - mae: 0.0401 - mse: 0.0159\n",
            "Epoch 00568: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0117 - accuracy: 0.4765 - mae: 0.0117 - mse: 0.0019 - val_loss: 0.0255 - val_accuracy: 0.4808 - val_mae: 0.0255 - val_mse: 0.0098\n",
            "Epoch 569/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0084 - accuracy: 0.6250 - mae: 0.0084 - mse: 2.1332e-04\n",
            "Epoch 00569: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0119 - accuracy: 0.4765 - mae: 0.0119 - mse: 0.0019 - val_loss: 0.0258 - val_accuracy: 0.4808 - val_mae: 0.0258 - val_mse: 0.0095\n",
            "Epoch 570/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0107 - accuracy: 0.4688 - mae: 0.0107 - mse: 3.5103e-04\n",
            "Epoch 00570: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0117 - accuracy: 0.4765 - mae: 0.0117 - mse: 0.0019 - val_loss: 0.0262 - val_accuracy: 0.4808 - val_mae: 0.0262 - val_mse: 0.0095\n",
            "Epoch 571/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0324 - accuracy: 0.5312 - mae: 0.0324 - mse: 0.0145\n",
            "Epoch 00571: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0120 - accuracy: 0.4765 - mae: 0.0120 - mse: 0.0018 - val_loss: 0.0251 - val_accuracy: 0.4808 - val_mae: 0.0251 - val_mse: 0.0096\n",
            "Epoch 572/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0139 - accuracy: 0.4375 - mae: 0.0139 - mse: 0.0011\n",
            "Epoch 00572: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0117 - accuracy: 0.4765 - mae: 0.0117 - mse: 0.0019 - val_loss: 0.0251 - val_accuracy: 0.4808 - val_mae: 0.0251 - val_mse: 0.0095\n",
            "Epoch 573/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0120 - accuracy: 0.4688 - mae: 0.0120 - mse: 0.0011\n",
            "Epoch 00573: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0119 - accuracy: 0.4765 - mae: 0.0119 - mse: 0.0019 - val_loss: 0.0249 - val_accuracy: 0.4808 - val_mae: 0.0249 - val_mse: 0.0096\n",
            "Epoch 574/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0062 - accuracy: 0.4375 - mae: 0.0062 - mse: 1.6123e-04\n",
            "Epoch 00574: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0116 - accuracy: 0.4765 - mae: 0.0116 - mse: 0.0019 - val_loss: 0.0244 - val_accuracy: 0.4808 - val_mae: 0.0244 - val_mse: 0.0097\n",
            "Epoch 575/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0102 - accuracy: 0.5625 - mae: 0.0102 - mse: 0.0011\n",
            "Epoch 00575: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0118 - accuracy: 0.4765 - mae: 0.0118 - mse: 0.0019 - val_loss: 0.0264 - val_accuracy: 0.4808 - val_mae: 0.0264 - val_mse: 0.0099\n",
            "Epoch 576/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0190 - accuracy: 0.5312 - mae: 0.0190 - mse: 0.0023\n",
            "Epoch 00576: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0118 - accuracy: 0.4765 - mae: 0.0118 - mse: 0.0019 - val_loss: 0.0245 - val_accuracy: 0.4808 - val_mae: 0.0245 - val_mse: 0.0097\n",
            "Epoch 577/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0096 - accuracy: 0.5625 - mae: 0.0096 - mse: 0.0011\n",
            "Epoch 00577: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0117 - accuracy: 0.4765 - mae: 0.0117 - mse: 0.0019 - val_loss: 0.0248 - val_accuracy: 0.4808 - val_mae: 0.0248 - val_mse: 0.0096\n",
            "Epoch 578/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0111 - accuracy: 0.4062 - mae: 0.0111 - mse: 8.2332e-04\n",
            "Epoch 00578: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0116 - accuracy: 0.4765 - mae: 0.0116 - mse: 0.0018 - val_loss: 0.0245 - val_accuracy: 0.4808 - val_mae: 0.0245 - val_mse: 0.0097\n",
            "Epoch 579/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0066 - accuracy: 0.5000 - mae: 0.0066 - mse: 3.6383e-04\n",
            "Epoch 00579: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0118 - accuracy: 0.4765 - mae: 0.0118 - mse: 0.0019 - val_loss: 0.0244 - val_accuracy: 0.4808 - val_mae: 0.0244 - val_mse: 0.0096\n",
            "Epoch 580/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0113 - accuracy: 0.5625 - mae: 0.0113 - mse: 0.0011\n",
            "Epoch 00580: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0117 - accuracy: 0.4765 - mae: 0.0117 - mse: 0.0019 - val_loss: 0.0256 - val_accuracy: 0.4808 - val_mae: 0.0256 - val_mse: 0.0095\n",
            "Epoch 581/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0165 - accuracy: 0.4375 - mae: 0.0165 - mse: 0.0013\n",
            "Epoch 00581: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0118 - accuracy: 0.4765 - mae: 0.0118 - mse: 0.0019 - val_loss: 0.0245 - val_accuracy: 0.4808 - val_mae: 0.0245 - val_mse: 0.0096\n",
            "Epoch 582/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0082 - accuracy: 0.4375 - mae: 0.0082 - mse: 2.3551e-04\n",
            "Epoch 00582: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0115 - accuracy: 0.4765 - mae: 0.0115 - mse: 0.0019 - val_loss: 0.0245 - val_accuracy: 0.4808 - val_mae: 0.0245 - val_mse: 0.0096\n",
            "Epoch 583/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0071 - accuracy: 0.4375 - mae: 0.0071 - mse: 2.5241e-04\n",
            "Epoch 00583: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0118 - accuracy: 0.4765 - mae: 0.0118 - mse: 0.0019 - val_loss: 0.0251 - val_accuracy: 0.4808 - val_mae: 0.0251 - val_mse: 0.0095\n",
            "Epoch 584/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0144 - accuracy: 0.5312 - mae: 0.0144 - mse: 0.0011\n",
            "Epoch 00584: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0116 - accuracy: 0.4765 - mae: 0.0116 - mse: 0.0019 - val_loss: 0.0255 - val_accuracy: 0.4808 - val_mae: 0.0255 - val_mse: 0.0098\n",
            "Epoch 585/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0128 - accuracy: 0.3125 - mae: 0.0128 - mse: 8.7878e-04\n",
            "Epoch 00585: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0117 - accuracy: 0.4765 - mae: 0.0117 - mse: 0.0019 - val_loss: 0.0248 - val_accuracy: 0.4808 - val_mae: 0.0248 - val_mse: 0.0096\n",
            "Epoch 586/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0034 - accuracy: 0.4688 - mae: 0.0034 - mse: 1.6879e-05\n",
            "Epoch 00586: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0116 - accuracy: 0.4765 - mae: 0.0116 - mse: 0.0019 - val_loss: 0.0248 - val_accuracy: 0.4808 - val_mae: 0.0248 - val_mse: 0.0097\n",
            "Epoch 587/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0102 - accuracy: 0.4062 - mae: 0.0102 - mse: 4.0831e-04\n",
            "Epoch 00587: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0116 - accuracy: 0.4765 - mae: 0.0116 - mse: 0.0019 - val_loss: 0.0250 - val_accuracy: 0.4808 - val_mae: 0.0250 - val_mse: 0.0095\n",
            "Epoch 588/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0109 - accuracy: 0.5938 - mae: 0.0109 - mse: 4.3409e-04\n",
            "Epoch 00588: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0115 - accuracy: 0.4765 - mae: 0.0115 - mse: 0.0018 - val_loss: 0.0244 - val_accuracy: 0.4808 - val_mae: 0.0244 - val_mse: 0.0097\n",
            "Epoch 589/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0133 - accuracy: 0.4375 - mae: 0.0133 - mse: 6.9444e-04\n",
            "Epoch 00589: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0117 - accuracy: 0.4765 - mae: 0.0117 - mse: 0.0018 - val_loss: 0.0250 - val_accuracy: 0.4808 - val_mae: 0.0250 - val_mse: 0.0097\n",
            "Epoch 590/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0093 - accuracy: 0.3750 - mae: 0.0093 - mse: 2.4694e-04\n",
            "Epoch 00590: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0121 - accuracy: 0.4765 - mae: 0.0121 - mse: 0.0019 - val_loss: 0.0250 - val_accuracy: 0.4808 - val_mae: 0.0250 - val_mse: 0.0098\n",
            "Epoch 591/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0150 - accuracy: 0.5000 - mae: 0.0150 - mse: 0.0011\n",
            "Epoch 00591: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0116 - accuracy: 0.4765 - mae: 0.0116 - mse: 0.0019 - val_loss: 0.0255 - val_accuracy: 0.4808 - val_mae: 0.0255 - val_mse: 0.0095\n",
            "Epoch 592/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0215 - accuracy: 0.5000 - mae: 0.0215 - mse: 0.0017\n",
            "Epoch 00592: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0117 - accuracy: 0.4765 - mae: 0.0117 - mse: 0.0018 - val_loss: 0.0244 - val_accuracy: 0.4808 - val_mae: 0.0244 - val_mse: 0.0097\n",
            "Epoch 593/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0097 - accuracy: 0.4688 - mae: 0.0097 - mse: 5.0285e-04\n",
            "Epoch 00593: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0119 - accuracy: 0.4765 - mae: 0.0119 - mse: 0.0019 - val_loss: 0.0247 - val_accuracy: 0.4808 - val_mae: 0.0247 - val_mse: 0.0097\n",
            "Epoch 594/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0111 - accuracy: 0.3438 - mae: 0.0111 - mse: 4.6842e-04\n",
            "Epoch 00594: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0118 - accuracy: 0.4765 - mae: 0.0118 - mse: 0.0019 - val_loss: 0.0251 - val_accuracy: 0.4808 - val_mae: 0.0251 - val_mse: 0.0095\n",
            "Epoch 595/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0074 - accuracy: 0.5312 - mae: 0.0074 - mse: 1.8172e-04\n",
            "Epoch 00595: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0117 - accuracy: 0.4765 - mae: 0.0117 - mse: 0.0019 - val_loss: 0.0250 - val_accuracy: 0.4808 - val_mae: 0.0250 - val_mse: 0.0095\n",
            "Epoch 596/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0058 - accuracy: 0.5312 - mae: 0.0058 - mse: 9.1725e-05\n",
            "Epoch 00596: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0119 - accuracy: 0.4765 - mae: 0.0119 - mse: 0.0018 - val_loss: 0.0265 - val_accuracy: 0.4808 - val_mae: 0.0265 - val_mse: 0.0094\n",
            "Epoch 597/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0246 - accuracy: 0.5625 - mae: 0.0246 - mse: 0.0022\n",
            "Epoch 00597: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0118 - accuracy: 0.4765 - mae: 0.0118 - mse: 0.0018 - val_loss: 0.0245 - val_accuracy: 0.4808 - val_mae: 0.0245 - val_mse: 0.0097\n",
            "Epoch 598/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0045 - accuracy: 0.5625 - mae: 0.0045 - mse: 7.0853e-05\n",
            "Epoch 00598: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0116 - accuracy: 0.4765 - mae: 0.0116 - mse: 0.0018 - val_loss: 0.0243 - val_accuracy: 0.4808 - val_mae: 0.0243 - val_mse: 0.0096\n",
            "Epoch 599/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0068 - accuracy: 0.5312 - mae: 0.0068 - mse: 1.6769e-04\n",
            "Epoch 00599: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0114 - accuracy: 0.4765 - mae: 0.0114 - mse: 0.0019 - val_loss: 0.0244 - val_accuracy: 0.4808 - val_mae: 0.0244 - val_mse: 0.0096\n",
            "Epoch 600/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0110 - accuracy: 0.4062 - mae: 0.0110 - mse: 5.0966e-04\n",
            "Epoch 00600: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0117 - accuracy: 0.4765 - mae: 0.0117 - mse: 0.0018 - val_loss: 0.0243 - val_accuracy: 0.4808 - val_mae: 0.0243 - val_mse: 0.0096\n",
            "Epoch 601/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0089 - accuracy: 0.6250 - mae: 0.0089 - mse: 2.5912e-04\n",
            "Epoch 00601: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0116 - accuracy: 0.4765 - mae: 0.0116 - mse: 0.0018 - val_loss: 0.0243 - val_accuracy: 0.4808 - val_mae: 0.0243 - val_mse: 0.0096\n",
            "Epoch 602/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0107 - accuracy: 0.5312 - mae: 0.0107 - mse: 8.3633e-04\n",
            "Epoch 00602: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0116 - accuracy: 0.4765 - mae: 0.0116 - mse: 0.0019 - val_loss: 0.0264 - val_accuracy: 0.4808 - val_mae: 0.0264 - val_mse: 0.0094\n",
            "Epoch 603/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0090 - accuracy: 0.4688 - mae: 0.0090 - mse: 1.4253e-04\n",
            "Epoch 00603: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0118 - accuracy: 0.4765 - mae: 0.0118 - mse: 0.0019 - val_loss: 0.0251 - val_accuracy: 0.4808 - val_mae: 0.0251 - val_mse: 0.0095\n",
            "Epoch 604/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0076 - accuracy: 0.4688 - mae: 0.0076 - mse: 1.8090e-04\n",
            "Epoch 00604: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0118 - accuracy: 0.4765 - mae: 0.0118 - mse: 0.0019 - val_loss: 0.0243 - val_accuracy: 0.4808 - val_mae: 0.0243 - val_mse: 0.0096\n",
            "Epoch 605/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0074 - accuracy: 0.6250 - mae: 0.0074 - mse: 3.3418e-04\n",
            "Epoch 00605: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0117 - accuracy: 0.4765 - mae: 0.0117 - mse: 0.0018 - val_loss: 0.0252 - val_accuracy: 0.4808 - val_mae: 0.0252 - val_mse: 0.0097\n",
            "Epoch 606/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0070 - accuracy: 0.5000 - mae: 0.0070 - mse: 1.5996e-04\n",
            "Epoch 00606: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0116 - accuracy: 0.4765 - mae: 0.0116 - mse: 0.0019 - val_loss: 0.0243 - val_accuracy: 0.4808 - val_mae: 0.0243 - val_mse: 0.0096\n",
            "Epoch 607/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0118 - accuracy: 0.5938 - mae: 0.0118 - mse: 0.0012\n",
            "Epoch 00607: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0116 - accuracy: 0.4765 - mae: 0.0116 - mse: 0.0019 - val_loss: 0.0242 - val_accuracy: 0.4808 - val_mae: 0.0242 - val_mse: 0.0097\n",
            "Epoch 608/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0134 - accuracy: 0.5000 - mae: 0.0134 - mse: 7.0675e-04\n",
            "Epoch 00608: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0119 - accuracy: 0.4765 - mae: 0.0119 - mse: 0.0019 - val_loss: 0.0244 - val_accuracy: 0.4808 - val_mae: 0.0244 - val_mse: 0.0097\n",
            "Epoch 609/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0050 - accuracy: 0.4688 - mae: 0.0050 - mse: 7.0131e-05\n",
            "Epoch 00609: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0119 - accuracy: 0.4765 - mae: 0.0119 - mse: 0.0019 - val_loss: 0.0245 - val_accuracy: 0.4808 - val_mae: 0.0245 - val_mse: 0.0096\n",
            "Epoch 610/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0060 - accuracy: 0.4062 - mae: 0.0060 - mse: 1.4611e-04\n",
            "Epoch 00610: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0119 - accuracy: 0.4765 - mae: 0.0119 - mse: 0.0019 - val_loss: 0.0242 - val_accuracy: 0.4808 - val_mae: 0.0242 - val_mse: 0.0096\n",
            "Epoch 611/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0102 - accuracy: 0.5000 - mae: 0.0102 - mse: 5.0855e-04\n",
            "Epoch 00611: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0119 - accuracy: 0.4765 - mae: 0.0119 - mse: 0.0018 - val_loss: 0.0251 - val_accuracy: 0.4808 - val_mae: 0.0251 - val_mse: 0.0095\n",
            "Epoch 612/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0073 - accuracy: 0.5938 - mae: 0.0073 - mse: 1.6532e-04\n",
            "Epoch 00612: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0118 - accuracy: 0.4765 - mae: 0.0118 - mse: 0.0018 - val_loss: 0.0250 - val_accuracy: 0.4808 - val_mae: 0.0250 - val_mse: 0.0095\n",
            "Epoch 613/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0197 - accuracy: 0.4375 - mae: 0.0197 - mse: 0.0022\n",
            "Epoch 00613: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0115 - accuracy: 0.4765 - mae: 0.0115 - mse: 0.0018 - val_loss: 0.0249 - val_accuracy: 0.4808 - val_mae: 0.0249 - val_mse: 0.0095\n",
            "Epoch 614/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0100 - accuracy: 0.4375 - mae: 0.0100 - mse: 2.6077e-04\n",
            "Epoch 00614: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0116 - accuracy: 0.4765 - mae: 0.0116 - mse: 0.0018 - val_loss: 0.0247 - val_accuracy: 0.4808 - val_mae: 0.0247 - val_mse: 0.0097\n",
            "Epoch 615/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0079 - accuracy: 0.2188 - mae: 0.0079 - mse: 1.5076e-04\n",
            "Epoch 00615: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0117 - accuracy: 0.4765 - mae: 0.0117 - mse: 0.0019 - val_loss: 0.0250 - val_accuracy: 0.4808 - val_mae: 0.0250 - val_mse: 0.0095\n",
            "Epoch 616/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0092 - accuracy: 0.4062 - mae: 0.0092 - mse: 2.7117e-04\n",
            "Epoch 00616: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0114 - accuracy: 0.4765 - mae: 0.0114 - mse: 0.0018 - val_loss: 0.0243 - val_accuracy: 0.4808 - val_mae: 0.0243 - val_mse: 0.0096\n",
            "Epoch 617/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0069 - accuracy: 0.5625 - mae: 0.0069 - mse: 2.1196e-04\n",
            "Epoch 00617: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0116 - accuracy: 0.4765 - mae: 0.0116 - mse: 0.0019 - val_loss: 0.0249 - val_accuracy: 0.4808 - val_mae: 0.0249 - val_mse: 0.0095\n",
            "Epoch 618/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0064 - accuracy: 0.5625 - mae: 0.0064 - mse: 8.2049e-05\n",
            "Epoch 00618: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0117 - accuracy: 0.4765 - mae: 0.0117 - mse: 0.0019 - val_loss: 0.0242 - val_accuracy: 0.4808 - val_mae: 0.0242 - val_mse: 0.0096\n",
            "Epoch 619/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0068 - accuracy: 0.3750 - mae: 0.0068 - mse: 1.6239e-04\n",
            "Epoch 00619: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0115 - accuracy: 0.4765 - mae: 0.0115 - mse: 0.0019 - val_loss: 0.0256 - val_accuracy: 0.4808 - val_mae: 0.0256 - val_mse: 0.0094\n",
            "Epoch 620/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0111 - accuracy: 0.3750 - mae: 0.0111 - mse: 3.4225e-04\n",
            "Epoch 00620: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0115 - accuracy: 0.4765 - mae: 0.0115 - mse: 0.0019 - val_loss: 0.0254 - val_accuracy: 0.4808 - val_mae: 0.0254 - val_mse: 0.0095\n",
            "Epoch 621/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0303 - accuracy: 0.5312 - mae: 0.0303 - mse: 0.0146\n",
            "Epoch 00621: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0116 - accuracy: 0.4765 - mae: 0.0116 - mse: 0.0018 - val_loss: 0.0260 - val_accuracy: 0.4808 - val_mae: 0.0260 - val_mse: 0.0098\n",
            "Epoch 622/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0154 - accuracy: 0.5312 - mae: 0.0154 - mse: 0.0014\n",
            "Epoch 00622: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0115 - accuracy: 0.4765 - mae: 0.0115 - mse: 0.0019 - val_loss: 0.0248 - val_accuracy: 0.4808 - val_mae: 0.0248 - val_mse: 0.0095\n",
            "Epoch 623/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0130 - accuracy: 0.3750 - mae: 0.0130 - mse: 7.8345e-04\n",
            "Epoch 00623: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0117 - accuracy: 0.4765 - mae: 0.0117 - mse: 0.0019 - val_loss: 0.0257 - val_accuracy: 0.4808 - val_mae: 0.0257 - val_mse: 0.0097\n",
            "Epoch 624/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0116 - accuracy: 0.5625 - mae: 0.0116 - mse: 5.0651e-04\n",
            "Epoch 00624: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0116 - accuracy: 0.4765 - mae: 0.0116 - mse: 0.0019 - val_loss: 0.0253 - val_accuracy: 0.4808 - val_mae: 0.0253 - val_mse: 0.0094\n",
            "Epoch 625/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0180 - accuracy: 0.5000 - mae: 0.0180 - mse: 0.0013\n",
            "Epoch 00625: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0118 - accuracy: 0.4765 - mae: 0.0118 - mse: 0.0019 - val_loss: 0.0243 - val_accuracy: 0.4808 - val_mae: 0.0243 - val_mse: 0.0096\n",
            "Epoch 626/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0053 - accuracy: 0.5000 - mae: 0.0053 - mse: 1.0209e-04\n",
            "Epoch 00626: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0116 - accuracy: 0.4765 - mae: 0.0116 - mse: 0.0018 - val_loss: 0.0265 - val_accuracy: 0.4808 - val_mae: 0.0265 - val_mse: 0.0098\n",
            "Epoch 627/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0121 - accuracy: 0.4688 - mae: 0.0121 - mse: 3.7790e-04\n",
            "Epoch 00627: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0118 - accuracy: 0.4765 - mae: 0.0118 - mse: 0.0019 - val_loss: 0.0245 - val_accuracy: 0.4808 - val_mae: 0.0245 - val_mse: 0.0097\n",
            "Epoch 628/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0076 - accuracy: 0.4688 - mae: 0.0076 - mse: 2.2262e-04\n",
            "Epoch 00628: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0116 - accuracy: 0.4765 - mae: 0.0116 - mse: 0.0018 - val_loss: 0.0242 - val_accuracy: 0.4808 - val_mae: 0.0242 - val_mse: 0.0096\n",
            "Epoch 629/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0317 - accuracy: 0.4688 - mae: 0.0317 - mse: 0.0151\n",
            "Epoch 00629: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0114 - accuracy: 0.4765 - mae: 0.0114 - mse: 0.0019 - val_loss: 0.0245 - val_accuracy: 0.4808 - val_mae: 0.0245 - val_mse: 0.0095\n",
            "Epoch 630/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0096 - accuracy: 0.5000 - mae: 0.0096 - mse: 6.7445e-04\n",
            "Epoch 00630: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0116 - accuracy: 0.4765 - mae: 0.0116 - mse: 0.0018 - val_loss: 0.0242 - val_accuracy: 0.4808 - val_mae: 0.0242 - val_mse: 0.0095\n",
            "Epoch 631/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0054 - accuracy: 0.3438 - mae: 0.0054 - mse: 8.9228e-05\n",
            "Epoch 00631: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0114 - accuracy: 0.4765 - mae: 0.0114 - mse: 0.0018 - val_loss: 0.0247 - val_accuracy: 0.4808 - val_mae: 0.0247 - val_mse: 0.0095\n",
            "Epoch 632/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0109 - accuracy: 0.3750 - mae: 0.0109 - mse: 5.5980e-04\n",
            "Epoch 00632: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0113 - accuracy: 0.4765 - mae: 0.0113 - mse: 0.0018 - val_loss: 0.0251 - val_accuracy: 0.4808 - val_mae: 0.0251 - val_mse: 0.0094\n",
            "Epoch 633/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0153 - accuracy: 0.4375 - mae: 0.0153 - mse: 0.0013\n",
            "Epoch 00633: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0115 - accuracy: 0.4765 - mae: 0.0115 - mse: 0.0018 - val_loss: 0.0248 - val_accuracy: 0.4808 - val_mae: 0.0248 - val_mse: 0.0095\n",
            "Epoch 634/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0044 - accuracy: 0.4062 - mae: 0.0044 - mse: 2.7183e-05\n",
            "Epoch 00634: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0117 - accuracy: 0.4765 - mae: 0.0117 - mse: 0.0019 - val_loss: 0.0263 - val_accuracy: 0.4808 - val_mae: 0.0263 - val_mse: 0.0094\n",
            "Epoch 635/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0096 - accuracy: 0.5000 - mae: 0.0096 - mse: 1.4911e-04\n",
            "Epoch 00635: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0119 - accuracy: 0.4765 - mae: 0.0119 - mse: 0.0018 - val_loss: 0.0254 - val_accuracy: 0.4808 - val_mae: 0.0254 - val_mse: 0.0094\n",
            "Epoch 636/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0118 - accuracy: 0.5312 - mae: 0.0118 - mse: 7.1055e-04\n",
            "Epoch 00636: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0116 - accuracy: 0.4765 - mae: 0.0116 - mse: 0.0018 - val_loss: 0.0245 - val_accuracy: 0.4808 - val_mae: 0.0245 - val_mse: 0.0095\n",
            "Epoch 637/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0121 - accuracy: 0.5625 - mae: 0.0121 - mse: 0.0012\n",
            "Epoch 00637: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0115 - accuracy: 0.4765 - mae: 0.0115 - mse: 0.0019 - val_loss: 0.0242 - val_accuracy: 0.4808 - val_mae: 0.0242 - val_mse: 0.0096\n",
            "Epoch 638/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0049 - accuracy: 0.3750 - mae: 0.0049 - mse: 5.8030e-05\n",
            "Epoch 00638: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0117 - accuracy: 0.4765 - mae: 0.0117 - mse: 0.0018 - val_loss: 0.0248 - val_accuracy: 0.4808 - val_mae: 0.0248 - val_mse: 0.0095\n",
            "Epoch 639/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0117 - accuracy: 0.5000 - mae: 0.0117 - mse: 0.0012\n",
            "Epoch 00639: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0117 - accuracy: 0.4765 - mae: 0.0117 - mse: 0.0019 - val_loss: 0.0244 - val_accuracy: 0.4808 - val_mae: 0.0244 - val_mse: 0.0095\n",
            "Epoch 640/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0117 - accuracy: 0.5312 - mae: 0.0117 - mse: 7.5033e-04\n",
            "Epoch 00640: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0115 - accuracy: 0.4765 - mae: 0.0115 - mse: 0.0018 - val_loss: 0.0242 - val_accuracy: 0.4808 - val_mae: 0.0242 - val_mse: 0.0096\n",
            "Epoch 641/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0166 - accuracy: 0.6250 - mae: 0.0166 - mse: 0.0017\n",
            "Epoch 00641: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0116 - accuracy: 0.4765 - mae: 0.0116 - mse: 0.0018 - val_loss: 0.0250 - val_accuracy: 0.4808 - val_mae: 0.0250 - val_mse: 0.0094\n",
            "Epoch 642/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0049 - accuracy: 0.4375 - mae: 0.0049 - mse: 4.7376e-05\n",
            "Epoch 00642: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0117 - accuracy: 0.4765 - mae: 0.0117 - mse: 0.0018 - val_loss: 0.0242 - val_accuracy: 0.4808 - val_mae: 0.0242 - val_mse: 0.0096\n",
            "Epoch 643/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0143 - accuracy: 0.5000 - mae: 0.0143 - mse: 0.0013\n",
            "Epoch 00643: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0115 - accuracy: 0.4765 - mae: 0.0115 - mse: 0.0018 - val_loss: 0.0271 - val_accuracy: 0.4808 - val_mae: 0.0271 - val_mse: 0.0098\n",
            "Epoch 644/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0117 - accuracy: 0.4688 - mae: 0.0117 - mse: 2.9582e-04\n",
            "Epoch 00644: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0118 - accuracy: 0.4765 - mae: 0.0118 - mse: 0.0019 - val_loss: 0.0249 - val_accuracy: 0.4808 - val_mae: 0.0249 - val_mse: 0.0096\n",
            "Epoch 645/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0094 - accuracy: 0.4062 - mae: 0.0094 - mse: 3.1696e-04\n",
            "Epoch 00645: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0117 - accuracy: 0.4765 - mae: 0.0117 - mse: 0.0018 - val_loss: 0.0249 - val_accuracy: 0.4808 - val_mae: 0.0249 - val_mse: 0.0094\n",
            "Epoch 646/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0089 - accuracy: 0.4375 - mae: 0.0089 - mse: 3.8890e-04\n",
            "Epoch 00646: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0115 - accuracy: 0.4765 - mae: 0.0115 - mse: 0.0018 - val_loss: 0.0251 - val_accuracy: 0.4808 - val_mae: 0.0251 - val_mse: 0.0094\n",
            "Epoch 647/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0096 - accuracy: 0.4688 - mae: 0.0096 - mse: 2.9593e-04\n",
            "Epoch 00647: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0118 - accuracy: 0.4765 - mae: 0.0118 - mse: 0.0018 - val_loss: 0.0247 - val_accuracy: 0.4808 - val_mae: 0.0247 - val_mse: 0.0096\n",
            "Epoch 648/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0074 - accuracy: 0.5312 - mae: 0.0074 - mse: 2.1605e-04\n",
            "Epoch 00648: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0116 - accuracy: 0.4765 - mae: 0.0116 - mse: 0.0018 - val_loss: 0.0247 - val_accuracy: 0.4808 - val_mae: 0.0247 - val_mse: 0.0094\n",
            "Epoch 649/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0364 - accuracy: 0.5312 - mae: 0.0364 - mse: 0.0154\n",
            "Epoch 00649: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0114 - accuracy: 0.4765 - mae: 0.0114 - mse: 0.0018 - val_loss: 0.0240 - val_accuracy: 0.4808 - val_mae: 0.0240 - val_mse: 0.0095\n",
            "Epoch 650/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0075 - accuracy: 0.5000 - mae: 0.0075 - mse: 2.0833e-04\n",
            "Epoch 00650: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0114 - accuracy: 0.4765 - mae: 0.0114 - mse: 0.0018 - val_loss: 0.0242 - val_accuracy: 0.4808 - val_mae: 0.0242 - val_mse: 0.0095\n",
            "Epoch 651/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0095 - accuracy: 0.5938 - mae: 0.0095 - mse: 3.0554e-04\n",
            "Epoch 00651: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0114 - accuracy: 0.4765 - mae: 0.0114 - mse: 0.0018 - val_loss: 0.0252 - val_accuracy: 0.4808 - val_mae: 0.0252 - val_mse: 0.0097\n",
            "Epoch 652/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0084 - accuracy: 0.4375 - mae: 0.0084 - mse: 2.3014e-04\n",
            "Epoch 00652: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0118 - accuracy: 0.4765 - mae: 0.0118 - mse: 0.0018 - val_loss: 0.0241 - val_accuracy: 0.4808 - val_mae: 0.0241 - val_mse: 0.0096\n",
            "Epoch 653/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0127 - accuracy: 0.5625 - mae: 0.0127 - mse: 5.0432e-04\n",
            "Epoch 00653: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0115 - accuracy: 0.4765 - mae: 0.0115 - mse: 0.0018 - val_loss: 0.0241 - val_accuracy: 0.4808 - val_mae: 0.0241 - val_mse: 0.0096\n",
            "Epoch 654/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0093 - accuracy: 0.4062 - mae: 0.0093 - mse: 4.1971e-04\n",
            "Epoch 00654: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0115 - accuracy: 0.4765 - mae: 0.0115 - mse: 0.0019 - val_loss: 0.0243 - val_accuracy: 0.4808 - val_mae: 0.0243 - val_mse: 0.0095\n",
            "Epoch 655/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0110 - accuracy: 0.5000 - mae: 0.0110 - mse: 0.0011\n",
            "Epoch 00655: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0113 - accuracy: 0.4765 - mae: 0.0113 - mse: 0.0018 - val_loss: 0.0266 - val_accuracy: 0.4808 - val_mae: 0.0266 - val_mse: 0.0098\n",
            "Epoch 656/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0210 - accuracy: 0.3125 - mae: 0.0210 - mse: 0.0016\n",
            "Epoch 00656: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0118 - accuracy: 0.4765 - mae: 0.0118 - mse: 0.0018 - val_loss: 0.0261 - val_accuracy: 0.4808 - val_mae: 0.0261 - val_mse: 0.0098\n",
            "Epoch 657/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0106 - accuracy: 0.5625 - mae: 0.0106 - mse: 4.4220e-04\n",
            "Epoch 00657: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0117 - accuracy: 0.4765 - mae: 0.0117 - mse: 0.0018 - val_loss: 0.0243 - val_accuracy: 0.4808 - val_mae: 0.0243 - val_mse: 0.0095\n",
            "Epoch 658/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0078 - accuracy: 0.5000 - mae: 0.0078 - mse: 3.6884e-04\n",
            "Epoch 00658: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0116 - accuracy: 0.4765 - mae: 0.0116 - mse: 0.0019 - val_loss: 0.0245 - val_accuracy: 0.4808 - val_mae: 0.0245 - val_mse: 0.0096\n",
            "Epoch 659/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0147 - accuracy: 0.4062 - mae: 0.0147 - mse: 7.4400e-04\n",
            "Epoch 00659: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0117 - accuracy: 0.4765 - mae: 0.0117 - mse: 0.0018 - val_loss: 0.0244 - val_accuracy: 0.4808 - val_mae: 0.0244 - val_mse: 0.0095\n",
            "Epoch 660/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0067 - accuracy: 0.5312 - mae: 0.0067 - mse: 1.4151e-04\n",
            "Epoch 00660: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0115 - accuracy: 0.4765 - mae: 0.0115 - mse: 0.0018 - val_loss: 0.0243 - val_accuracy: 0.4808 - val_mae: 0.0243 - val_mse: 0.0095\n",
            "Epoch 661/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0061 - accuracy: 0.5625 - mae: 0.0061 - mse: 1.4864e-04\n",
            "Epoch 00661: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0114 - accuracy: 0.4765 - mae: 0.0114 - mse: 0.0018 - val_loss: 0.0242 - val_accuracy: 0.4808 - val_mae: 0.0242 - val_mse: 0.0095\n",
            "Epoch 662/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0130 - accuracy: 0.4688 - mae: 0.0130 - mse: 0.0012\n",
            "Epoch 00662: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0116 - accuracy: 0.4765 - mae: 0.0116 - mse: 0.0018 - val_loss: 0.0243 - val_accuracy: 0.4808 - val_mae: 0.0243 - val_mse: 0.0096\n",
            "Epoch 663/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0267 - accuracy: 0.4375 - mae: 0.0267 - mse: 0.0146\n",
            "Epoch 00663: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0115 - accuracy: 0.4765 - mae: 0.0115 - mse: 0.0018 - val_loss: 0.0248 - val_accuracy: 0.4808 - val_mae: 0.0248 - val_mse: 0.0094\n",
            "Epoch 664/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0127 - accuracy: 0.5938 - mae: 0.0127 - mse: 7.4731e-04\n",
            "Epoch 00664: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0114 - accuracy: 0.4765 - mae: 0.0114 - mse: 0.0018 - val_loss: 0.0246 - val_accuracy: 0.4808 - val_mae: 0.0246 - val_mse: 0.0095\n",
            "Epoch 665/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0122 - accuracy: 0.4688 - mae: 0.0122 - mse: 8.0159e-04\n",
            "Epoch 00665: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0117 - accuracy: 0.4765 - mae: 0.0117 - mse: 0.0018 - val_loss: 0.0253 - val_accuracy: 0.4808 - val_mae: 0.0253 - val_mse: 0.0097\n",
            "Epoch 666/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0080 - accuracy: 0.5312 - mae: 0.0080 - mse: 2.5005e-04\n",
            "Epoch 00666: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0116 - accuracy: 0.4765 - mae: 0.0116 - mse: 0.0018 - val_loss: 0.0241 - val_accuracy: 0.4808 - val_mae: 0.0241 - val_mse: 0.0096\n",
            "Epoch 667/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0048 - accuracy: 0.4062 - mae: 0.0048 - mse: 9.4413e-05\n",
            "Epoch 00667: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0113 - accuracy: 0.4765 - mae: 0.0113 - mse: 0.0018 - val_loss: 0.0248 - val_accuracy: 0.4808 - val_mae: 0.0248 - val_mse: 0.0094\n",
            "Epoch 668/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0156 - accuracy: 0.5000 - mae: 0.0156 - mse: 0.0014\n",
            "Epoch 00668: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0115 - accuracy: 0.4765 - mae: 0.0115 - mse: 0.0018 - val_loss: 0.0246 - val_accuracy: 0.4808 - val_mae: 0.0246 - val_mse: 0.0096\n",
            "Epoch 669/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0155 - accuracy: 0.5938 - mae: 0.0155 - mse: 0.0015\n",
            "Epoch 00669: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0114 - accuracy: 0.4765 - mae: 0.0114 - mse: 0.0018 - val_loss: 0.0246 - val_accuracy: 0.4808 - val_mae: 0.0246 - val_mse: 0.0094\n",
            "Epoch 670/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0067 - accuracy: 0.5625 - mae: 0.0067 - mse: 1.1821e-04\n",
            "Epoch 00670: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0116 - accuracy: 0.4765 - mae: 0.0116 - mse: 0.0018 - val_loss: 0.0241 - val_accuracy: 0.4808 - val_mae: 0.0241 - val_mse: 0.0096\n",
            "Epoch 671/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0118 - accuracy: 0.5312 - mae: 0.0118 - mse: 0.0010\n",
            "Epoch 00671: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0113 - accuracy: 0.4765 - mae: 0.0113 - mse: 0.0018 - val_loss: 0.0239 - val_accuracy: 0.4808 - val_mae: 0.0239 - val_mse: 0.0095\n",
            "Epoch 672/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0038 - accuracy: 0.5312 - mae: 0.0038 - mse: 3.3095e-05\n",
            "Epoch 00672: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0113 - accuracy: 0.4765 - mae: 0.0113 - mse: 0.0018 - val_loss: 0.0243 - val_accuracy: 0.4808 - val_mae: 0.0243 - val_mse: 0.0094\n",
            "Epoch 673/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0352 - accuracy: 0.5312 - mae: 0.0352 - mse: 0.0157\n",
            "Epoch 00673: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0117 - accuracy: 0.4765 - mae: 0.0117 - mse: 0.0018 - val_loss: 0.0243 - val_accuracy: 0.4808 - val_mae: 0.0243 - val_mse: 0.0095\n",
            "Epoch 674/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0105 - accuracy: 0.6250 - mae: 0.0105 - mse: 7.4554e-04\n",
            "Epoch 00674: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0113 - accuracy: 0.4765 - mae: 0.0113 - mse: 0.0018 - val_loss: 0.0241 - val_accuracy: 0.4808 - val_mae: 0.0241 - val_mse: 0.0096\n",
            "Epoch 675/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0076 - accuracy: 0.3750 - mae: 0.0076 - mse: 1.6362e-04\n",
            "Epoch 00675: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0116 - accuracy: 0.4765 - mae: 0.0116 - mse: 0.0018 - val_loss: 0.0255 - val_accuracy: 0.4808 - val_mae: 0.0255 - val_mse: 0.0094\n",
            "Epoch 676/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0137 - accuracy: 0.4688 - mae: 0.0137 - mse: 8.3647e-04\n",
            "Epoch 00676: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0119 - accuracy: 0.4765 - mae: 0.0119 - mse: 0.0018 - val_loss: 0.0243 - val_accuracy: 0.4808 - val_mae: 0.0243 - val_mse: 0.0094\n",
            "Epoch 677/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0060 - accuracy: 0.3438 - mae: 0.0060 - mse: 9.4137e-05\n",
            "Epoch 00677: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0114 - accuracy: 0.4765 - mae: 0.0114 - mse: 0.0018 - val_loss: 0.0240 - val_accuracy: 0.4808 - val_mae: 0.0240 - val_mse: 0.0095\n",
            "Epoch 678/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0107 - accuracy: 0.5312 - mae: 0.0107 - mse: 5.5248e-04\n",
            "Epoch 00678: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0115 - accuracy: 0.4765 - mae: 0.0115 - mse: 0.0018 - val_loss: 0.0253 - val_accuracy: 0.4808 - val_mae: 0.0253 - val_mse: 0.0097\n",
            "Epoch 679/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0099 - accuracy: 0.5000 - mae: 0.0099 - mse: 4.6275e-04\n",
            "Epoch 00679: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0113 - accuracy: 0.4765 - mae: 0.0113 - mse: 0.0018 - val_loss: 0.0245 - val_accuracy: 0.4808 - val_mae: 0.0245 - val_mse: 0.0094\n",
            "Epoch 680/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0105 - accuracy: 0.5625 - mae: 0.0105 - mse: 6.0546e-04\n",
            "Epoch 00680: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0113 - accuracy: 0.4765 - mae: 0.0113 - mse: 0.0018 - val_loss: 0.0245 - val_accuracy: 0.4808 - val_mae: 0.0245 - val_mse: 0.0094\n",
            "Epoch 681/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0106 - accuracy: 0.3438 - mae: 0.0106 - mse: 4.8282e-04\n",
            "Epoch 00681: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0116 - accuracy: 0.4765 - mae: 0.0116 - mse: 0.0018 - val_loss: 0.0245 - val_accuracy: 0.4808 - val_mae: 0.0245 - val_mse: 0.0094\n",
            "Epoch 682/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0160 - accuracy: 0.3750 - mae: 0.0160 - mse: 0.0010\n",
            "Epoch 00682: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0115 - accuracy: 0.4765 - mae: 0.0115 - mse: 0.0018 - val_loss: 0.0255 - val_accuracy: 0.4808 - val_mae: 0.0255 - val_mse: 0.0094\n",
            "Epoch 683/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0071 - accuracy: 0.4062 - mae: 0.0071 - mse: 1.0502e-04\n",
            "Epoch 00683: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0116 - accuracy: 0.4765 - mae: 0.0116 - mse: 0.0018 - val_loss: 0.0240 - val_accuracy: 0.4808 - val_mae: 0.0240 - val_mse: 0.0095\n",
            "Epoch 684/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0121 - accuracy: 0.5625 - mae: 0.0121 - mse: 5.9431e-04\n",
            "Epoch 00684: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0115 - accuracy: 0.4765 - mae: 0.0115 - mse: 0.0018 - val_loss: 0.0267 - val_accuracy: 0.4808 - val_mae: 0.0267 - val_mse: 0.0097\n",
            "Epoch 685/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0105 - accuracy: 0.4375 - mae: 0.0105 - mse: 3.0706e-04\n",
            "Epoch 00685: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0116 - accuracy: 0.4765 - mae: 0.0116 - mse: 0.0018 - val_loss: 0.0239 - val_accuracy: 0.4808 - val_mae: 0.0239 - val_mse: 0.0095\n",
            "Epoch 686/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0121 - accuracy: 0.5312 - mae: 0.0121 - mse: 6.8803e-04\n",
            "Epoch 00686: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0112 - accuracy: 0.4765 - mae: 0.0112 - mse: 0.0018 - val_loss: 0.0240 - val_accuracy: 0.4808 - val_mae: 0.0240 - val_mse: 0.0095\n",
            "Epoch 687/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0102 - accuracy: 0.5625 - mae: 0.0102 - mse: 7.5770e-04\n",
            "Epoch 00687: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0112 - accuracy: 0.4765 - mae: 0.0112 - mse: 0.0018 - val_loss: 0.0243 - val_accuracy: 0.4808 - val_mae: 0.0243 - val_mse: 0.0094\n",
            "Epoch 688/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0084 - accuracy: 0.5625 - mae: 0.0084 - mse: 3.1541e-04\n",
            "Epoch 00688: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0115 - accuracy: 0.4765 - mae: 0.0115 - mse: 0.0018 - val_loss: 0.0251 - val_accuracy: 0.4808 - val_mae: 0.0251 - val_mse: 0.0096\n",
            "Epoch 689/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0146 - accuracy: 0.3750 - mae: 0.0146 - mse: 8.8135e-04\n",
            "Epoch 00689: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0115 - accuracy: 0.4765 - mae: 0.0115 - mse: 0.0018 - val_loss: 0.0240 - val_accuracy: 0.4808 - val_mae: 0.0240 - val_mse: 0.0095\n",
            "Epoch 690/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0086 - accuracy: 0.4375 - mae: 0.0086 - mse: 3.1751e-04\n",
            "Epoch 00690: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0114 - accuracy: 0.4765 - mae: 0.0114 - mse: 0.0018 - val_loss: 0.0240 - val_accuracy: 0.4808 - val_mae: 0.0240 - val_mse: 0.0095\n",
            "Epoch 691/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0078 - accuracy: 0.5000 - mae: 0.0078 - mse: 1.8671e-04\n",
            "Epoch 00691: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0116 - accuracy: 0.4765 - mae: 0.0116 - mse: 0.0019 - val_loss: 0.0249 - val_accuracy: 0.4808 - val_mae: 0.0249 - val_mse: 0.0094\n",
            "Epoch 692/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0047 - accuracy: 0.4062 - mae: 0.0047 - mse: 3.1606e-05\n",
            "Epoch 00692: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0113 - accuracy: 0.4765 - mae: 0.0113 - mse: 0.0018 - val_loss: 0.0241 - val_accuracy: 0.4808 - val_mae: 0.0241 - val_mse: 0.0095\n",
            "Epoch 693/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0050 - accuracy: 0.3438 - mae: 0.0050 - mse: 1.2028e-04\n",
            "Epoch 00693: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0112 - accuracy: 0.4765 - mae: 0.0112 - mse: 0.0018 - val_loss: 0.0241 - val_accuracy: 0.4808 - val_mae: 0.0241 - val_mse: 0.0094\n",
            "Epoch 694/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0050 - accuracy: 0.5312 - mae: 0.0050 - mse: 1.8549e-04\n",
            "Epoch 00694: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0114 - accuracy: 0.4765 - mae: 0.0114 - mse: 0.0018 - val_loss: 0.0250 - val_accuracy: 0.4808 - val_mae: 0.0250 - val_mse: 0.0094\n",
            "Epoch 695/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0178 - accuracy: 0.3750 - mae: 0.0178 - mse: 0.0021\n",
            "Epoch 00695: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0113 - accuracy: 0.4765 - mae: 0.0113 - mse: 0.0018 - val_loss: 0.0244 - val_accuracy: 0.4808 - val_mae: 0.0244 - val_mse: 0.0096\n",
            "Epoch 696/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0103 - accuracy: 0.3438 - mae: 0.0103 - mse: 4.8810e-04\n",
            "Epoch 00696: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0112 - accuracy: 0.4765 - mae: 0.0112 - mse: 0.0018 - val_loss: 0.0243 - val_accuracy: 0.4808 - val_mae: 0.0243 - val_mse: 0.0094\n",
            "Epoch 697/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0108 - accuracy: 0.5312 - mae: 0.0108 - mse: 7.3347e-04\n",
            "Epoch 00697: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0116 - accuracy: 0.4765 - mae: 0.0116 - mse: 0.0018 - val_loss: 0.0244 - val_accuracy: 0.4808 - val_mae: 0.0244 - val_mse: 0.0096\n",
            "Epoch 698/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0070 - accuracy: 0.5938 - mae: 0.0070 - mse: 2.1221e-04\n",
            "Epoch 00698: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0116 - accuracy: 0.4765 - mae: 0.0116 - mse: 0.0018 - val_loss: 0.0246 - val_accuracy: 0.4808 - val_mae: 0.0246 - val_mse: 0.0094\n",
            "Epoch 699/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0055 - accuracy: 0.4375 - mae: 0.0055 - mse: 9.6631e-05\n",
            "Epoch 00699: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0118 - accuracy: 0.4765 - mae: 0.0118 - mse: 0.0018 - val_loss: 0.0240 - val_accuracy: 0.4808 - val_mae: 0.0240 - val_mse: 0.0095\n",
            "Epoch 700/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0062 - accuracy: 0.4375 - mae: 0.0062 - mse: 1.0834e-04\n",
            "Epoch 00700: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0112 - accuracy: 0.4765 - mae: 0.0112 - mse: 0.0018 - val_loss: 0.0241 - val_accuracy: 0.4808 - val_mae: 0.0241 - val_mse: 0.0095\n",
            "Epoch 701/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0122 - accuracy: 0.3750 - mae: 0.0122 - mse: 8.1704e-04\n",
            "Epoch 00701: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0115 - accuracy: 0.4765 - mae: 0.0115 - mse: 0.0018 - val_loss: 0.0239 - val_accuracy: 0.4808 - val_mae: 0.0239 - val_mse: 0.0095\n",
            "Epoch 702/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0266 - accuracy: 0.5312 - mae: 0.0266 - mse: 0.0147\n",
            "Epoch 00702: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0111 - accuracy: 0.4765 - mae: 0.0111 - mse: 0.0018 - val_loss: 0.0241 - val_accuracy: 0.4808 - val_mae: 0.0241 - val_mse: 0.0095\n",
            "Epoch 703/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0113 - accuracy: 0.5625 - mae: 0.0113 - mse: 6.5209e-04\n",
            "Epoch 00703: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0112 - accuracy: 0.4765 - mae: 0.0112 - mse: 0.0018 - val_loss: 0.0244 - val_accuracy: 0.4808 - val_mae: 0.0244 - val_mse: 0.0094\n",
            "Epoch 704/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0093 - accuracy: 0.4688 - mae: 0.0093 - mse: 4.0368e-04\n",
            "Epoch 00704: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0117 - accuracy: 0.4765 - mae: 0.0117 - mse: 0.0018 - val_loss: 0.0239 - val_accuracy: 0.4808 - val_mae: 0.0239 - val_mse: 0.0095\n",
            "Epoch 705/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0313 - accuracy: 0.4062 - mae: 0.0313 - mse: 0.0150\n",
            "Epoch 00705: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0113 - accuracy: 0.4765 - mae: 0.0113 - mse: 0.0018 - val_loss: 0.0243 - val_accuracy: 0.4808 - val_mae: 0.0243 - val_mse: 0.0096\n",
            "Epoch 706/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0147 - accuracy: 0.4375 - mae: 0.0147 - mse: 0.0013\n",
            "Epoch 00706: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0114 - accuracy: 0.4765 - mae: 0.0114 - mse: 0.0018 - val_loss: 0.0253 - val_accuracy: 0.4808 - val_mae: 0.0253 - val_mse: 0.0096\n",
            "Epoch 707/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0162 - accuracy: 0.4375 - mae: 0.0162 - mse: 0.0014\n",
            "Epoch 00707: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0116 - accuracy: 0.4765 - mae: 0.0116 - mse: 0.0018 - val_loss: 0.0242 - val_accuracy: 0.4808 - val_mae: 0.0242 - val_mse: 0.0094\n",
            "Epoch 708/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0182 - accuracy: 0.4062 - mae: 0.0182 - mse: 0.0024\n",
            "Epoch 00708: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0115 - accuracy: 0.4765 - mae: 0.0115 - mse: 0.0018 - val_loss: 0.0265 - val_accuracy: 0.4808 - val_mae: 0.0265 - val_mse: 0.0093\n",
            "Epoch 709/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0136 - accuracy: 0.3750 - mae: 0.0136 - mse: 7.8395e-04\n",
            "Epoch 00709: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0114 - accuracy: 0.4765 - mae: 0.0114 - mse: 0.0018 - val_loss: 0.0240 - val_accuracy: 0.4808 - val_mae: 0.0240 - val_mse: 0.0095\n",
            "Epoch 710/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0088 - accuracy: 0.4062 - mae: 0.0088 - mse: 3.6070e-04\n",
            "Epoch 00710: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0112 - accuracy: 0.4765 - mae: 0.0112 - mse: 0.0018 - val_loss: 0.0239 - val_accuracy: 0.4808 - val_mae: 0.0239 - val_mse: 0.0095\n",
            "Epoch 711/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0032 - accuracy: 0.6250 - mae: 0.0032 - mse: 2.3303e-05\n",
            "Epoch 00711: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0112 - accuracy: 0.4765 - mae: 0.0112 - mse: 0.0018 - val_loss: 0.0241 - val_accuracy: 0.4808 - val_mae: 0.0241 - val_mse: 0.0094\n",
            "Epoch 712/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0075 - accuracy: 0.5625 - mae: 0.0075 - mse: 2.0537e-04\n",
            "Epoch 00712: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0113 - accuracy: 0.4765 - mae: 0.0113 - mse: 0.0018 - val_loss: 0.0254 - val_accuracy: 0.4808 - val_mae: 0.0254 - val_mse: 0.0093\n",
            "Epoch 713/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0056 - accuracy: 0.4375 - mae: 0.0056 - mse: 4.0179e-05\n",
            "Epoch 00713: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0113 - accuracy: 0.4765 - mae: 0.0113 - mse: 0.0018 - val_loss: 0.0239 - val_accuracy: 0.4808 - val_mae: 0.0239 - val_mse: 0.0095\n",
            "Epoch 714/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0318 - accuracy: 0.5000 - mae: 0.0318 - mse: 0.0151\n",
            "Epoch 00714: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0112 - accuracy: 0.4765 - mae: 0.0112 - mse: 0.0018 - val_loss: 0.0240 - val_accuracy: 0.4808 - val_mae: 0.0240 - val_mse: 0.0094\n",
            "Epoch 715/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0105 - accuracy: 0.5938 - mae: 0.0105 - mse: 5.1486e-04\n",
            "Epoch 00715: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0114 - accuracy: 0.4765 - mae: 0.0114 - mse: 0.0018 - val_loss: 0.0239 - val_accuracy: 0.4808 - val_mae: 0.0239 - val_mse: 0.0095\n",
            "Epoch 716/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0073 - accuracy: 0.5000 - mae: 0.0073 - mse: 2.3741e-04\n",
            "Epoch 00716: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0113 - accuracy: 0.4765 - mae: 0.0113 - mse: 0.0018 - val_loss: 0.0270 - val_accuracy: 0.4808 - val_mae: 0.0270 - val_mse: 0.0097\n",
            "Epoch 717/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0117 - accuracy: 0.3438 - mae: 0.0117 - mse: 3.9466e-04\n",
            "Epoch 00717: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0119 - accuracy: 0.4765 - mae: 0.0119 - mse: 0.0018 - val_loss: 0.0248 - val_accuracy: 0.4808 - val_mae: 0.0248 - val_mse: 0.0096\n",
            "Epoch 718/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0118 - accuracy: 0.4375 - mae: 0.0118 - mse: 3.7195e-04\n",
            "Epoch 00718: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0116 - accuracy: 0.4765 - mae: 0.0116 - mse: 0.0018 - val_loss: 0.0261 - val_accuracy: 0.4808 - val_mae: 0.0261 - val_mse: 0.0097\n",
            "Epoch 719/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0178 - accuracy: 0.4688 - mae: 0.0178 - mse: 0.0016\n",
            "Epoch 00719: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0116 - accuracy: 0.4765 - mae: 0.0116 - mse: 0.0018 - val_loss: 0.0242 - val_accuracy: 0.4808 - val_mae: 0.0242 - val_mse: 0.0094\n",
            "Epoch 720/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0048 - accuracy: 0.4375 - mae: 0.0048 - mse: 4.6580e-05\n",
            "Epoch 00720: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0112 - accuracy: 0.4765 - mae: 0.0112 - mse: 0.0018 - val_loss: 0.0260 - val_accuracy: 0.4808 - val_mae: 0.0260 - val_mse: 0.0096\n",
            "Epoch 721/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0070 - accuracy: 0.5000 - mae: 0.0070 - mse: 1.5662e-04\n",
            "Epoch 00721: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0113 - accuracy: 0.4765 - mae: 0.0113 - mse: 0.0018 - val_loss: 0.0268 - val_accuracy: 0.4808 - val_mae: 0.0268 - val_mse: 0.0097\n",
            "Epoch 722/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0183 - accuracy: 0.4688 - mae: 0.0183 - mse: 0.0016\n",
            "Epoch 00722: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0116 - accuracy: 0.4765 - mae: 0.0116 - mse: 0.0018 - val_loss: 0.0240 - val_accuracy: 0.4808 - val_mae: 0.0240 - val_mse: 0.0094\n",
            "Epoch 723/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0189 - accuracy: 0.4375 - mae: 0.0189 - mse: 0.0022\n",
            "Epoch 00723: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0114 - accuracy: 0.4765 - mae: 0.0114 - mse: 0.0018 - val_loss: 0.0239 - val_accuracy: 0.4808 - val_mae: 0.0239 - val_mse: 0.0094\n",
            "Epoch 724/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0144 - accuracy: 0.5000 - mae: 0.0144 - mse: 0.0010\n",
            "Epoch 00724: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0112 - accuracy: 0.4765 - mae: 0.0112 - mse: 0.0018 - val_loss: 0.0241 - val_accuracy: 0.4808 - val_mae: 0.0241 - val_mse: 0.0095\n",
            "Epoch 725/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0091 - accuracy: 0.4688 - mae: 0.0091 - mse: 4.1888e-04\n",
            "Epoch 00725: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0113 - accuracy: 0.4765 - mae: 0.0113 - mse: 0.0018 - val_loss: 0.0238 - val_accuracy: 0.4808 - val_mae: 0.0238 - val_mse: 0.0094\n",
            "Epoch 726/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0045 - accuracy: 0.4688 - mae: 0.0045 - mse: 6.5041e-05\n",
            "Epoch 00726: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0116 - accuracy: 0.4765 - mae: 0.0116 - mse: 0.0018 - val_loss: 0.0255 - val_accuracy: 0.4808 - val_mae: 0.0255 - val_mse: 0.0093\n",
            "Epoch 727/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0099 - accuracy: 0.3750 - mae: 0.0099 - mse: 2.1608e-04\n",
            "Epoch 00727: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0112 - accuracy: 0.4765 - mae: 0.0112 - mse: 0.0018 - val_loss: 0.0239 - val_accuracy: 0.4808 - val_mae: 0.0239 - val_mse: 0.0095\n",
            "Epoch 728/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0076 - accuracy: 0.4688 - mae: 0.0076 - mse: 1.9086e-04\n",
            "Epoch 00728: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0113 - accuracy: 0.4765 - mae: 0.0113 - mse: 0.0018 - val_loss: 0.0242 - val_accuracy: 0.4808 - val_mae: 0.0242 - val_mse: 0.0095\n",
            "Epoch 729/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0167 - accuracy: 0.4375 - mae: 0.0167 - mse: 0.0014\n",
            "Epoch 00729: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0112 - accuracy: 0.4765 - mae: 0.0112 - mse: 0.0018 - val_loss: 0.0238 - val_accuracy: 0.4808 - val_mae: 0.0238 - val_mse: 0.0094\n",
            "Epoch 730/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0084 - accuracy: 0.5312 - mae: 0.0084 - mse: 7.5588e-04\n",
            "Epoch 00730: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0113 - accuracy: 0.4765 - mae: 0.0113 - mse: 0.0018 - val_loss: 0.0242 - val_accuracy: 0.4808 - val_mae: 0.0242 - val_mse: 0.0094\n",
            "Epoch 731/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0331 - accuracy: 0.4688 - mae: 0.0331 - mse: 0.0150\n",
            "Epoch 00731: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0111 - accuracy: 0.4765 - mae: 0.0111 - mse: 0.0018 - val_loss: 0.0238 - val_accuracy: 0.4808 - val_mae: 0.0238 - val_mse: 0.0095\n",
            "Epoch 732/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0135 - accuracy: 0.6250 - mae: 0.0135 - mse: 0.0017\n",
            "Epoch 00732: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0111 - accuracy: 0.4765 - mae: 0.0111 - mse: 0.0018 - val_loss: 0.0240 - val_accuracy: 0.4808 - val_mae: 0.0240 - val_mse: 0.0094\n",
            "Epoch 733/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0095 - accuracy: 0.5000 - mae: 0.0095 - mse: 4.1558e-04\n",
            "Epoch 00733: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0113 - accuracy: 0.4765 - mae: 0.0113 - mse: 0.0018 - val_loss: 0.0238 - val_accuracy: 0.4808 - val_mae: 0.0238 - val_mse: 0.0095\n",
            "Epoch 734/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0087 - accuracy: 0.4375 - mae: 0.0087 - mse: 3.8998e-04\n",
            "Epoch 00734: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0113 - accuracy: 0.4765 - mae: 0.0113 - mse: 0.0018 - val_loss: 0.0249 - val_accuracy: 0.4808 - val_mae: 0.0249 - val_mse: 0.0093\n",
            "Epoch 735/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0045 - accuracy: 0.5625 - mae: 0.0045 - mse: 2.5710e-05\n",
            "Epoch 00735: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0113 - accuracy: 0.4765 - mae: 0.0113 - mse: 0.0018 - val_loss: 0.0238 - val_accuracy: 0.4808 - val_mae: 0.0238 - val_mse: 0.0094\n",
            "Epoch 736/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0099 - accuracy: 0.4688 - mae: 0.0099 - mse: 4.9457e-04\n",
            "Epoch 00736: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0111 - accuracy: 0.4765 - mae: 0.0111 - mse: 0.0018 - val_loss: 0.0247 - val_accuracy: 0.4808 - val_mae: 0.0247 - val_mse: 0.0093\n",
            "Epoch 737/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0273 - accuracy: 0.5312 - mae: 0.0273 - mse: 0.0145\n",
            "Epoch 00737: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0113 - accuracy: 0.4765 - mae: 0.0113 - mse: 0.0018 - val_loss: 0.0249 - val_accuracy: 0.4808 - val_mae: 0.0249 - val_mse: 0.0093\n",
            "Epoch 738/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0136 - accuracy: 0.4375 - mae: 0.0136 - mse: 9.7148e-04\n",
            "Epoch 00738: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0114 - accuracy: 0.4765 - mae: 0.0114 - mse: 0.0018 - val_loss: 0.0238 - val_accuracy: 0.4808 - val_mae: 0.0238 - val_mse: 0.0094\n",
            "Epoch 739/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0071 - accuracy: 0.5000 - mae: 0.0071 - mse: 2.9900e-04\n",
            "Epoch 00739: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0114 - accuracy: 0.4765 - mae: 0.0114 - mse: 0.0018 - val_loss: 0.0239 - val_accuracy: 0.4808 - val_mae: 0.0239 - val_mse: 0.0095\n",
            "Epoch 740/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0086 - accuracy: 0.4688 - mae: 0.0086 - mse: 2.5757e-04\n",
            "Epoch 00740: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0114 - accuracy: 0.4765 - mae: 0.0114 - mse: 0.0018 - val_loss: 0.0259 - val_accuracy: 0.4808 - val_mae: 0.0259 - val_mse: 0.0096\n",
            "Epoch 741/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0104 - accuracy: 0.4062 - mae: 0.0104 - mse: 2.2717e-04\n",
            "Epoch 00741: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0116 - accuracy: 0.4765 - mae: 0.0116 - mse: 0.0018 - val_loss: 0.0253 - val_accuracy: 0.4808 - val_mae: 0.0253 - val_mse: 0.0096\n",
            "Epoch 742/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0126 - accuracy: 0.2812 - mae: 0.0126 - mse: 4.6177e-04\n",
            "Epoch 00742: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0116 - accuracy: 0.4765 - mae: 0.0116 - mse: 0.0018 - val_loss: 0.0239 - val_accuracy: 0.4808 - val_mae: 0.0239 - val_mse: 0.0094\n",
            "Epoch 743/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0101 - accuracy: 0.4062 - mae: 0.0101 - mse: 0.0011\n",
            "Epoch 00743: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0112 - accuracy: 0.4765 - mae: 0.0112 - mse: 0.0018 - val_loss: 0.0243 - val_accuracy: 0.4808 - val_mae: 0.0243 - val_mse: 0.0094\n",
            "Epoch 744/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0089 - accuracy: 0.5625 - mae: 0.0089 - mse: 2.6532e-04\n",
            "Epoch 00744: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0115 - accuracy: 0.4765 - mae: 0.0115 - mse: 0.0018 - val_loss: 0.0239 - val_accuracy: 0.4808 - val_mae: 0.0239 - val_mse: 0.0094\n",
            "Epoch 745/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0067 - accuracy: 0.4688 - mae: 0.0067 - mse: 1.0348e-04\n",
            "Epoch 00745: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0112 - accuracy: 0.4765 - mae: 0.0112 - mse: 0.0018 - val_loss: 0.0239 - val_accuracy: 0.4808 - val_mae: 0.0239 - val_mse: 0.0094\n",
            "Epoch 746/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0035 - accuracy: 0.4375 - mae: 0.0035 - mse: 1.9431e-05\n",
            "Epoch 00746: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0111 - accuracy: 0.4765 - mae: 0.0111 - mse: 0.0018 - val_loss: 0.0239 - val_accuracy: 0.4808 - val_mae: 0.0239 - val_mse: 0.0094\n",
            "Epoch 747/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0103 - accuracy: 0.3750 - mae: 0.0103 - mse: 6.9508e-04\n",
            "Epoch 00747: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0114 - accuracy: 0.4765 - mae: 0.0114 - mse: 0.0018 - val_loss: 0.0253 - val_accuracy: 0.4808 - val_mae: 0.0253 - val_mse: 0.0093\n",
            "Epoch 748/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0104 - accuracy: 0.3438 - mae: 0.0104 - mse: 3.7600e-04\n",
            "Epoch 00748: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0113 - accuracy: 0.4765 - mae: 0.0113 - mse: 0.0018 - val_loss: 0.0256 - val_accuracy: 0.4808 - val_mae: 0.0256 - val_mse: 0.0093\n",
            "Epoch 749/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0096 - accuracy: 0.5312 - mae: 0.0096 - mse: 2.9425e-04\n",
            "Epoch 00749: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0114 - accuracy: 0.4765 - mae: 0.0114 - mse: 0.0018 - val_loss: 0.0243 - val_accuracy: 0.4808 - val_mae: 0.0243 - val_mse: 0.0095\n",
            "Epoch 750/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0328 - accuracy: 0.5312 - mae: 0.0328 - mse: 0.0159\n",
            "Epoch 00750: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0113 - accuracy: 0.4765 - mae: 0.0113 - mse: 0.0018 - val_loss: 0.0243 - val_accuracy: 0.4808 - val_mae: 0.0243 - val_mse: 0.0095\n",
            "Epoch 751/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0069 - accuracy: 0.3750 - mae: 0.0069 - mse: 1.1312e-04\n",
            "Epoch 00751: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0114 - accuracy: 0.4765 - mae: 0.0114 - mse: 0.0018 - val_loss: 0.0248 - val_accuracy: 0.4808 - val_mae: 0.0248 - val_mse: 0.0096\n",
            "Epoch 752/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0082 - accuracy: 0.5000 - mae: 0.0082 - mse: 1.7065e-04\n",
            "Epoch 00752: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0116 - accuracy: 0.4765 - mae: 0.0116 - mse: 0.0018 - val_loss: 0.0243 - val_accuracy: 0.4808 - val_mae: 0.0243 - val_mse: 0.0093\n",
            "Epoch 753/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0062 - accuracy: 0.5938 - mae: 0.0062 - mse: 1.9225e-04\n",
            "Epoch 00753: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0112 - accuracy: 0.4765 - mae: 0.0112 - mse: 0.0018 - val_loss: 0.0239 - val_accuracy: 0.4808 - val_mae: 0.0239 - val_mse: 0.0094\n",
            "Epoch 754/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0098 - accuracy: 0.4688 - mae: 0.0098 - mse: 3.8778e-04\n",
            "Epoch 00754: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0111 - accuracy: 0.4765 - mae: 0.0111 - mse: 0.0018 - val_loss: 0.0244 - val_accuracy: 0.4808 - val_mae: 0.0244 - val_mse: 0.0093\n",
            "Epoch 755/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0103 - accuracy: 0.3750 - mae: 0.0103 - mse: 7.3174e-04\n",
            "Epoch 00755: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0112 - accuracy: 0.4765 - mae: 0.0112 - mse: 0.0018 - val_loss: 0.0242 - val_accuracy: 0.4808 - val_mae: 0.0242 - val_mse: 0.0093\n",
            "Epoch 756/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0098 - accuracy: 0.3438 - mae: 0.0098 - mse: 3.5124e-04\n",
            "Epoch 00756: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0111 - accuracy: 0.4765 - mae: 0.0111 - mse: 0.0018 - val_loss: 0.0238 - val_accuracy: 0.4808 - val_mae: 0.0238 - val_mse: 0.0095\n",
            "Epoch 757/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0072 - accuracy: 0.4375 - mae: 0.0072 - mse: 1.9758e-04\n",
            "Epoch 00757: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0110 - accuracy: 0.4765 - mae: 0.0110 - mse: 0.0018 - val_loss: 0.0238 - val_accuracy: 0.4808 - val_mae: 0.0238 - val_mse: 0.0094\n",
            "Epoch 758/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0096 - accuracy: 0.3750 - mae: 0.0096 - mse: 2.5418e-04\n",
            "Epoch 00758: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0113 - accuracy: 0.4765 - mae: 0.0113 - mse: 0.0018 - val_loss: 0.0240 - val_accuracy: 0.4808 - val_mae: 0.0240 - val_mse: 0.0094\n",
            "Epoch 759/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0290 - accuracy: 0.4688 - mae: 0.0290 - mse: 0.0148\n",
            "Epoch 00759: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0112 - accuracy: 0.4765 - mae: 0.0112 - mse: 0.0018 - val_loss: 0.0238 - val_accuracy: 0.4808 - val_mae: 0.0238 - val_mse: 0.0094\n",
            "Epoch 760/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0044 - accuracy: 0.5312 - mae: 0.0044 - mse: 1.3911e-04\n",
            "Epoch 00760: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0111 - accuracy: 0.4765 - mae: 0.0111 - mse: 0.0018 - val_loss: 0.0251 - val_accuracy: 0.4808 - val_mae: 0.0251 - val_mse: 0.0093\n",
            "Epoch 761/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0129 - accuracy: 0.5625 - mae: 0.0129 - mse: 5.2257e-04\n",
            "Epoch 00761: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0111 - accuracy: 0.4765 - mae: 0.0111 - mse: 0.0018 - val_loss: 0.0243 - val_accuracy: 0.4808 - val_mae: 0.0243 - val_mse: 0.0095\n",
            "Epoch 762/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0162 - accuracy: 0.6562 - mae: 0.0162 - mse: 0.0020\n",
            "Epoch 00762: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0112 - accuracy: 0.4765 - mae: 0.0112 - mse: 0.0018 - val_loss: 0.0239 - val_accuracy: 0.4808 - val_mae: 0.0239 - val_mse: 0.0095\n",
            "Epoch 763/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0060 - accuracy: 0.5938 - mae: 0.0060 - mse: 1.5116e-04\n",
            "Epoch 00763: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0109 - accuracy: 0.4765 - mae: 0.0109 - mse: 0.0018 - val_loss: 0.0257 - val_accuracy: 0.4808 - val_mae: 0.0257 - val_mse: 0.0092\n",
            "Epoch 764/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0094 - accuracy: 0.6875 - mae: 0.0094 - mse: 1.5744e-04\n",
            "Epoch 00764: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0116 - accuracy: 0.4765 - mae: 0.0116 - mse: 0.0018 - val_loss: 0.0237 - val_accuracy: 0.4808 - val_mae: 0.0237 - val_mse: 0.0094\n",
            "Epoch 765/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0077 - accuracy: 0.5000 - mae: 0.0077 - mse: 3.4548e-04\n",
            "Epoch 00765: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0109 - accuracy: 0.4765 - mae: 0.0109 - mse: 0.0018 - val_loss: 0.0250 - val_accuracy: 0.4808 - val_mae: 0.0250 - val_mse: 0.0096\n",
            "Epoch 766/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0299 - accuracy: 0.5000 - mae: 0.0299 - mse: 0.0150\n",
            "Epoch 00766: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.0112 - accuracy: 0.4765 - mae: 0.0112 - mse: 0.0018 - val_loss: 0.0239 - val_accuracy: 0.4808 - val_mae: 0.0239 - val_mse: 0.0094\n",
            "Epoch 767/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0046 - accuracy: 0.4375 - mae: 0.0046 - mse: 4.8098e-05\n",
            "Epoch 00767: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0113 - accuracy: 0.4765 - mae: 0.0113 - mse: 0.0018 - val_loss: 0.0236 - val_accuracy: 0.4808 - val_mae: 0.0236 - val_mse: 0.0094\n",
            "Epoch 768/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0110 - accuracy: 0.6250 - mae: 0.0110 - mse: 5.4875e-04\n",
            "Epoch 00768: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0111 - accuracy: 0.4765 - mae: 0.0111 - mse: 0.0018 - val_loss: 0.0238 - val_accuracy: 0.4808 - val_mae: 0.0238 - val_mse: 0.0094\n",
            "Epoch 769/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0089 - accuracy: 0.3125 - mae: 0.0089 - mse: 3.4643e-04\n",
            "Epoch 00769: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0113 - accuracy: 0.4765 - mae: 0.0113 - mse: 0.0018 - val_loss: 0.0243 - val_accuracy: 0.4808 - val_mae: 0.0243 - val_mse: 0.0095\n",
            "Epoch 770/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0133 - accuracy: 0.5312 - mae: 0.0133 - mse: 9.9128e-04\n",
            "Epoch 00770: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0113 - accuracy: 0.4765 - mae: 0.0113 - mse: 0.0018 - val_loss: 0.0237 - val_accuracy: 0.4808 - val_mae: 0.0237 - val_mse: 0.0094\n",
            "Epoch 771/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0089 - accuracy: 0.4375 - mae: 0.0089 - mse: 6.1584e-04\n",
            "Epoch 00771: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0110 - accuracy: 0.4765 - mae: 0.0110 - mse: 0.0018 - val_loss: 0.0236 - val_accuracy: 0.4808 - val_mae: 0.0236 - val_mse: 0.0094\n",
            "Epoch 772/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0060 - accuracy: 0.6250 - mae: 0.0060 - mse: 2.2540e-04\n",
            "Epoch 00772: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0111 - accuracy: 0.4765 - mae: 0.0111 - mse: 0.0018 - val_loss: 0.0247 - val_accuracy: 0.4808 - val_mae: 0.0247 - val_mse: 0.0093\n",
            "Epoch 773/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0086 - accuracy: 0.5312 - mae: 0.0086 - mse: 1.7384e-04\n",
            "Epoch 00773: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0113 - accuracy: 0.4765 - mae: 0.0113 - mse: 0.0018 - val_loss: 0.0237 - val_accuracy: 0.4808 - val_mae: 0.0237 - val_mse: 0.0094\n",
            "Epoch 774/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0184 - accuracy: 0.4062 - mae: 0.0184 - mse: 0.0016\n",
            "Epoch 00774: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0111 - accuracy: 0.4765 - mae: 0.0111 - mse: 0.0018 - val_loss: 0.0238 - val_accuracy: 0.4808 - val_mae: 0.0238 - val_mse: 0.0095\n",
            "Epoch 775/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0070 - accuracy: 0.4062 - mae: 0.0070 - mse: 2.8463e-04\n",
            "Epoch 00775: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0113 - accuracy: 0.4765 - mae: 0.0113 - mse: 0.0018 - val_loss: 0.0236 - val_accuracy: 0.4808 - val_mae: 0.0236 - val_mse: 0.0094\n",
            "Epoch 776/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0257 - accuracy: 0.5938 - mae: 0.0257 - mse: 0.0146\n",
            "Epoch 00776: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0112 - accuracy: 0.4765 - mae: 0.0112 - mse: 0.0018 - val_loss: 0.0237 - val_accuracy: 0.4808 - val_mae: 0.0237 - val_mse: 0.0094\n",
            "Epoch 777/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0105 - accuracy: 0.5312 - mae: 0.0105 - mse: 7.3933e-04\n",
            "Epoch 00777: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0112 - accuracy: 0.4765 - mae: 0.0112 - mse: 0.0018 - val_loss: 0.0238 - val_accuracy: 0.4808 - val_mae: 0.0238 - val_mse: 0.0093\n",
            "Epoch 778/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0086 - accuracy: 0.5625 - mae: 0.0086 - mse: 0.0010\n",
            "Epoch 00778: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0113 - accuracy: 0.4765 - mae: 0.0113 - mse: 0.0018 - val_loss: 0.0242 - val_accuracy: 0.4808 - val_mae: 0.0242 - val_mse: 0.0095\n",
            "Epoch 779/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0111 - accuracy: 0.5938 - mae: 0.0111 - mse: 0.0012\n",
            "Epoch 00779: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0115 - accuracy: 0.4765 - mae: 0.0115 - mse: 0.0018 - val_loss: 0.0237 - val_accuracy: 0.4808 - val_mae: 0.0237 - val_mse: 0.0094\n",
            "Epoch 780/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0060 - accuracy: 0.5000 - mae: 0.0060 - mse: 2.8251e-04\n",
            "Epoch 00780: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0112 - accuracy: 0.4765 - mae: 0.0112 - mse: 0.0018 - val_loss: 0.0240 - val_accuracy: 0.4808 - val_mae: 0.0240 - val_mse: 0.0095\n",
            "Epoch 781/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0154 - accuracy: 0.4062 - mae: 0.0154 - mse: 0.0015\n",
            "Epoch 00781: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0113 - accuracy: 0.4765 - mae: 0.0113 - mse: 0.0018 - val_loss: 0.0245 - val_accuracy: 0.4808 - val_mae: 0.0245 - val_mse: 0.0095\n",
            "Epoch 782/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0122 - accuracy: 0.3438 - mae: 0.0122 - mse: 0.0012\n",
            "Epoch 00782: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0112 - accuracy: 0.4765 - mae: 0.0112 - mse: 0.0018 - val_loss: 0.0243 - val_accuracy: 0.4808 - val_mae: 0.0243 - val_mse: 0.0095\n",
            "Epoch 783/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0110 - accuracy: 0.3438 - mae: 0.0110 - mse: 3.1820e-04\n",
            "Epoch 00783: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0113 - accuracy: 0.4765 - mae: 0.0113 - mse: 0.0018 - val_loss: 0.0244 - val_accuracy: 0.4808 - val_mae: 0.0244 - val_mse: 0.0095\n",
            "Epoch 784/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0299 - accuracy: 0.3438 - mae: 0.0299 - mse: 0.0150\n",
            "Epoch 00784: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0113 - accuracy: 0.4765 - mae: 0.0113 - mse: 0.0018 - val_loss: 0.0247 - val_accuracy: 0.4808 - val_mae: 0.0247 - val_mse: 0.0095\n",
            "Epoch 785/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0039 - accuracy: 0.5938 - mae: 0.0039 - mse: 2.7334e-05\n",
            "Epoch 00785: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0115 - accuracy: 0.4765 - mae: 0.0115 - mse: 0.0018 - val_loss: 0.0245 - val_accuracy: 0.4808 - val_mae: 0.0245 - val_mse: 0.0093\n",
            "Epoch 786/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0156 - accuracy: 0.5000 - mae: 0.0156 - mse: 0.0011\n",
            "Epoch 00786: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0110 - accuracy: 0.4765 - mae: 0.0110 - mse: 0.0018 - val_loss: 0.0235 - val_accuracy: 0.4808 - val_mae: 0.0235 - val_mse: 0.0094\n",
            "Epoch 787/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0072 - accuracy: 0.6562 - mae: 0.0072 - mse: 4.0093e-04\n",
            "Epoch 00787: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0110 - accuracy: 0.4765 - mae: 0.0110 - mse: 0.0018 - val_loss: 0.0237 - val_accuracy: 0.4808 - val_mae: 0.0237 - val_mse: 0.0094\n",
            "Epoch 788/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0070 - accuracy: 0.5625 - mae: 0.0070 - mse: 2.3633e-04\n",
            "Epoch 00788: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0110 - accuracy: 0.4765 - mae: 0.0110 - mse: 0.0018 - val_loss: 0.0243 - val_accuracy: 0.4808 - val_mae: 0.0243 - val_mse: 0.0093\n",
            "Epoch 789/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0106 - accuracy: 0.4688 - mae: 0.0106 - mse: 4.0997e-04\n",
            "Epoch 00789: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0112 - accuracy: 0.4765 - mae: 0.0112 - mse: 0.0018 - val_loss: 0.0236 - val_accuracy: 0.4808 - val_mae: 0.0236 - val_mse: 0.0094\n",
            "Epoch 790/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0263 - accuracy: 0.5000 - mae: 0.0263 - mse: 0.0147\n",
            "Epoch 00790: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0110 - accuracy: 0.4765 - mae: 0.0110 - mse: 0.0018 - val_loss: 0.0246 - val_accuracy: 0.4808 - val_mae: 0.0246 - val_mse: 0.0095\n",
            "Epoch 791/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0108 - accuracy: 0.5000 - mae: 0.0108 - mse: 4.5988e-04\n",
            "Epoch 00791: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0111 - accuracy: 0.4765 - mae: 0.0111 - mse: 0.0018 - val_loss: 0.0238 - val_accuracy: 0.4808 - val_mae: 0.0238 - val_mse: 0.0093\n",
            "Epoch 792/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0104 - accuracy: 0.5000 - mae: 0.0104 - mse: 7.1157e-04\n",
            "Epoch 00792: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0115 - accuracy: 0.4765 - mae: 0.0115 - mse: 0.0018 - val_loss: 0.0260 - val_accuracy: 0.4808 - val_mae: 0.0260 - val_mse: 0.0092\n",
            "Epoch 793/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0142 - accuracy: 0.5312 - mae: 0.0142 - mse: 0.0011\n",
            "Epoch 00793: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0113 - accuracy: 0.4765 - mae: 0.0113 - mse: 0.0018 - val_loss: 0.0246 - val_accuracy: 0.4808 - val_mae: 0.0246 - val_mse: 0.0092\n",
            "Epoch 794/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0078 - accuracy: 0.3438 - mae: 0.0078 - mse: 1.2238e-04\n",
            "Epoch 00794: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0109 - accuracy: 0.4765 - mae: 0.0109 - mse: 0.0018 - val_loss: 0.0235 - val_accuracy: 0.4808 - val_mae: 0.0235 - val_mse: 0.0094\n",
            "Epoch 795/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0068 - accuracy: 0.5000 - mae: 0.0068 - mse: 2.4860e-04\n",
            "Epoch 00795: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0111 - accuracy: 0.4765 - mae: 0.0111 - mse: 0.0018 - val_loss: 0.0236 - val_accuracy: 0.4808 - val_mae: 0.0236 - val_mse: 0.0094\n",
            "Epoch 796/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0096 - accuracy: 0.5625 - mae: 0.0096 - mse: 5.4695e-04\n",
            "Epoch 00796: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0110 - accuracy: 0.4765 - mae: 0.0110 - mse: 0.0018 - val_loss: 0.0254 - val_accuracy: 0.4808 - val_mae: 0.0254 - val_mse: 0.0096\n",
            "Epoch 797/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0122 - accuracy: 0.4062 - mae: 0.0122 - mse: 5.5736e-04\n",
            "Epoch 00797: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0113 - accuracy: 0.4765 - mae: 0.0113 - mse: 0.0018 - val_loss: 0.0240 - val_accuracy: 0.4808 - val_mae: 0.0240 - val_mse: 0.0093\n",
            "Epoch 798/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0128 - accuracy: 0.3125 - mae: 0.0128 - mse: 0.0011\n",
            "Epoch 00798: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0109 - accuracy: 0.4765 - mae: 0.0109 - mse: 0.0018 - val_loss: 0.0238 - val_accuracy: 0.4808 - val_mae: 0.0238 - val_mse: 0.0095\n",
            "Epoch 799/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0177 - accuracy: 0.3438 - mae: 0.0177 - mse: 0.0020\n",
            "Epoch 00799: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0110 - accuracy: 0.4765 - mae: 0.0110 - mse: 0.0018 - val_loss: 0.0241 - val_accuracy: 0.4808 - val_mae: 0.0241 - val_mse: 0.0093\n",
            "Epoch 800/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0257 - accuracy: 0.4375 - mae: 0.0257 - mse: 0.0144\n",
            "Epoch 00800: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0115 - accuracy: 0.4765 - mae: 0.0115 - mse: 0.0018 - val_loss: 0.0239 - val_accuracy: 0.4808 - val_mae: 0.0239 - val_mse: 0.0095\n",
            "Epoch 801/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0146 - accuracy: 0.3438 - mae: 0.0146 - mse: 0.0011\n",
            "Epoch 00801: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0110 - accuracy: 0.4765 - mae: 0.0110 - mse: 0.0018 - val_loss: 0.0243 - val_accuracy: 0.4808 - val_mae: 0.0243 - val_mse: 0.0093\n",
            "Epoch 802/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0187 - accuracy: 0.3750 - mae: 0.0187 - mse: 0.0022\n",
            "Epoch 00802: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0111 - accuracy: 0.4765 - mae: 0.0111 - mse: 0.0018 - val_loss: 0.0261 - val_accuracy: 0.4808 - val_mae: 0.0261 - val_mse: 0.0092\n",
            "Epoch 803/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0133 - accuracy: 0.5938 - mae: 0.0133 - mse: 7.3693e-04\n",
            "Epoch 00803: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0112 - accuracy: 0.4765 - mae: 0.0112 - mse: 0.0018 - val_loss: 0.0236 - val_accuracy: 0.4808 - val_mae: 0.0236 - val_mse: 0.0093\n",
            "Epoch 804/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0088 - accuracy: 0.3125 - mae: 0.0088 - mse: 4.9152e-04\n",
            "Epoch 00804: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0109 - accuracy: 0.4765 - mae: 0.0109 - mse: 0.0018 - val_loss: 0.0246 - val_accuracy: 0.4808 - val_mae: 0.0246 - val_mse: 0.0095\n",
            "Epoch 805/1000\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.0084 - accuracy: 0.4688 - mae: 0.0084 - mse: 3.9750e-04\n",
            "Epoch 00805: mae did not improve from 0.00937\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0111 - accuracy: 0.4765 - mae: 0.0111 - mse: 0.0018 - val_loss: 0.0251 - val_accuracy: 0.4808 - val_mae: 0.0251 - val_mse: 0.0092\n",
            "Epoch 00805: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hasil callback\n"
      ],
      "metadata": {
        "id": "LNlYKZtI6fHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history3.history['loss'], label='loss')\n",
        "plt.plot(history3.history['val_loss'], label='Valid')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history3.history['accuracy'], label='acc')\n",
        "plt.plot(history3.history['val_accuracy'], label='Valid')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history3.history['mae'], label='mae')\n",
        "plt.plot(history3.history['val_mae'], label='Valid mae')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history3.history['mse'], label='mse')\n",
        "plt.plot(history3.history['val_mse'], label='Valid mse')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0f2ebee2-9dfd-4103-c4bd-db542638fc0c",
        "id": "hVzZ5iXB6fHk"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}